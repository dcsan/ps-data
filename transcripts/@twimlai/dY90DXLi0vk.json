{
  "episodeId": "dY90DXLi0vk",
  "channelSlug": "@twimlai",
  "title": "Scaling Up Test-Time Compute with Latent Reasoning with Jonas Geiping - 723",
  "publishedAt": "2025-03-17T16:03:41.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "I think it's clear that we can sometimes",
      "offset": 0.199,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "think longer about a",
      "offset": 2.159,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "problem without actually verbalizing the",
      "offset": 4.759,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "answer in our head like we don't always",
      "offset": 7.279,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "like think in steps in our head but even",
      "offset": 8.639,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as humans we have these these two AES in",
      "offset": 11.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "which we can",
      "offset": 13.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "scale compute or thinking in this way",
      "offset": 15.76,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "and I think that's kind of interesting",
      "offset": 20,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "right this is like uh it's not",
      "offset": 21.16,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "necessarily uh you can do this or the",
      "offset": 22.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "other but maybe as humans we naturally",
      "offset": 26.039,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "do both of those and these models also",
      "offset": 28.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "could do",
      "offset": 31.16,
      "duration": 2.439
    },
    {
      "lang": "en",
      "text": "both all right everyone welcome to",
      "offset": 44.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "another episode of the twiml AI podcast",
      "offset": 46.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "I am your host Sam charington today I'm",
      "offset": 49.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "joined by Jonas Skyping Jonas is a",
      "offset": 51.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "research group leader at Ellis Institute",
      "offset": 53.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and the mech plank Institute for",
      "offset": 56.32,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "intelligent systems in tubingen before",
      "offset": 58.359,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "we get going be sure to take a moment to",
      "offset": 61.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "hit that subscribe button wherever",
      "offset": 62.879,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "you're listening to Today's Show Jonas",
      "offset": 64.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "welcome back to the",
      "offset": 66.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "podcast yeah happy to be back a lot has",
      "offset": 68.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "happened in the last year A lot has",
      "offset": 70.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "happened in the past year um I'm really",
      "offset": 73.52,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "excited about the conversation we're",
      "offset": 76.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "about to have you recently published a",
      "offset": 78.119,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "paper called scaling up test time",
      "offset": 80.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "compute with latent reasoning a",
      "offset": 82.6,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "recurrent depth approach uh that fits in",
      "offset": 84.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "at a unique time when you know reasoning",
      "offset": 87.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "models are getting popular you know just",
      "offset": 90.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "after the quote unquote deep deep seek",
      "offset": 92.159,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "moment if we want to call it that um and",
      "offset": 94.32,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "this paper proposes a different approach",
      "offset": 97.759,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "to reasoning um so really looking",
      "offset": 100.759,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "forward to digging into that work uh I'd",
      "offset": 103.64,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "love to have you spend a few minutes you",
      "offset": 106.799,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "know talking a little bit about your",
      "offset": 108.439,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "background and refreshing listeners who",
      "offset": 109.759,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "um you know may not have heard you",
      "offset": 113.399,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "introduced last year yeah I'm yunas I",
      "offset": 115.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "have an old background in meth ICS but",
      "offset": 118.399,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "I've been doing computer science maybe",
      "offset": 120.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "since 2016 and I've been a bit around in",
      "offset": 121.799,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Germany in the US now I'm back in",
      "offset": 124.96,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Germany um and I have and",
      "offset": 126.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I'm leading a little research group in",
      "offset": 129.56,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Germany in tuban where we work on safety",
      "offset": 131.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and efficiency aligned",
      "offset": 134.48,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "learning and these are really things",
      "offset": 136.12,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "that interest me a lot",
      "offset": 138.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "and something this project is particular",
      "offset": 140.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "is one that's has been a favorite of",
      "offset": 142.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "mine and really have been this has been",
      "offset": 144.92,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "a long time that we've worked on this",
      "offset": 146.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "because we really wanted to figure",
      "offset": 149.959,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "out how can we train these models very",
      "offset": 152.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "differently right or I could be like we",
      "offset": 155.879,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "had this idea that we could train these",
      "offset": 158.04,
      "duration": 2.279
    },
    {
      "lang": "en",
      "text": "models very differently and it would",
      "offset": 159.159,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "still be interesting and still be a good",
      "offset": 160.319,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "model and so um because this whole back",
      "offset": 162.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "story to this whole um recurrent",
      "offset": 165.879,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "approach is that uh way back when some",
      "offset": 168.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "good colleagues of mine trained these",
      "offset": 171.599,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "models in",
      "offset": 172.599,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "20221 we train trained these recurrent",
      "offset": 174.4,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "models to solve mazes",
      "offset": 177.2,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "I'm not sure if you have seen that it's",
      "offset": 180.519,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "a it's a it's really a toy problem right",
      "offset": 182.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "but it's kind of a very intriguing",
      "offset": 184,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "algorithmic",
      "offset": 185.4,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "task because you train these small",
      "offset": 186.56,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "recurrent models on Mazes of sizes like",
      "offset": 188.64,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "13 by 13 so it's really it's a tiny",
      "offset": 191.959,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "pixelized maze right and it's like it's",
      "offset": 194.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "like you look at it it's trivial what",
      "offset": 197.84,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "the solution is so you train these",
      "offset": 199.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "models on these 13 by 13 Maes and then",
      "offset": 201.519,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "at test time you just run them for more",
      "offset": 204.68,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "um recurrences so you do you put in more",
      "offset": 207.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "comp",
      "offset": 209.519,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "and then a test time you can solve",
      "offset": 210.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "arbitrarily large",
      "offset": 211.959,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "Maes like up to way like you have they",
      "offset": 213.72,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "have these funny pictures in the paper",
      "offset": 216.08,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "where it's like a you know the whole",
      "offset": 217.239,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "screen is filled like every pixel is a",
      "offset": 219.159,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "different pixel in this maze it's like a",
      "offset": 220.64,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "I don't know 8,000 by 8,000 pixel Maze",
      "offset": 222.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and the model just puts in a lot of",
      "offset": 225.879,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "compute and just solves it right and",
      "offset": 227.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "it's an interesting example because",
      "offset": 229.159,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "there this model really has learned the",
      "offset": 230.76,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "algorithm to solve Maes or solve this",
      "offset": 232.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "particular kind of maze um right and",
      "offset": 235.519,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that I think that was Al very motiv",
      "offset": 238.239,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "F so see that like uh you could really",
      "offset": 240.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "learn an algorithm like that but it was",
      "offset": 243.959,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "always a bit restricted too like you",
      "offset": 246.68,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "know you could only learn either you",
      "offset": 248,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "learn only Maes or you learn only",
      "offset": 249.28,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "addition or only um chess problems these",
      "offset": 251.879,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "are all bit of like maybe toy task or",
      "offset": 255.519,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "very like very um limited domains where",
      "offset": 257.44,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "we could see that these approaches would",
      "offset": 262.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "scale and what I may should also say is",
      "offset": 264.639,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "that so what's interesting here about",
      "offset": 266.919,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "recurrency here or how that comes in is",
      "offset": 269.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "um I think people are very familiar or",
      "offset": 271.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "maybe this is maybe the ml person",
      "offset": 274.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "speaking maybe are very familiar with",
      "offset": 276.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "recurrent neural",
      "offset": 278.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "networks right and in a and this is a",
      "offset": 280.84,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "bit different in a recurrent neural",
      "offset": 283.16,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "network what you have is that you have a",
      "offset": 285.039,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "sequence and the recurrence is basically",
      "offset": 287.639,
      "duration": 3.241
    },
    {
      "lang": "en",
      "text": "that like every time you go forward in",
      "offset": 289.68,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "the in the in the sequence you do",
      "offset": 290.88,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "another step of",
      "offset": 292.919,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "computation and this is an efficient way",
      "offset": 294.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "to compute things about sequences",
      "offset": 297.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "but what we're doing here is actually a",
      "offset": 300,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "bit different what we're here doing is",
      "offset": 301.08,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "um we're doing a recurrence in really in",
      "offset": 302.8,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "the depth of the model and like and a",
      "offset": 304.479,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "good analogy here is that the model",
      "offset": 307.479,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "doesn't have a fixed amount of",
      "offset": 309.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "layers but it has a small number of",
      "offset": 311.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "layers that it",
      "offset": 313.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "repeats and this relates of like lots of",
      "offset": 314.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "approaches in that field like a weight",
      "offset": 317.24,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "sharing sometimes called a weight",
      "offset": 318.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "tying in this way the model really has",
      "offset": 320.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like an arbitrary number of",
      "offset": 322.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "layers which really means that",
      "offset": 324.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "um in comparison to our recurrent new",
      "offset": 326.96,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "network",
      "offset": 329.319,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "where you always produce more input or",
      "offset": 330.759,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "more output to do more compute same way",
      "offset": 332.759,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "in in a transformer in the Transformer",
      "offset": 335.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "you also every time you want to do more",
      "offset": 336.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Compu you have to produce more",
      "offset": 339.039,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "outputs with this sort of like um level",
      "offset": 341.52,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "of uh of this recurrent depth you can",
      "offset": 344.8,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "produce an arbitrary amount of compute",
      "offset": 348.479,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "before your compute the next output so",
      "offset": 351.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it's sort of like the the compute axis",
      "offset": 353.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "is is disentangled from the",
      "offset": 355.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "output axis from the context of the",
      "offset": 358.24,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "model and that's what's so interesting I",
      "offset": 360.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "mean algorithmically about this kind of",
      "offset": 363.759,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "approach got it got it so the immediate",
      "offset": 366,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "thought in hearing the title um and the",
      "offset": 369.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "idea of recurrence is to think oh maybe",
      "offset": 372.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're doing something with rnns or like",
      "offset": 374.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "splicing that kind of occurrence into a",
      "offset": 377.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Transformer it's not really like that",
      "offset": 380,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's um manipulating the number of",
      "offset": 381.68,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "layers at test time to kind of flex the",
      "offset": 384.4,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "amount of compute that being uh applied",
      "offset": 388.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "to a given",
      "offset": 391.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "generation yeah exact exactly it's a bit",
      "offset": 392.88,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "um itates relates a bit to these",
      "offset": 396.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "analogies for maybe for an RNN or for a",
      "offset": 398.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Transformer where you",
      "offset": 400.16,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "um for example I think the analogy for",
      "offset": 403.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "an RNN would be that you actually don't",
      "offset": 406.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "move to the next token in the",
      "offset": 409.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "sequence but you sort of like produce",
      "offset": 410.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like filler outputs and then you keep on",
      "offset": 413,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "iterating on these filler outputs that",
      "offset": 415.28,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "would be may like where you really like",
      "offset": 417.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you really stop on the sequence and then",
      "offset": 418.759,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "you on your point where you currently",
      "offset": 420.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "stopped you keep on like recurring a bit",
      "offset": 421.84,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "and then you move forward you mentioned",
      "offset": 424.16,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "how you you had this um paper that",
      "offset": 426.319,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "explored this recurrent approach uh as",
      "offset": 429.599,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "kind of",
      "offset": 432.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "background um but then I'm",
      "offset": 433.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "wondering talk a little bit about kind",
      "offset": 438.36,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "of the impetus to apply that it comes at",
      "offset": 440.4,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "an interesting time when you know we've",
      "offset": 442.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "got these reasoning models like the 01",
      "offset": 444.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "series from open AI um you know deepsea",
      "offset": 446.24,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "uh showed how you could take a very",
      "offset": 449.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different approach to training those um",
      "offset": 451.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "you know this paper came out shortly",
      "offset": 454.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "after that but I'm suspecting you've",
      "offset": 455.84,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "been working on it for quite a while um",
      "offset": 457.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "so kind of situated situate it for us in",
      "offset": 460.28,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "time and you know what motivated you to",
      "offset": 463.56,
      "duration": 6.759
    },
    {
      "lang": "en",
      "text": "um go after this particular approach of",
      "offset": 467.479,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "um reasoning Yeah so basically this is",
      "offset": 470.319,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "all the way back in 2023 like Tom and me",
      "offset": 474.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Tom is the last author we had just been",
      "offset": 477.44,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "working on a paper on um training",
      "offset": 479.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "language models more efficiently and",
      "offset": 480.879,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then we've been talking a lot about",
      "offset": 483.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these recurrent models and how they",
      "offset": 484.639,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "would be cool to do in language and",
      "offset": 486.84,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "because we really think it would be an",
      "offset": 489.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "interesting language model that could",
      "offset": 490.08,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "also solve things like math and code",
      "offset": 492.12,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "better if it could reason more because",
      "offset": 493.599,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "back then people were also doing a lot",
      "offset": 496.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "of uhe models and are interesting",
      "offset": 497.96,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "because they're The Other Extreme it's a",
      "offset": 501.56,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "model that has more parameters but uses",
      "offset": 503.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "less of",
      "offset": 504.879,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "them right so that seems like Optimal to",
      "offset": 506,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like maybe store information",
      "offset": 508.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and this is in some conceptual way it's",
      "offset": 510.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the opposite it's a model that has fewer",
      "offset": 512.479,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "parameters it's like fewer parameters",
      "offset": 513.919,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "but uses them over and over and over yes",
      "offset": 515.399,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "exactly right so we really wanted to",
      "offset": 517.279,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "figure out how we could make this",
      "offset": 520.159,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "Paradigm of recurrent depth work with",
      "offset": 521.479,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "language and how we could get um how we",
      "offset": 524.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "could modify it what a good like",
      "offset": 528.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "training objective would be to really",
      "offset": 530.12,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "scale this",
      "offset": 532,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "up um because it really wasn't really",
      "offset": 533.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "shown before in Transformers at all like",
      "offset": 535.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "uh or there's some at least like from",
      "offset": 537.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "our like from this like maze Direction",
      "offset": 540.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "these were always convolutional models",
      "offset": 542.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "and it wasn't so clear how to put this",
      "offset": 544.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "in Transformer and be nice there's some",
      "offset": 545.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "interesting parallel work on looped",
      "offset": 548.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Transformers which goes in the related",
      "offset": 549.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Direction but it didn't quite work as",
      "offset": 552.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "well for us um just scaling those up",
      "offset": 554.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "larger um and that relates a bit to this",
      "offset": 557.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "part that um this actually also is uh so",
      "offset": 560.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like one part that's different here is",
      "offset": 564.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "that it's",
      "offset": 565.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "latent where basically um",
      "offset": 567.44,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "we actually have a few non-recurrent",
      "offset": 570.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "layers in the model and then we have",
      "offset": 572.519,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this recurrent part in the middle and",
      "offset": 574.76,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "then we have a few more non recur layers",
      "offset": 576.279,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "at the",
      "offset": 577.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "end to really have this computation be",
      "offset": 578.68,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "uh in this like centralized Laten",
      "offset": 581.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "space and then still have some layers",
      "offset": 584.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that actually decode back into language",
      "offset": 587,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and have some layers at the start that",
      "offset": 589.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "decode from",
      "offset": 590.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "language and then with the right",
      "offset": 592.68,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "objective this really helped us a lot",
      "offset": 594.32,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and then we had",
      "offset": 596.399,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "some let's say like 100 Mill parameter",
      "offset": 597.519,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "prototypes in",
      "offset": 600.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "2023 and we're like okay what next we're",
      "offset": 602.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "academics right so we we teamed up with",
      "offset": 605.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "lots of people that know a lot more",
      "offset": 607.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "about um high performance Computing and",
      "offset": 609.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we wrote this big allocation grant for",
      "offset": 612.24,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "um Insite is like some big allocation",
      "offset": 614.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Grant basically this compute that we",
      "offset": 616.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "then got at oakd at Oak National",
      "offset": 619.32,
      "duration": 8.44
    },
    {
      "lang": "en",
      "text": "Labs and but then like it's still there",
      "offset": 623.04,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "um it was quite interesting Journey",
      "offset": 627.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "because it's it's one thing to like",
      "offset": 630.6,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "train this model and P toin 100 million",
      "offset": 632.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "parameters and it's kind of nice right",
      "offset": 634.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "to actually seeing okay now we now there",
      "offset": 636.839,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "are 4,000 accelerators and it still has",
      "offset": 639.16,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "to work and they have to work together",
      "offset": 642.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and this like funny",
      "offset": 644.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "little probalistic objective that you",
      "offset": 646.76,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "wrote still has to scale and has to work",
      "offset": 649.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "here and that was a pretty interesting",
      "offset": 651.959,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "Journey that I personally enjoyed a lot",
      "offset": 653.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "although it was a bit Haring at times as",
      "offset": 656,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "well to actually make this work and run",
      "offset": 658.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "this on these cards right because we",
      "offset": 660.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "also um so this is um this was computed",
      "offset": 662.6,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "on Frontier and Frontier are the largest",
      "offset": 665.56,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "AMD super computer in the world and just",
      "offset": 668.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "and then we were just given that amount",
      "offset": 672.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "of compute because the um the folks that",
      "offset": 673.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "inside were saying yeah here we have",
      "offset": 676.2,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "this you could have",
      "offset": 677.48,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "this and so um but uh 2024 was also the",
      "offset": 678.88,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "year",
      "offset": 683.959,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "where a lot of things were moving at AMD",
      "offset": 684.839,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and were just beginning to be supported",
      "offset": 687.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "for example flash attention like flash",
      "offset": 690.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "attention or something is like a very",
      "offset": 692.16,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "basic algorithm for us nowadays but it",
      "offset": 693.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "took like a lot of time maybe like until",
      "offset": 696.36,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "like half the year maybe until like last",
      "offset": 698.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "July where I felt this was really was",
      "offset": 700.72,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "stable enough that we",
      "offset": 702.04,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "could reliably deploy it and run it and",
      "offset": 703.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "it was a lot back and forth and mg did a",
      "offset": 706.519,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "lot and we had",
      "offset": 708.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "some maybe like a lot of issues but we",
      "offset": 709.519,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "got around them it did it did train in",
      "offset": 711.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "the end but it was a whole journey maybe",
      "offset": 713.639,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 715.92,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "like maybe a year where the underlying",
      "offset": 717.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "little model didn't really change right",
      "offset": 720.519,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "or like",
      "offset": 722.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the I think scale it up yeah right like",
      "offset": 723.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "in the paper maybe that's like section",
      "offset": 727.519,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "three and section three is like okay",
      "offset": 728.6,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "2023 and then all of 20 all of 2024 is",
      "offset": 731,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "just um section four which is oh we",
      "offset": 734.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "trained it on 4,000 gpus oh wow and that",
      "offset": 736.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "was kind of an interesting journey and",
      "offset": 739.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "um yeah then but then it also it got",
      "offset": 740.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "pretty exciting because as you mentioned",
      "offset": 743.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "then uh we had these test s comput",
      "offset": 744.76,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "reasoning models coming out in fall",
      "offset": 747.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and we still hadn't like finished",
      "offset": 750.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "training the model and we were thinking",
      "offset": 751.36,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "oh and suddenly like this was in the",
      "offset": 754.32,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "news everywhere and sudden this was",
      "offset": 757.48,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "happening and we still had finished",
      "offset": 758.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "training the model and then now we did",
      "offset": 760.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "finish training the model in early",
      "offset": 763.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "December last year and it's also why we",
      "offset": 764.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "pretty much wrote up our results in",
      "offset": 767.76,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "January and published them but it was a",
      "offset": 769.279,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "bit of a scary moment as like the like",
      "offset": 773.24,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "the world caught up to us a bit in that",
      "offset": 775,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "although that's also why the approach is",
      "offset": 776.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "a bit different yeah to what degree did",
      "offset": 778.279,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "the way you position the results around",
      "offset": 781.48,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "reasoning um you know was that impacted",
      "offset": 786.079,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "by the the timing of when you finished",
      "offset": 789.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the the work like was that specifically",
      "offset": 792.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the way you were thinking about it going",
      "offset": 796.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "in or was it more like you've got this",
      "offset": 797.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "architecture let's try to scale it and",
      "offset": 799.24,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "see what it can do and then lo and",
      "offset": 800.72,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "behold Like It produced results that",
      "offset": 802.639,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "were relevant to you know the Zeitgeist",
      "offset": 804.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "at the time it was always just idea of",
      "offset": 807.48,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "reasoning and learning algorithms but",
      "offset": 809.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "maybe like that that framing is maybe",
      "offset": 811.76,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "like the more classical framing would be",
      "offset": 813.839,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "it's it's learning an",
      "offset": 815.16,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "algorithm this would be like this older",
      "offset": 817.04,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "L papers would maybe call this learning",
      "offset": 818.959,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "an algorithm or and a prior towards",
      "offset": 820.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "learning algorithmic",
      "offset": 822.519,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "reasoning to call this all just blanket",
      "offset": 824.92,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "reasoning maybe that's a more modern way",
      "offset": 827.079,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of looking at it and the other part",
      "offset": 828.519,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "that's also very modern is that like in",
      "offset": 830.759,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "the intro now we clearly call this out",
      "offset": 832.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "that there's a",
      "offset": 834.079,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "um I think we we we made this term up",
      "offset": 835.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "but I think it fits quite well to",
      "offset": 838.32,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "compare sort of like verbalized",
      "offset": 839.839,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "reasoning which we which is how we how",
      "offset": 842.519,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "we um maybe the Box we put in these",
      "offset": 845.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "other approaches to what we're doing and",
      "offset": 847.279,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "that's a mod distinction that we put",
      "offset": 849.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "into claim okay what we are we doing",
      "offset": 851,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "differently and I really hadn't thought",
      "offset": 853.759,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "about so much that Al like it wasn't so",
      "offset": 855.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "clear to me",
      "offset": 857.48,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "that just long chain of thoughts could",
      "offset": 858.72,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "scale so well but maybe this wasn't",
      "offset": 861.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "clear to many of us a year ago but I",
      "offset": 863.32,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "think it's interesting to look at them",
      "offset": 865.199,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "in this dichotomy that one is really",
      "offset": 866.279,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "trying to",
      "offset": 868.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to do these um to learn these algorithms",
      "offset": 869.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "in a high dimensional space so like",
      "offset": 872.639,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "before it really decodes into language",
      "offset": 874.8,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and the other approach is really trying",
      "offset": 878.04,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "to scaffold reasoning onto language",
      "offset": 879.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "let's maybe maybe even back up a step",
      "offset": 882.639,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "and really dig into that because I think",
      "offset": 885.32,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "that that is an idea that um you know",
      "offset": 887.959,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "people are really excited about uh and",
      "offset": 892.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that is really exciting this idea that",
      "offset": 894.56,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "uh with traditional language models you",
      "offset": 897.56,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "know the you know the high comput and",
      "offset": 900.92,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "reasoning that we're seeing it's all",
      "offset": 903.519,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "like having the models think out loud",
      "offset": 906.32,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and in a sense what this paper is",
      "offset": 909.759,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "proposing is that we could also have the",
      "offset": 911.839,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "models kind of think to themselves and",
      "offset": 914.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "then you know not necessarily generate",
      "offset": 917.48,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "you know all of that thinking as",
      "offset": 920,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "language",
      "offset": 921.68,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "and so for with with that in mind kind",
      "offset": 923.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "of talk about how you think about you",
      "offset": 926.199,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "know the Rel ship between those two and",
      "offset": 928.12,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "you know the the relative merits of One",
      "offset": 931.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Versus the other and how they might",
      "offset": 933.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "compare like I'm curious like how you",
      "offset": 935.48,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "think about all that yeah so maybe the",
      "offset": 937.68,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "first thing I think is said",
      "offset": 939.36,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "interestingly I think this is quite",
      "offset": 941,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "natural for us as well maybe really as",
      "offset": 942.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "humans this is a bit of an",
      "offset": 944.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "anthropomorphizing I hope that's fine",
      "offset": 945.44,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "but basically um I think it's clear that",
      "offset": 947.839,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "we can sometimes think longer about a",
      "offset": 951.199,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "problem without actually verbalizing the",
      "offset": 954.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "answer in our head like we don't always",
      "offset": 957.199,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "like think steps in our",
      "offset": 958.519,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "head um especially if it's a problem",
      "offset": 960.279,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "that maybe relates to maybe spatial",
      "offset": 962.6,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "reasoning or maybe something that you",
      "offset": 964.56,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "have some intuitive maybe motor planning",
      "offset": 967.16,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "or like planning reasoning or how you go",
      "offset": 969.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from one place to another these are",
      "offset": 971.519,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "often things that we think about and we",
      "offset": 973.48,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "also think about longer but we don't",
      "offset": 975.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "really verbalize this kind of thinking",
      "offset": 977.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and then interestingly we also like",
      "offset": 980.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these models we can we can still write",
      "offset": 982.36,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "things down right we can sort of like",
      "offset": 983.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "extend our thinking process by",
      "offset": 985,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "verbalizing it by writing things down in",
      "offset": 987.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Notebook books in equations right in",
      "offset": 988.6,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "this way we can externalize thinking and",
      "offset": 991.36,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "think even",
      "offset": 992.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "longer but even as humans we have these",
      "offset": 994.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "these two AES in which we can",
      "offset": 996.68,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "scale compute or thinking in this way",
      "offset": 1000,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "and I think that's kind of interesting",
      "offset": 1004.24,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "right this is like a it's not",
      "offset": 1005.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "necessarily uh you can do this or the",
      "offset": 1006.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "other but maybe as humans we naturally",
      "offset": 1010.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "do both of those and these models also",
      "offset": 1012.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "could do",
      "offset": 1015.44,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "both but um what's interesting about",
      "offset": 1016.88,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "thinking without verbalizing it is that",
      "offset": 1020.519,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "um maybe like from a very basic uh",
      "offset": 1024.6,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "perspective this just seems much more",
      "offset": 1027.959,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "powerful um because basically the",
      "offset": 1029.48,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "model now if we come bit like uh like",
      "offset": 1033.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the like theory of like what algorithms",
      "offset": 1036.64,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "can Transformers",
      "offset": 1038.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "represent basically the Transformer does",
      "offset": 1039.839,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "some calculation because it is like n",
      "offset": 1042.039,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "layers deep let's say that's like 96",
      "offset": 1044.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "layers deep so the Transformer has some",
      "offset": 1046.439,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "the model does some calculation and the",
      "offset": 1050.12,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "next W comes",
      "offset": 1051.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "out and then in the next",
      "offset": 1053.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "layer it can maybe access these previous",
      "offset": 1055.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "compet calculations but again only up to",
      "offset": 1058.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the 96th",
      "offset": 1061.52,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "layer and if the model were to do a",
      "offset": 1063.2,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "calculation that's deeper than 96 it's",
      "offset": 1065.559,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "kind of hard because it has to go into",
      "offset": 1068.559,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "these previous attention",
      "offset": 1070.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Parts um so like the the model is a bit",
      "offset": 1072.919,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Limited in its depth you're kind of",
      "offset": 1075.72,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "calling out this idea a that like I mean",
      "offset": 1077.919,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "in some ways it's parallel to the",
      "offset": 1080.919,
      "duration": 7.441
    },
    {
      "lang": "en",
      "text": "argument for test time compute in",
      "offset": 1085.679,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "general it's like that argument is often",
      "offset": 1088.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "stated as well you know look at us as",
      "offset": 1091,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "humans like there are things that we you",
      "offset": 1092.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "know think about and the answer comes to",
      "offset": 1094.96,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "us very quickly and there are other",
      "offset": 1096.28,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "things that we spend a lot of time",
      "offset": 1097.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thinking about and so why should models",
      "offset": 1098.72,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "have like just a fixed uh you know",
      "offset": 1101.4,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "compute uh fixed budget for you know",
      "offset": 1104.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "response uh which kind of led to the",
      "offset": 1107.799,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "idea of test time compute to some degree",
      "offset": 1110.28,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and allowing models to spend more time",
      "offset": 1112.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "on that compute and what you're kind of",
      "offset": 1115.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "suggesting is that there's like another",
      "offset": 1117,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "dimension of that um and",
      "offset": 1119.2,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "um",
      "offset": 1123.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "there's there's another dimension of it",
      "offset": 1125.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "from a Transformer architecture",
      "offset": 1128.159,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "perspective like you're both time and",
      "offset": 1129.72,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "compute limited and also architecturally",
      "offset": 1133.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "limited and you the recurrence kind kind",
      "offset": 1135.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "of gives you some Flex in that other",
      "offset": 1138.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "dimension I don't know like what the",
      "offset": 1140.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "implications of that are but um it it",
      "offset": 1142.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "strikes me that there are some parallels",
      "offset": 1145.44,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "there yeah yeah I think an interesting",
      "offset": 1147.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "analogy to make this clearer is that in",
      "offset": 1149.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "some way if we like exract a bit away",
      "offset": 1152.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "from it what the Transformer is doing if",
      "offset": 1155.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "it's doing a very long Chain of Thought",
      "offset": 1157.159,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "is that it it has some hidden",
      "offset": 1159.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "representation that computed on what the",
      "offset": 1161.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "next token should",
      "offset": 1162.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "be then that whole hidden representation",
      "offset": 1164.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "which is a high dimensional vector",
      "offset": 1167.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is thrown away and then one token is",
      "offset": 1169.559,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "sampled out of it and the next step that",
      "offset": 1171.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "token is embedded again to compute a",
      "offset": 1174.159,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "whole another hidden representation",
      "offset": 1176.52,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "which is thrown away and then a new",
      "offset": 1178.799,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "token comp is like embedded right",
      "offset": 1181.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "because we and we s like keep like",
      "offset": 1183.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "embedding and",
      "offset": 1185.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "unemed these representations that the",
      "offset": 1186.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "model is",
      "offset": 1188.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "providing and in some",
      "offset": 1189.679,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "way um maybe the only thing that this",
      "offset": 1192,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "recurrent model is doing is that it's",
      "offset": 1194.6,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "just not doing that it's basically",
      "offset": 1195.919,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "keeping it keeps on recurring and keeps",
      "offset": 1197.36,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "on refining this hidden",
      "offset": 1199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "representation instead of always like",
      "offset": 1200.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "encoding and decoding it in every step",
      "offset": 1203.96,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and so how does it um how does it know",
      "offset": 1205.799,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "how much to do this when to stop doing",
      "offset": 1209.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "it that kind of thing yeah so",
      "offset": 1211.96,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "interestingly that's actually not",
      "offset": 1213.679,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "something we have we have done a lot in",
      "offset": 1214.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "this work um so what what we've done a",
      "offset": 1216.28,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "lot is basically is um show how to scale",
      "offset": 1218.799,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "these models and how to train them to",
      "offset": 1221.919,
      "duration": 1.921
    },
    {
      "lang": "en",
      "text": "have this",
      "offset": 1223,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "capability but we actually found that um",
      "offset": 1223.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the best way we could scale these models",
      "offset": 1227.24,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "was just to train them with a randomized",
      "offset": 1230.4,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "depth or randomized number of",
      "offset": 1232.12,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "occurrences really okay so basically",
      "offset": 1233.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "there's some older work also like from",
      "offset": 1236.559,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "called Universal Transformers right",
      "offset": 1238.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because this sort like this whole thing",
      "offset": 1240.52,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "relates to like Universal complete",
      "offset": 1241.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Universal to in completeness and these",
      "offset": 1243.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "models had",
      "offset": 1246.12,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "halting uh sub modules that where them",
      "offset": 1247.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "during training you had to decide when",
      "offset": 1250.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to stop but this those are pretty hard",
      "offset": 1251.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "to scale those are pretty hard to train",
      "offset": 1254.36,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "because you you're kind of training a",
      "offset": 1255.679,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "model to get better",
      "offset": 1257.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "and at the same time you're training it",
      "offset": 1259.4,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "to exit",
      "offset": 1260.44,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "early and that's a bit of it's it's it's",
      "offset": 1261.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "hard to bootstrap a model that has to",
      "offset": 1264.08,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "get better on harder",
      "offset": 1265.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "problems and has to know when to exit",
      "offset": 1266.919,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "early and so one way we got around this",
      "offset": 1270,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "was basically that like during training",
      "offset": 1272.36,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "we have a a Lo normal person",
      "offset": 1274.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "distribution really a heavy tale",
      "offset": 1277.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "distribution over possible steps and we",
      "offset": 1279.2,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "simple one of them and we just compute",
      "offset": 1281.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "loss on this step and then we back we",
      "offset": 1284.039,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "back propagate and this is how we train",
      "offset": 1285.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "the model but then interestingly at",
      "offset": 1287.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "inference time what we found is that",
      "offset": 1290.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "just we could just zero shot uh adaptive",
      "offset": 1292.88,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "exits for example just by by uh",
      "offset": 1296.36,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Computing how much the state changes",
      "offset": 1299.279,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "from step to",
      "offset": 1301.6,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "step we could just make a simple rule",
      "offset": 1303.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "based on that and just exit based on the",
      "offset": 1305.48,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "simple rule that we just put in zero",
      "offset": 1307.799,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "shot afterwards which was kind of",
      "offset": 1309.679,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "exciting that this actually did work at",
      "offset": 1311.36,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "all but it wasn't a guarantee that this",
      "offset": 1312.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "what's the specific state that you're",
      "offset": 1314.919,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "applying that algorithm to so we've",
      "offset": 1317.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "tried this with several ones we've tried",
      "offset": 1319.919,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "this with the actual like latent",
      "offset": 1321.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "recurrent State and for examp the L2",
      "offset": 1323.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Norm of that state to the previous",
      "offset": 1325.84,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "state but we've also done this in the",
      "offset": 1327.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "output distribution so the right we have",
      "offset": 1329.72,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "like a well maybe we at State 16 and so",
      "offset": 1332.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "we compute the next token",
      "offset": 1335.559,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "distribution and we can then we go to",
      "offset": 1337.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "step step let's say 24 we again comput",
      "offset": 1339.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the next token distribution and then we",
      "offset": 1342.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "can compare the C Li Divergence of these",
      "offset": 1344.64,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "two probability distributions",
      "offset": 1346.799,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "and if that is small then we just exit",
      "offset": 1349.32,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "the idea being that the model's",
      "offset": 1351.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "converging on some thought or idea",
      "offset": 1352.64,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "exactly and interestingly like the",
      "offset": 1355.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "second one seems a bit more practically",
      "offset": 1357.279,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "better because what we observe is",
      "offset": 1359.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "sometimes the model keeps on um thinking",
      "offset": 1361.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "in this in this recurrent",
      "offset": 1363.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "space and we have some interesting",
      "offset": 1365.559,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Graphics in the paper where like what is",
      "offset": 1367.279,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "actually or like how we can project that",
      "offset": 1368.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "down into lower Dimensions how it's",
      "offset": 1370.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "thinking there but this thinking doesn't",
      "offset": 1371.96,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "always reflect in the um probability of",
      "offset": 1374.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the next token",
      "offset": 1377.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "right that's sort of like a more stable",
      "offset": 1379.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "state is actually what do you predict",
      "offset": 1381.039,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "next right and it's it's like it's not",
      "offset": 1382.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "it wasn't so clear to us beforehand or",
      "offset": 1385.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "maybe also just wasn't clear in general",
      "offset": 1387.679,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "whether the model would learn the",
      "offset": 1389.36,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "specialization right basically this was",
      "offset": 1391.6,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "a research question was like whether",
      "offset": 1393.279,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "just with",
      "offset": 1395.24,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "scale would the model learn to",
      "offset": 1396.84,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "specialize this recurrence right because",
      "offset": 1399.159,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "um we have this picture in the paper how",
      "offset": 1401.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "the model for example converges",
      "offset": 1403.039,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "quickly when the next Tok prediction is",
      "offset": 1405.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "easy",
      "offset": 1407.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "but the convergence is much slower if",
      "offset": 1409.64,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the next Tok prediction is hard or if",
      "offset": 1411.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "this a token that's somewh important for",
      "offset": 1413.52,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "the question to understand the",
      "offset": 1415,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "question why don't we we totally didn't",
      "offset": 1416.919,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "train for this we trained with um we",
      "offset": 1418.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "take fixed sequence we take um training",
      "offset": 1421.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sequences and we assign them the same",
      "offset": 1423.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "step budget for the entire sequence just",
      "offset": 1426.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "because that makes sense for",
      "offset": 1429.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "pre-training scale right and this like",
      "offset": 1429.84,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "per token specialization just seem to",
      "offset": 1432.159,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "emerge with scale that the model",
      "offset": 1434.559,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "actually converges quicker on easy",
      "offset": 1436.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "tokens than on hard ones which was that",
      "offset": 1438.84,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "was pretty",
      "offset": 1441.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "interesting and we really didn't know it",
      "offset": 1443.36,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "would work like this before we saw the",
      "offset": 1445.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "final model uh so elaborate on that when",
      "offset": 1447.2,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "you call when you refer to per token",
      "offset": 1449.32,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "specialization um what do you mean by",
      "offset": 1452.799,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that so basically um the way the model",
      "offset": 1455.159,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "was pre-trained was not only do we um",
      "offset": 1457.96,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "pick a random step for every",
      "offset": 1461.799,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "recurrence but we also picked the same",
      "offset": 1464.6,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "step count for all the tokens in a in",
      "offset": 1467.36,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "example",
      "offset": 1470.799,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "sequence because basically um for this",
      "offset": 1473.48,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to um be parallelized right the model",
      "offset": 1476.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "has to do the same amount of compute at",
      "offset": 1478.6,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "every token in the like this maybe train",
      "offset": 1481,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "this is train on sequences are 4,000",
      "offset": 1484.039,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "tokens long so meaning that random",
      "offset": 1485.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "number is at like a batch level or",
      "offset": 1488,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "something exactly right we actually",
      "offset": 1490.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "saying okay this batch gets three steps",
      "offset": 1492.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "this batch gets eight steps this batch",
      "offset": 1494.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "gets 128 steps",
      "offset": 1496.24,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "but then we look at it the test time and",
      "offset": 1498.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we actually see that the model now",
      "offset": 1500.919,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "um has this convergence graph that's",
      "offset": 1503.32,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "very different on a per token",
      "offset": 1505.84,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "level where on some tokens it it goes",
      "offset": 1508.399,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "like green very quickly which means like",
      "offset": 1511,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "it converges very quickly and in other",
      "offset": 1512.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "ones it just keeps on doing things for a",
      "offset": 1514.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "longer time and this specialization",
      "offset": 1517.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "really just emerged during training okay",
      "offset": 1519.72,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "so let me make sure I'm understanding",
      "offset": 1521.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this right what you're saying is that",
      "offset": 1523.84,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "you've trained it on just these random",
      "offset": 1525.24,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "number numbers on a per sequence level",
      "offset": 1528.039,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "like random number of steps but then at",
      "offset": 1530.64,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "test time the model actually does apply",
      "offset": 1533.64,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "more compute to more difficult tokens in",
      "offset": 1536.08,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "this case I was a bit more like dancing",
      "offset": 1540.32,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "around saying like does it use more",
      "offset": 1542.919,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "compute for that um meaning because",
      "offset": 1544.12,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "that's more nuanced it's more nuanced",
      "offset": 1547.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "than that oh it's more like in the in",
      "offset": 1550,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "the chart like in the chart and paper",
      "offset": 1551.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "what we show actually is that we in that",
      "offset": 1553.44,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "chart we actually run the model",
      "offset": 1555.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for the the same number of comput right",
      "offset": 1557.08,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for all steps that's why this is like a",
      "offset": 1559.279,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "rectangular",
      "offset": 1560.919,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "chart and then on that chart we just see",
      "offset": 1563.039,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "how the convergence like how easily like",
      "offset": 1565.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "how quickly it's done in every",
      "offset": 1567.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "token but to make that chart we actually",
      "offset": 1569.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "run all the steps to completion and okay",
      "offset": 1571.799,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "from that we from that we imply that you",
      "offset": 1574.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "could have exited early but the chart",
      "offset": 1576.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "really has like is quadratic it has like",
      "offset": 1579.12,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the full compute Yeah so basically what",
      "offset": 1580.72,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "I'm really saying is like it can do that",
      "offset": 1582.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "um for most of the paper we haven't done",
      "offset": 1584.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "this a lot yet maybe this is maybe most",
      "offset": 1586.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the exact way to say it",
      "offset": 1588.32,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "um uh right because we have the section",
      "offset": 1590.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "about um how you could do adaptive exits",
      "offset": 1593,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and you could exit",
      "offset": 1595.039,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "early and that's six that's section",
      "offset": 1596.279,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "three 6.3 in the paper and there we try",
      "offset": 1598.399,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "this but that's also where we discover",
      "offset": 1601.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that this is possible and so like lot a",
      "offset": 1603.32,
      "duration": 3.239
    },
    {
      "lang": "en",
      "text": "lot of the rest of the paper just hasn't",
      "offset": 1604.88,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "had this yet a lot of rest paper is like",
      "offset": 1606.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "here's a query I'm going to run the",
      "offset": 1608.679,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "query with 32 steps use a different",
      "offset": 1610.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "query I'm going to run 64 steps so a lot",
      "offset": 1612.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "of the paper hasn't caught up to this",
      "offset": 1615.08,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "more adaptive Paradigm yet where it's Al",
      "offset": 1616.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "something that like we were surprised it",
      "offset": 1619.12,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "would work at all and which is something",
      "offset": 1620.279,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "where um I'm POS personally like very",
      "offset": 1622.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "excited thinking back to these holding",
      "offset": 1625.36,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "modules of um now that this model is",
      "offset": 1627.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "trained we can probably just fine tune",
      "offset": 1631.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "these stopping modeles onto it again",
      "offset": 1634.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "because now the boot serving problem is",
      "offset": 1637.72,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "solved and now we can just fine tune",
      "offset": 1639.24,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "these and have these exit conditions",
      "offset": 1640.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "again that I learned but we just haven't",
      "offset": 1642,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "gotten to that yet or no one has I think",
      "offset": 1644.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I mean even before you said this like it",
      "offset": 1646.52,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "was surprising to me that the random",
      "offset": 1648.84,
      "duration": 8.959
    },
    {
      "lang": "en",
      "text": "stopping worked at all like a and why is",
      "offset": 1653.52,
      "duration": 8.519
    },
    {
      "lang": "en",
      "text": "that better than just like you know just",
      "offset": 1657.799,
      "duration": 7.401
    },
    {
      "lang": "en",
      "text": "maximizing um during training I'm",
      "offset": 1662.039,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "maximizing the number of steps right uh",
      "offset": 1665.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "yeah that that's that's a good question",
      "offset": 1668.64,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "right because they have been like there",
      "offset": 1669.84,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "have been previous architectures for",
      "offset": 1671.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "example do not sure if you remember",
      "offset": 1672.96,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "something like",
      "offset": 1674.96,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Albert which was a we shared bird",
      "offset": 1676.159,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "variant so these and there's also some",
      "offset": 1680.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "some UT models that have this um where",
      "offset": 1683.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "basically they have recurrence but the",
      "offset": 1685.559,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "recurrence is always fixed it's always",
      "offset": 1688.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like you always do eight times or you",
      "offset": 1690.6,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "always do 16 times or something and",
      "offset": 1691.96,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "those also work but interestingly",
      "offset": 1694.559,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "um those models don't",
      "offset": 1697.799,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "really um generalize to fewer or more",
      "offset": 1700.44,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "steps of compute they really are they",
      "offset": 1703.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "really train like a fixed St depth",
      "offset": 1706.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Transformer where they lose performance",
      "offset": 1708.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "once they go for more steps and they",
      "offset": 1709.96,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "also don't work if they have fewer",
      "offset": 1712.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "steps and with this objective really try",
      "offset": 1714.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to to generalize to maybe really an",
      "offset": 1716.32,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "arbitrary number of",
      "offset": 1718.84,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "steps so the model is stable and can uh",
      "offset": 1720.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "really learn a um more mathematically",
      "offset": 1724.88,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "you can really learn a some fixed point",
      "offset": 1726.64,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "or some steady state",
      "offset": 1730.519,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "behavior and that's what this objective",
      "offset": 1733,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "is trying to get it to do",
      "offset": 1734.919,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "because otherwise you would maybe you",
      "offset": 1739.24,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "try you put in more computer test time",
      "offset": 1740.72,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and then it's it would get worse again",
      "offset": 1742.2,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "it would like Veer off track and go into",
      "offset": 1744.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some direction it wasn't trained",
      "offset": 1746.559,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "on and so like right because like",
      "offset": 1748.44,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "actually it really is this very",
      "offset": 1751.36,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "nonlinear dynamical system that we're",
      "offset": 1754.48,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "training here and we're trying to get it",
      "offset": 1756.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "to do what we want it to do and to",
      "offset": 1757.679,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "predict the next token and that's a bit",
      "offset": 1759.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's a bit tricky to",
      "offset": 1762.679,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "train and if it's for example if it's",
      "offset": 1764,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "fixed then it has this it behaves like",
      "offset": 1765.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "it's a fixed uh Transformer is there an",
      "offset": 1768.32,
      "duration": 7.839
    },
    {
      "lang": "en",
      "text": "interpretation of the paper that is",
      "offset": 1771.84,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "searching in Laten space for the next",
      "offset": 1776.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "token it's interesting it's interesting",
      "offset": 1779.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "way to look at it um in some ways yes",
      "offset": 1781.84,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "um especially because you if you think",
      "offset": 1785.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "about it um so what actually is",
      "offset": 1787.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "happening is that you have some initial",
      "offset": 1789.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "state in the recurrence and the initial",
      "offset": 1791.159,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "state is always",
      "offset": 1793.6,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "random and then we can also visualize",
      "offset": 1794.799,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "those and often theseand random States",
      "offset": 1797.32,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they lie on like some what if you for",
      "offset": 1798.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "example if you plot PCA",
      "offset": 1802.36,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "directions then you will see these",
      "offset": 1804.12,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "random States just somewhere in the",
      "offset": 1805.679,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "middle because they don't have a",
      "offset": 1806.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "significant component in any direction",
      "offset": 1807.519,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "so they sort of like lie in the middle",
      "offset": 1810.44,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "and then you see as the model recur it",
      "offset": 1812.799,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "keeps on like it keeps moving these",
      "offset": 1814.32,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "points away onto some Target position",
      "offset": 1815.76,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "that where it wants them at and where",
      "offset": 1818.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "then it can predict the next token from",
      "offset": 1819.679,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "them and we have these kind of funny",
      "offset": 1821.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "little",
      "offset": 1824.039,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "spine charts in the end of the paper",
      "offset": 1824.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "where you s like see this like middle",
      "offset": 1827.12,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "column rising and then you have these",
      "offset": 1829.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "trajectories going out at the end I",
      "offset": 1831.559,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "think this is somewhere in the appendix",
      "offset": 1833.399,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where you really see that like from like",
      "offset": 1835.279,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "these like from random it then converges",
      "offset": 1837.159,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "to this um or try or finds this point",
      "offset": 1841.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "from which to predict the next",
      "offset": 1844.799,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "token and what's also super exciting for",
      "offset": 1847.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "us there is that this is not always uh",
      "offset": 1849.919,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "just a point it doesn't always just",
      "offset": 1853.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "converge to a fixed point because we",
      "offset": 1854.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "also some actually also observe the",
      "offset": 1857.2,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "model actually rotating it has these",
      "offset": 1858.799,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "Little Orbits that it makes and it's",
      "offset": 1860.36,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "kind of interesting because then we have",
      "offset": 1862.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "these right it's still very hard for me",
      "offset": 1864.08,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "to actually visualize this 5,280",
      "offset": 1866.039,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "dimensional space so I only can show",
      "offset": 1869.519,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "like right I can only show these like",
      "offset": 1872.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "first 40 first 80 PCA directions or",
      "offset": 1873.72,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "something right but in those then we can",
      "offset": 1876.12,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "see like okay what is it doing it's like",
      "offset": 1878.32,
      "duration": 2.239
    },
    {
      "lang": "en",
      "text": "often like there's like some",
      "offset": 1879.679,
      "duration": 2.201
    },
    {
      "lang": "en",
      "text": "trajectories and they all come together",
      "offset": 1880.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "and then they do these little",
      "offset": 1881.88,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "circles and that's quite interesting",
      "offset": 1884.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "right because then like in later",
      "offset": 1886.519,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "tokens it can pick up on these circles",
      "offset": 1889.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "or these like these uh orbits that it",
      "offset": 1891.84,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "had done in earlier steps and they can",
      "offset": 1895.039,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "do some calculation with that because we",
      "offset": 1897.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "we do know that models often Implement",
      "offset": 1898.639,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "additional algorithms with these sort of",
      "offset": 1900.799,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "schemes where um you",
      "offset": 1903.6,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "have um you have orbits that reinforce",
      "offset": 1906.36,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "each other and it was quite interesting",
      "offset": 1910.159,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "for us right because it really was an",
      "offset": 1911.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "emerging behavior that it would",
      "offset": 1912.88,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Implement um some of these problems like",
      "offset": 1914.399,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "this",
      "offset": 1917.679,
      "duration": 2.161
    },
    {
      "lang": "en",
      "text": "it wasn't really trained to have these",
      "offset": 1919.88,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "Little Orbits but it just emerged from",
      "offset": 1921.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "training that this was a good solution",
      "offset": 1923.639,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "to predict the next tokens there's a",
      "offset": 1925,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "note in the paper that kind of Compares",
      "offset": 1927.32,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "this to diffusion models elaborate on",
      "offset": 1929.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "that a bit from from some perspective",
      "offset": 1933.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "this really is a diffusion model I mean",
      "offset": 1935.559,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "maybe this is a bit generic maybe",
      "offset": 1938.039,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "because all recurrent models are or",
      "offset": 1939.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "maybe both of them are uh recurrent",
      "offset": 1941.679,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "models diffusion models are also",
      "offset": 1944.48,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "recurrent models in a different way um",
      "offset": 1945.96,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "um it's important to separate that from",
      "offset": 1948.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "uh like there also been been some very",
      "offset": 1951.44,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "exciting language diffusion models",
      "offset": 1954.039,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "recently but this one's this one's a bit",
      "offset": 1956.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "if you think about this as a diffusion",
      "offset": 1959,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "model it would be that um you pick one",
      "offset": 1960.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "token the next token right and then you",
      "offset": 1964.399,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "diffuse all the way until you found the",
      "offset": 1966.559,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "next token and then you go one step",
      "offset": 1968.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "forward and you diffuse again until you",
      "offset": 1970.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "found the next token and you go one step",
      "offset": 1972.36,
      "duration": 3.159
    },
    {
      "lang": "en",
      "text": "forward you diffuse again you found the",
      "offset": 1974,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "next token this kind of analogy would",
      "offset": 1975.519,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "work here",
      "offset": 1977.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and um during development we actually",
      "offset": 1979.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "tried to make this a bit more explicit",
      "offset": 1980.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "because we also tried",
      "offset": 1983.039,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "to not only start from a noisy state but",
      "offset": 1984.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "also inject noise proportional to how",
      "offset": 1988.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "many states you've done how many steps",
      "offset": 1990.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you've done so far like you would in",
      "offset": 1991.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "diffus in a diffusion",
      "offset": 1994.24,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "model and this didn't really help doing",
      "offset": 1995.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "pre-training so we removed that part",
      "offset": 1998.12,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "again but it really shows this analogy",
      "offset": 2000.399,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "that um this recurrence is not so",
      "offset": 2002.96,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "different from a diffusion model would",
      "offset": 2005.519,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "diffuse the next",
      "offset": 2008.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "token what is quite different is that",
      "offset": 2009.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the objective the training objectives",
      "offset": 2011.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "very different in a diffusion model also",
      "offset": 2013.799,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "this is why discrete diffusion models or",
      "offset": 2016.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "language all bit hard is um ideally in a",
      "offset": 2019.519,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "diffusion model you would want this uh",
      "offset": 2023.039,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "process where you have an you have a a",
      "offset": 2025.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "continuous",
      "offset": 2028.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "Target and you add some noise to it and",
      "offset": 2029.559,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "then you learn this D noising",
      "offset": 2032,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "operation that's really how how large",
      "offset": 2034,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "scale image diffusion models are trained",
      "offset": 2036.12,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "and a language that's not so clear",
      "offset": 2038.399,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "because you can try to do noise",
      "offset": 2039.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "embeddings",
      "offset": 2040.96,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "but it's like not so nice to do that",
      "offset": 2043.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "there and so the training objective here",
      "offset": 2046.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "is still quite different from a",
      "offset": 2048.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "diffusion model but they are both",
      "offset": 2049.56,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "related models in that",
      "offset": 2052.079,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "um yeah they both diffused T token",
      "offset": 2055.72,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "that's way to say it okay so you",
      "offset": 2059,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "mentioned that you started with a 100",
      "offset": 2061.56,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "million I think parameter model as kind",
      "offset": 2063.679,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "of your your test model uh uh and then",
      "offset": 2065.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "you ultimately scaled that up to a three",
      "offset": 2068.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "and a half billion parameter model",
      "offset": 2070.56,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "um did that all happen in one shot or",
      "offset": 2073.679,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "was that an incremental process and",
      "offset": 2076.399,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "maybe talk a little bit about the the",
      "offset": 2077.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "process yeah what was kind of tough",
      "offset": 2080.879,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "about this was that ultimately this was",
      "offset": 2083.359,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "pretty much a one shot um because we had",
      "offset": 2086.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "a limited amount of compute on the",
      "offset": 2089.079,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "supercomputer and which we also had in",
      "offset": 2090.839,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "some little segments where right you",
      "offset": 2092.72,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "apply for in a queing system and then",
      "offset": 2094.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "you get some 12 hour slot sometime in",
      "offset": 2096.839,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "the year or sometime in the month right",
      "offset": 2099.92,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "and you kind of hope that it's running",
      "offset": 2102.56,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "when it's",
      "offset": 2104.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "running and um so it really was like we",
      "offset": 2105.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "only had these like little 100 million",
      "offset": 2109,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "parameter test models where lots of",
      "offset": 2110.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "things worked and then we're saying okay",
      "offset": 2113.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "now we're scaling it up and then there's",
      "offset": 2114.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "some um there's some first questions",
      "offset": 2117.119,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "like okay what's the data paraly",
      "offset": 2119.2,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "strategy right what's the model",
      "offset": 2120.599,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "parallelism strategy that we can do",
      "offset": 2121.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "there and there was was a bit hering",
      "offset": 2124.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "that was these um it was on AMD gpus so",
      "offset": 2127.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "we had a bit of issues and um so",
      "offset": 2129.599,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "actually the cluster wasn't really",
      "offset": 2133.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "there's some like notes that it's not",
      "offset": 2136.119,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "wasn't supposed to be to be running deep",
      "offset": 2137.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "learning workloads on more than 128",
      "offset": 2139.359,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "noes and there was something that like",
      "offset": 2142.68,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the the interconnect between the nodes",
      "offset": 2144.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "wasn't quite",
      "offset": 2146.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "stable and but we we kind of had this",
      "offset": 2149,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "allocation for 512 nodes so for 4,000",
      "offset": 2151.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "cards which was a bit required because",
      "offset": 2154.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "these are the 250X cards this the MD",
      "offset": 2156.2,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "equivalent of the A1 100s so these are",
      "offset": 2158.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "older generation cards so you need a bit",
      "offset": 2161.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "more of those to get maybe a similar",
      "offset": 2162.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "number of flops that you would have with",
      "offset": 2165.359,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "a few or more modern",
      "offset": 2166.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "cards and the cluster wasn't really like",
      "offset": 2168.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "set up to do this kind of thing and then",
      "offset": 2170.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "we ended up cooking this uh distributed",
      "offset": 2173.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "data parallel pipeline that pretty much",
      "offset": 2176.319,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "we hand wrote and then it only worked if",
      "offset": 2180.28,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "you send packages that are exactly 64",
      "offset": 2181.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "megabytes over the",
      "offset": 2183.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "interconnect and that ended up working",
      "offset": 2186.16,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and it was stable and it works for no",
      "offset": 2188.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "other like note count in 512 this",
      "offset": 2190.76,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "trading",
      "offset": 2192.839,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "script and it was a bit of a wacky",
      "offset": 2194.079,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "process to actually get there and to do",
      "offset": 2196.16,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "that and then right like ultimately we",
      "offset": 2197.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "we just uh threw out the pytorch DDP and",
      "offset": 2200.119,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "wrote in this like funny little uh",
      "offset": 2203.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "version of our own that only sends these",
      "offset": 2206.359,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "64 megabyte packages because that was",
      "offset": 2207.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "somehow optimal to like not overwhelm",
      "offset": 2210.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "the interconnect on these machines so",
      "offset": 2212.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "this was a whole side quest that took us",
      "offset": 2215.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "like several months to figure out how to",
      "offset": 2216.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "actually run with this allocation that",
      "offset": 2218.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "we were we were having right and we're",
      "offset": 2221,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "academics we're pretty happy about any",
      "offset": 2223.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "computer we have and we can make it work",
      "offset": 2224.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but it could it would take us some time",
      "offset": 2227.04,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to make it work is the implication that",
      "offset": 2228.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "reproducibility is going to be a",
      "offset": 2230.48,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "challenge for anyone that wants to try",
      "offset": 2232.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to do this on their own I think it's",
      "offset": 2234.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "much easier to produce an Nvidia cluster",
      "offset": 2236.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I think that's the easy way to say it",
      "offset": 2237.76,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "also like AMD has been like doing a lot",
      "offset": 2239.64,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "of improvements and the new series is a",
      "offset": 2240.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "bit better and uh so I don't want to say",
      "offset": 2242.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "like this wouldn't be reproducible on an",
      "offset": 2244.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "AMD cluster but on this particular",
      "offset": 2245.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "machine this script will reproduce it",
      "offset": 2248.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "but I think like some of these problems",
      "offset": 2251.599,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "are really also related to the",
      "offset": 2253.04,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "interconnect on this machine and to the",
      "offset": 2254.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "scaling but yeah we we've open sourced",
      "offset": 2255.68,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "all the code that ran on this in this",
      "offset": 2257.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "cluster and we have all everything we",
      "offset": 2259.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "also have the model in the open we have",
      "offset": 2260.839,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "all the checkpoints do you have a sense",
      "offset": 2261.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "for what the cost to reproduce would be",
      "offset": 2264.319,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "like on if you were to reproduce it on",
      "offset": 2267.96,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "uh you know Nvidia instances and Cloud I",
      "offset": 2271.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "I don't actually have it off the top of",
      "offset": 2274.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "my head um but basically we we trained",
      "offset": 2275.96,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "on like about 20 segments of 12 hours so",
      "offset": 2278.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "10",
      "offset": 2282.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "days 10 days on",
      "offset": 2283.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "4,000",
      "offset": 2286.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "cards maybe you You' convert these cards",
      "offset": 2287.72,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "to like from 4,000 to 2, A1 100s to",
      "offset": 2290.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "1,000 h100s very",
      "offset": 2293.119,
      "duration": 7.641
    },
    {
      "lang": "en",
      "text": "naively maybe 10 days on a th h100s not",
      "offset": 2297.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "a small probably you could get more",
      "offset": 2300.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "computer out of the h100s but as a buar",
      "offset": 2303.2,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "I make like not a little amount of",
      "offset": 2305.92,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "computer",
      "offset": 2307.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "um right",
      "offset": 2308.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "um and like the model ultimately in the",
      "offset": 2310.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "end is also like",
      "offset": 2313.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "a it's not a terrible model which",
      "offset": 2315.4,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "actually was surprising to us or to mebe",
      "offset": 2317.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "to most of it because um but like it was",
      "offset": 2319.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "like was still like a super small team",
      "offset": 2322.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "right we have like a few people that",
      "offset": 2323.68,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "know the",
      "offset": 2325.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "HBC and then we just kind of built this",
      "offset": 2325.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "model right and this is the first this",
      "offset": 2327.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "was the only training run and also we",
      "offset": 2329.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "had some idea what the data should be to",
      "offset": 2332.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "be",
      "offset": 2334.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "good and that was just because I picked",
      "offset": 2335.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that and I said okay let's train on this",
      "offset": 2338.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but there was like no computer to train",
      "offset": 2340.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "maybe a twin model on a different data",
      "offset": 2342,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "mixture or to really figure this out and",
      "offset": 2343.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "what data did you use so this is like a",
      "offset": 2346.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "so really like um the first",
      "offset": 2349.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "consideration that we had when we when",
      "offset": 2350.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "we made data set was that we wanted",
      "offset": 2353.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "something where we could hedge a bit",
      "offset": 2355.28,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "where the model would be good",
      "offset": 2356.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "at so we put in like a larger amount of",
      "offset": 2358.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "code a larger amount of latch some chess",
      "offset": 2360.8,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "puzzles some some math but also a lot of",
      "offset": 2362.839,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "web text some instr C data just try to",
      "offset": 2366.359,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "see like okay where would the strength",
      "offset": 2369.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of this model be and then that's the",
      "offset": 2370.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "kind of mix we trained on ultimately",
      "offset": 2372.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "mixes very heavily on on code and map",
      "offset": 2374.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "but also a good amount of synthetic data",
      "offset": 2377.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "and instruction data this was",
      "offset": 2379.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "pre-trained it was trained from scratch",
      "offset": 2381.56,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "as opposed to tune yeah right because",
      "offset": 2383.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "this this approach is so different it",
      "offset": 2387.24,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "really had to be trained from scratch to",
      "offset": 2389.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "do this interestingly there have been a",
      "offset": 2390.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "few other papers that um also try to get",
      "offset": 2392.4,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "this this latent reasoning idea where",
      "offset": 2395.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that you could do more there they try to",
      "offset": 2398.119,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "get that from retrofitting fixed depth",
      "offset": 2400.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Transformers which also is interesting",
      "offset": 2404.48,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "right and especially for the like um",
      "offset": 2406.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "there's like one paper from this out of",
      "offset": 2408.2,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "Google and one out of meta it's very",
      "offset": 2409.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "enticing for the companies if you",
      "offset": 2412.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "already have your large fixed depth",
      "offset": 2413.28,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "Transformer can you retrofit this",
      "offset": 2416.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "capability into it but for us we really",
      "offset": 2418.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "want to see okay can we actually train",
      "offset": 2420.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "from the ground up for this model to be",
      "offset": 2422.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "recurrent and to do more compute for",
      "offset": 2425,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "hard problems",
      "offset": 2426.839,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "and in the abstract you kind of compared",
      "offset": 2428.44,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "the results to a 50 billion parameter",
      "offset": 2432.2,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "model what exactly do you mean there",
      "offset": 2435.68,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "yeah so there it's always important that",
      "offset": 2438.16,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "we um right so a good way to think about",
      "offset": 2440.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "the model is that it actually only has",
      "offset": 2443.319,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "eight layers so has like two layers",
      "offset": 2445.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "before the recurrence and it has four",
      "offset": 2449,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "layers in the recurr that are being",
      "offset": 2450.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "repeated and then two layers at the end",
      "offset": 2451.92,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "that produ the final next next token",
      "offset": 2453.88,
      "duration": 8.959
    },
    {
      "lang": "en",
      "text": "um so this whole thing is just 3.5 B",
      "offset": 2457.839,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "parameters but now like how do",
      "offset": 2462.839,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "we but but actually if we run this model",
      "offset": 2465.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "for more steps and say we run it for 32",
      "offset": 2467.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "steps we actually run it we actually R",
      "offset": 2470.76,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the middle part much longer right so it",
      "offset": 2473.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "would be as if we had run it for",
      "offset": 2475.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "something like 50 billion",
      "offset": 2478.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "parameters we actually we still have",
      "offset": 2480.2,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "only have 3.5",
      "offset": 2482,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "but the actual Cal calculation is",
      "offset": 2483.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "somewhat that we have we have",
      "offset": 2486.079,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "1.5 billion so like half of them are",
      "offset": 2487.839,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "fixed",
      "offset": 2491.04,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "parameters or 1 billion is fixed",
      "offset": 2492.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "parameters and 1.5 billion is Rec",
      "offset": 2494.119,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "parameters and so that's like in compute",
      "offset": 2497.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it still takes as much time to um to get",
      "offset": 2499.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "the next answer as if it was a 50",
      "offset": 2502.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "billion parameter",
      "offset": 2504.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "model but it still is a tiny 2.5 billion",
      "offset": 2505.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "parameter model and we compare it like",
      "offset": 2508.319,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "we actually see that um in benchmarks",
      "offset": 2510.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "it's quite good it compares to like the",
      "offset": 2513,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the other open source models like Theo",
      "offset": 2515.04,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "series we actually very happy that we",
      "offset": 2516.52,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "could CAU up catch up to the omo V1",
      "offset": 2518.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "models and like",
      "offset": 2520.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "v1.5 so that was quite exciting because",
      "offset": 2523.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "this models were trained with a much",
      "offset": 2524.96,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "much larger much more capable team I I",
      "offset": 2527.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "would say right a very good open source",
      "offset": 2529.319,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "team um right ultimately this is we just",
      "offset": 2531.16,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "a bunch of guys in the basement right um",
      "offset": 2534.76,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so uh we're pretty happy that we could",
      "offset": 2537.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "catch up to to the V1 version of that",
      "offset": 2539.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "model so like where they were like last",
      "offset": 2541.28,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "March we couldn't catch up to the V2",
      "offset": 2544.52,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "version because they also improve a lot",
      "offset": 2546.599,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "and uh but what's interesting is that",
      "offset": 2548.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "for them like I think a lot of the",
      "offset": 2550.4,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "improvements also were in data so I",
      "offset": 2551.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think that makes us very hopeful that",
      "offset": 2553.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "maybe the approach is not fundamentally",
      "offset": 2555.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "limited but with better data with more",
      "offset": 2556.68,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "care with a different like a post",
      "offset": 2560.319,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "training or mid training schedule it",
      "offset": 2562.559,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "could be better so that was very",
      "offset": 2563.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "motivating for us to see those those",
      "offset": 2565.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "numbers at the end yeah but it's not a",
      "offset": 2566.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "50 billion parameter model in comp in",
      "offset": 2569.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "it's not like it's a llama 70b model",
      "offset": 2571.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "it's not nearly as good right because",
      "offset": 2573.8,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "ultimately it's trained for much less",
      "offset": 2575.64,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "compute still is it part of the",
      "offset": 2577.24,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "implication then that as model",
      "offset": 2579.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "architectures evolve and incorporate",
      "offset": 2582.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "ideas like this the idea of comparing",
      "offset": 2585.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "you know models by parameters is going",
      "offset": 2587.64,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "to be less useful I think so but like",
      "offset": 2589.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "for us even this was already uh super",
      "offset": 2592.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "tough to compare in parameters that the",
      "offset": 2595.24,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "model is actually smaller than all the",
      "offset": 2596.48,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "other ones we compared",
      "offset": 2597.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "to but in compute for example like uh",
      "offset": 2599.599,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "for some of the benchmarks uh how do we",
      "offset": 2602.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "compare this if the model is smaller",
      "offset": 2604.599,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "but we spend more compute on getting the",
      "offset": 2607.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "answer compared to this larger models",
      "offset": 2609.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "that spend less compute because and",
      "offset": 2611.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "right the same thing also happens if",
      "offset": 2614.44,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "with the other reasoning models right",
      "offset": 2616,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "it's like a we don't like for example",
      "offset": 2618.24,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "the R1 model maybe it's like 64 billion",
      "offset": 2621,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "active",
      "offset": 2624.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "parameters but do you count so do is it",
      "offset": 2625.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "a 64 billion parameter active parameter",
      "offset": 2627.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "model or is it like a a thousand tokens",
      "offset": 2629.8,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "of Chain of Thought times 64 billion",
      "offset": 2633.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "active parameter model",
      "offset": 2636.079,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "it's not so clear how to do this",
      "offset": 2637.72,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "calculation yeah it becomes somehow less",
      "offset": 2638.68,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "meaningful what's also pretty pretty",
      "offset": 2640.559,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "interesting is that um wait so so one",
      "offset": 2642.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "thing we can do is we could like convert",
      "offset": 2645.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "in pre-training flops to like what model",
      "offset": 2646.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "size could we have also trained for with",
      "offset": 2650.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "that compute budget exactly right and",
      "offset": 2652.4,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that comes up that's that comes up to",
      "offset": 2655.079,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "something like a 32 billion parameter",
      "offset": 2656.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model in ra flops so also not small for",
      "offset": 2658.119,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like a similar token count um but",
      "offset": 2661.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "interestingly we couldn't have trained a",
      "offset": 2664.119,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "32 billion parameter",
      "offset": 2665.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "uh noral Transformer on this on this uh",
      "offset": 2667.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "node setup",
      "offset": 2670.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "actually because what because this",
      "offset": 2672.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "because the model is so much smaller",
      "offset": 2674.48,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "it's so much more communication",
      "offset": 2676.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "efficient because we actually there's",
      "offset": 2678.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually less state to send to the other",
      "offset": 2680.119,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "devices and so it's kind of a funny",
      "offset": 2682.28,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "thing that with this cluster actually we",
      "offset": 2684.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "couldn't have trained a 32 billion",
      "offset": 2685.68,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "parameter model even though that would",
      "offset": 2686.8,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "be flop",
      "offset": 2688.16,
      "duration": 2.919
    },
    {
      "lang": "en",
      "text": "equivalent and so these comp these",
      "offset": 2689.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "comparisons is becoming even harder in",
      "offset": 2691.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the end we just have a table in the",
      "offset": 2693.8,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "paper with different models that are",
      "offset": 2694.839,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "also open source",
      "offset": 2696.359,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "and we say here you can pick your best",
      "offset": 2697.599,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "comparison from the table did you find",
      "offset": 2699.319,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "that the model performed uh better or",
      "offset": 2701.559,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "worse for in particular domains was",
      "offset": 2705.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "there any kind of domain affinity for",
      "offset": 2708.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "the model the model is surprisingly good",
      "offset": 2709.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "at reasoning like it really is like this",
      "offset": 2712.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "is maybe it's just the headline but",
      "offset": 2714.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "actually in the benchmarks especially on",
      "offset": 2716.04,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "um on graduate math not graduate ma not",
      "offset": 2718.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "on graduate math on grade school math",
      "offset": 2721.839,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "right uh tiny tiny big difference yeah",
      "offset": 2724.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "right gsmk is a classical grade school",
      "offset": 2727.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "math problem and on also on human eval",
      "offset": 2730.04,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "which is the coding right these are",
      "offset": 2732.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "nowadays easy coding questions there the",
      "offset": 2734.4,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "model is particularly good and it",
      "offset": 2736.92,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "actually is better than some of the",
      "offset": 2738.52,
      "duration": 2.599
    },
    {
      "lang": "en",
      "text": "other open source models by the other",
      "offset": 2739.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "teams and what's there particularly",
      "offset": 2741.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "exciting is that um so while we had",
      "offset": 2743.16,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "limited",
      "offset": 2745.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "compute we did train a baseline",
      "offset": 2746.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "comparison model where we trained the",
      "offset": 2748.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "exact same architecture the exact same",
      "offset": 2750.559,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "data the exact same cluster but we train",
      "offset": 2752.359,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "it to always have a recurrence of one",
      "offset": 2754.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and we so that would be a fixed depth",
      "offset": 2758.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "just comparison right the model has that",
      "offset": 2760.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "model has the same number of parameters",
      "offset": 2762.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and we trained it just the same way and",
      "offset": 2764.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "we trained that one for 10,000 steps",
      "offset": 2767.119,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "which is about 180 billion parameters",
      "offset": 2769.24,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "and interestingly that fixed depth",
      "offset": 2772.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "version got like a g8k score of 1.5% or",
      "offset": 2774.319,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "something and at 180 billion tokens the",
      "offset": 2778.839,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "recurrent model already was at 10 to 12%",
      "offset": 2782.16,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "of gsmk was like five times better",
      "offset": 2785.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "at grade school math right same number",
      "offset": 2787.559,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "of parameters just a different way of",
      "offset": 2789.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "using them and it was drastically better",
      "offset": 2791.839,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "at at uh at Great school math which was",
      "offset": 2794.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "pretty",
      "offset": 2796.599,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "exciting and um right like still the",
      "offset": 2797.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "comparison is hard but we haven't",
      "offset": 2801.04,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "trained the Baseline all the way to the",
      "offset": 2802.079,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "end so some of these comparisons still",
      "offset": 2804.68,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "are pretty hard because ultimately we",
      "offset": 2806.68,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "have this one data point right this is",
      "offset": 2808.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "the one model we",
      "offset": 2809.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "trained so as a scientist I was like I",
      "offset": 2812.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "always I'm a bit like I don't want to",
      "offset": 2814.96,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "see like",
      "offset": 2816.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "too much here right and I'd say this one",
      "offset": 2817.64,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "will always be great but it was",
      "offset": 2819.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "surprisingly good at code and math uh",
      "offset": 2821.559,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "there's a section in the paper where you",
      "offset": 2823.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "talk about recurrent depth simplifying",
      "offset": 2824.88,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "llms elaborate on that point a a bit",
      "offset": 2828.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "yeah so what we find pretty interesting",
      "offset": 2831.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "is that this model",
      "offset": 2834,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "um or what we want to really want to",
      "offset": 2836.44,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "show in this section is that this is a",
      "offset": 2838.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "very natural way to use Transformers so",
      "offset": 2840,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "leaving all the thinking and reasoning",
      "offset": 2843.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "all this like this like more lofty",
      "offset": 2845.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "motivations aside we think it's also a",
      "offset": 2847.319,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "practical architecture and there's like",
      "offset": 2849.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "several examples in that section about",
      "offset": 2851.559,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "why it's a practical architecture so",
      "offset": 2853.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we've talked about these adaptive exits",
      "offset": 2855.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "already right because this is something",
      "offset": 2857.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "that you can also do in a normal",
      "offset": 2859.119,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "Transformer and people have tried this",
      "offset": 2860.559,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "right people have try it like um if you",
      "offset": 2862.52,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "have 96 layers maybe on some tokens you",
      "offset": 2864.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "only use the first eight or",
      "offset": 2866.8,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "something and then they get into all",
      "offset": 2868.88,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "these okay how do you fine tune for this",
      "offset": 2870.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "how do you make this happen and it's a",
      "offset": 2872.079,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "big effort to find tune the model for",
      "offset": 2873.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that and then we show okay actually this",
      "offset": 2874.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "different architecture can just do a",
      "offset": 2877.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "zero shot can just exit and something",
      "offset": 2878.88,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "similarly this also holds for things",
      "offset": 2882.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "like a speculative decoding for",
      "offset": 2884.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "example I think you also had you you had",
      "offset": 2886.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "a you had a guest a few weeks ago but",
      "offset": 2889.4,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "spective decoding right mhm I talked",
      "offset": 2890.599,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "about it with Chris lot at Qualcomm",
      "offset": 2893.839,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "quite a bit yeah right so to recap that",
      "offset": 2896.04,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "slightly for spective decoding ideally",
      "offset": 2900.079,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "you need this little draft",
      "offset": 2902.359,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "model and it kind of needs to be aligned",
      "offset": 2903.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "with the with the blouch model and it",
      "offset": 2906,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "needs to fit and you need to really make",
      "offset": 2907.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "that work and do some engineering and",
      "offset": 2909.4,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "there was a lot of good research being",
      "offset": 2910.64,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "done to make that",
      "offset": 2912.24,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "work but what's kind of interesting",
      "offset": 2913.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "about this recurrent architecture is",
      "offset": 2916.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "that just Naturally by just running it",
      "offset": 2918.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "for one step or four",
      "offset": 2919.64,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "steps you naturally just have your own",
      "offset": 2921.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "draft model in your back",
      "offset": 2923.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pocket right you can pretty much just",
      "offset": 2925.48,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "you just run the model for like a small",
      "offset": 2927.28,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "number of steps to just draft and then",
      "offset": 2928.48,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "you can still verify with 64 steps",
      "offset": 2931.48,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "later and it just happen like that right",
      "offset": 2934.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "there no fine tuning necessary no draft",
      "offset": 2937.28,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "model making Necessary",
      "offset": 2939.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "just It just fits like that",
      "offset": 2941.2,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "right or something related to that is um",
      "offset": 2943.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "there also this idea that you maybe",
      "offset": 2947.16,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "you'd want to do KV cach uh sharing",
      "offset": 2948.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "where you have several layers that that",
      "offset": 2951.599,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "share the same KV cache so you can push",
      "offset": 2953.2,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "down in the KV",
      "offset": 2954.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "cache and in normal Transformers that's",
      "offset": 2956.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "a bit hard because all the layers have",
      "offset": 2958.16,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "different um k&amp;v projection",
      "offset": 2961.44,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "matrices so so the KV caches don't",
      "offset": 2965.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "really match to each",
      "offset": 2968.48,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "other but this model because it's just a",
      "offset": 2970.079,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "recurrence it's always the same",
      "offset": 2972.119,
      "duration": 2.44
    },
    {
      "lang": "en",
      "text": "parameters are being",
      "offset": 2973.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "used you can just naturally share KV",
      "offset": 2974.559,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "caches between steps if you wanted to",
      "offset": 2977.119,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "does the fact that the parameters are",
      "offset": 2980.52,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "the same also have an impact on like",
      "offset": 2982.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "memory bandwidth between um memory",
      "offset": 2984.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "bandwidth during inference basically yes",
      "offset": 2987.079,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "because the so the model is much smaller",
      "offset": 2989.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "right you can run it on a smaller chip",
      "offset": 2991.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "and you can just run it longer on the",
      "offset": 2994.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "smaller chip that for the it's pretty",
      "offset": 2995.319,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "great we kind of wanted it or maybe",
      "offset": 2997.48,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "maybe not wanted it but um maybe a",
      "offset": 3000.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "different thing you could want is that",
      "offset": 3002.92,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "the entire recurrence fits into like an",
      "offset": 3005.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "L1 cache but it's a bit too large for",
      "offset": 3007.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that but if it would then it would be",
      "offset": 3010.24,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "even",
      "offset": 3012.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "faster but but right now still you have",
      "offset": 3013.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "uh you have about a billion parameters",
      "offset": 3015.76,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "in the recurrence that you keep on",
      "offset": 3017.88,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "moving out of the L2 into the L1",
      "offset": 3020.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "so there's still some movement there but",
      "offset": 3022.96,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "overall you need much less memory on the",
      "offset": 3025.96,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "device to actually store the model which",
      "offset": 3028.119,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "we think is also pretty exciting for for",
      "offset": 3030.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "some low resource",
      "offset": 3031.96,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "applications and just for having just",
      "offset": 3033.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "for logging you know lobing fewer",
      "offset": 3036.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "parameters around instead of having",
      "offset": 3038.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "these massive models that we we where we",
      "offset": 3040.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "use every parameter only once right and",
      "offset": 3043.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "then we throw it",
      "offset": 3044.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "away it's quite an interesting Paradigm",
      "offset": 3045.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "that right that we",
      "offset": 3047.96,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "actually right like we really try to",
      "offset": 3049.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "reuse parameters and um",
      "offset": 3051.44,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "relating to these uh right natural where",
      "offset": 3057.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "this application that this is natural",
      "offset": 3060.68,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "for the",
      "offset": 3062,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "LM there I think there also there are",
      "offset": 3063.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "other like ways to use this in",
      "offset": 3065.16,
      "duration": 2.439
    },
    {
      "lang": "en",
      "text": "interesting ways that we just haven't",
      "offset": 3066.64,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "figured out yet and these are just like",
      "offset": 3067.599,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "some examples that we had the section",
      "offset": 3069.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "that I thought were pretty pretty",
      "offset": 3071.079,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "interesting but it's it's a bit of right",
      "offset": 3073.559,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "it's a different architecture it has",
      "offset": 3075.839,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "these different advantages that we",
      "offset": 3076.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "weren't we aren't maybe so aware of even",
      "offset": 3078.72,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "like wa like putting away all the",
      "offset": 3080.64,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "reasoning",
      "offset": 3081.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "aside this might just be more efficient",
      "offset": 3083.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "in some applications",
      "offset": 3085.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interesting interesting did you have any",
      "offset": 3087.76,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "interesting like qualitative",
      "offset": 3090.24,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "observations about the generations like",
      "offset": 3091.88,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "are they you know fairly standard or you",
      "offset": 3095.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "know did they differ in some qualitative",
      "offset": 3098.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "way than what you might expect for",
      "offset": 3100.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "similar",
      "offset": 3101.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "models similar in in some metric size or",
      "offset": 3103,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "something what was interesting was that",
      "offset": 3106.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "um this also a funny Quirk is that we",
      "offset": 3109.28,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "actually trained the model with",
      "offset": 3112.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "instruction data so with template with",
      "offset": 3115.16,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "chat templates in the",
      "offset": 3116.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "pre-training right which is a some other",
      "offset": 3118.119,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "people have also done that it's a bit",
      "offset": 3120.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "but it's a bit more of a rare thing to",
      "offset": 3122.16,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "do and we kind of did this because we",
      "offset": 3125.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "really also didn't have compute for like",
      "offset": 3128.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "a longer uh post training or cool down",
      "offset": 3129.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "phase but what it implies is that pretty",
      "offset": 3132.88,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "much the model learns the chat template",
      "offset": 3135.4,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "from the",
      "offset": 3136.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "get-go and you can just you can just you",
      "offset": 3138.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "can just chat with all these",
      "offset": 3140.359,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "intermediate checkpoints already kind of",
      "offset": 3141.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a fun thing we think about right you can",
      "offset": 3143.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "just chat with the 10,000 step chat and",
      "offset": 3145.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "uh state of the model and see how it's",
      "offset": 3149.119,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "doing and how it's understanding some",
      "offset": 3150.76,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "questions and you can also chat with the",
      "offset": 3153.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "later",
      "offset": 3154.559,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "ones right because it just inherently",
      "offset": 3155.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "understands this chat template format no",
      "offset": 3158.28,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "without any like post training which",
      "offset": 3160.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which is like similar this is a bit of",
      "offset": 3163.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "orthogonal to the recurrence right I it",
      "offset": 3164.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probably relates more to the data",
      "offset": 3167.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "mixture you know so this this work is",
      "offset": 3168.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "focused on efficiency",
      "offset": 3171.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "primarily you also spend a lot of time",
      "offset": 3173.76,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "thinking about um model safety you know",
      "offset": 3176.079,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "it strikes me that there are some",
      "offset": 3180.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "interesting implications in a model like",
      "offset": 3181.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "thinking to itself as opposed to",
      "offset": 3185.24,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Thinking Out Loud um you know via text",
      "offset": 3186.88,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "tokens uh have you thought much about",
      "offset": 3189.559,
      "duration": 8.841
    },
    {
      "lang": "en",
      "text": "that yeah a lot actually um but like",
      "offset": 3192.24,
      "duration": 9.319
    },
    {
      "lang": "en",
      "text": "uh so",
      "offset": 3198.4,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "um I think the first answer like why I",
      "offset": 3201.559,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "think this is not is maybe why I think",
      "offset": 3204.119,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "this is not worse which I also talked",
      "offset": 3206,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "with colleagues about right like um and",
      "offset": 3208.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "there maybe the interesting answer is",
      "offset": 3210.68,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "that",
      "offset": 3211.839,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "uh I already didn't think that uh",
      "offset": 3213.24,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "language models think in text so for me",
      "offset": 3216.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "this was a very small steps because if",
      "offset": 3219.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you think about it um if you have a very",
      "offset": 3221.48,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "long Chain of Thought the model",
      "offset": 3223.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "actually I mean of course it takes these",
      "offset": 3226.559,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "these text um intermediate steps go in",
      "offset": 3229.4,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "as text",
      "offset": 3231.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "inputs but actually mostly the model",
      "offset": 3232.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "attends to its own",
      "offset": 3235,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "uh internal representations of these",
      "offset": 3237.52,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "previous",
      "offset": 3239.079,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "tokens but if you think about the",
      "offset": 3240.28,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "attention and all the layers you are",
      "offset": 3242,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "already attending to these very deep",
      "offset": 3244.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "representations of these",
      "offset": 3246,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "tokens and you sort of like already",
      "offset": 3248.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "don't know what's in these states so",
      "offset": 3250.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "text is kind of a reflection of some",
      "offset": 3252.559,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "opaque uh you know Laten space anyway",
      "offset": 3255.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "and so the thinking isn't really",
      "offset": 3258.599,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "happening in you know in asy it's",
      "offset": 3261.119,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "happening in that domain yeah",
      "offset": 3263.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "right the model is like prompted with a",
      "offset": 3267,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "new with a new um token and it does",
      "offset": 3269.28,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "incorporate that into its into its like",
      "offset": 3272.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "hidden",
      "offset": 3274.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "States but it already Things based on",
      "offset": 3275.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "its previous hidden",
      "offset": 3277.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "States and with this model maybe this is",
      "offset": 3279.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "just a bit more",
      "offset": 3282.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "explicit um but what I also think is",
      "offset": 3284.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "like well like why I think this is",
      "offset": 3287.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "interesting is that um I do think it's a",
      "offset": 3288.4,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "question of like how do we oversee these",
      "offset": 3291.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "models and um to some extent I also want",
      "offset": 3293.079,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "some of these be some of these like",
      "offset": 3296.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "developments to happen in the open and",
      "offset": 3298.64,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "to happen this open source and this was",
      "offset": 3300.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "part of the motivation also why we open",
      "offset": 3303,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "source this particular way of doing it",
      "offset": 3304.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because ultimately like I also think",
      "offset": 3307.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "it's a very natural way of doing this",
      "offset": 3309.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like I think if we wouldn't have done it",
      "offset": 3312,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "would have",
      "offset": 3314.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "happened and so for me this is also a",
      "offset": 3315.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "way to say okay here is an open source",
      "offset": 3317.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "version of this model here's how it",
      "offset": 3319.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "works let's try to understand it and",
      "offset": 3321.559,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "that's also what I'm really interested",
      "offset": 3323.48,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "in right now is trying to understand it",
      "offset": 3324.48,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "now that we have it",
      "offset": 3325.72,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "and is that the kind of focus for you",
      "offset": 3327.16,
      "duration": 8.399
    },
    {
      "lang": "en",
      "text": "and and the group going forward is it to",
      "offset": 3333.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "evolve the model or there specific uh",
      "offset": 3335.559,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "experiments or increments incremental",
      "offset": 3338.079,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "directions that you're heading to or is",
      "offset": 3340.319,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "it more like we have this now let's try",
      "offset": 3342.039,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "to understand it better or you know",
      "offset": 3344.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "probably all of the above to some degree",
      "offset": 3346.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "but yeah it's really both of the above",
      "offset": 3348.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "actually my the most immediate thing is",
      "offset": 3351.319,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that um the model actually is not even",
      "offset": 3352.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "uh cooled down",
      "offset": 3355.599,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "um right just because we actually don't",
      "offset": 3357.24,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "have any Compu anymore that's the very",
      "offset": 3359,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "real answer why it do not cool down and",
      "offset": 3360.88,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "what does that mean for example like for",
      "offset": 3362.92,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "the latero models um we see the models",
      "offset": 3364.44,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "improve a lot with this mid training",
      "offset": 3367.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "phase where the model is slowly cooled",
      "offset": 3368.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "down from its peak learning rate to a",
      "offset": 3370.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "zero learning rate using a very high",
      "offset": 3372.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "quality data",
      "offset": 3375.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "mix so for me like on the first part",
      "offset": 3376.76,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "thing on the agenda is to try some data",
      "offset": 3379.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "mixes for example maybe tailor more",
      "offset": 3382.359,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "towards code more towards math and",
      "offset": 3383.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "you'll see what these cool down",
      "offset": 3385.96,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "behaviors will look like for this kind",
      "offset": 3386.839,
      "duration": 1.881
    },
    {
      "lang": "en",
      "text": "of",
      "offset": 3387.88,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "model and that's already interesting",
      "offset": 3388.72,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "right because you can also play around",
      "offset": 3390.52,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "with the parameters for example you",
      "offset": 3391.64,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "could imagine that you could cool down",
      "offset": 3392.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "with more steps that you did during",
      "offset": 3394.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "pre-training so you could incentivize to",
      "offset": 3396.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "think deeper during the cool down",
      "offset": 3398.68,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "phase but there's also some other",
      "offset": 3402,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "questions on that we have on the team of",
      "offset": 3404,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "like how would you even postra this kind",
      "offset": 3405.839,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "of model how would you do you know how",
      "offset": 3408.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "now we become all the way back around",
      "offset": 3411.4,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "yeah I was going to ask that what like",
      "offset": 3413.319,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "what does fine tuning mean here yeah",
      "offset": 3414.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "right or now that we've seen R1 like",
      "offset": 3416.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "what's the equivalent here how would you",
      "offset": 3419.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "reinforcement learn for this kind of",
      "offset": 3422.16,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "model architecture for this kind of",
      "offset": 3424.4,
      "duration": 2.679
    },
    {
      "lang": "en",
      "text": "Paradigm",
      "offset": 3425.48,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "right and these are questions that are",
      "offset": 3427.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "in the room aside from questions of how",
      "offset": 3429.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "do we what is it doing internally right",
      "offset": 3431.76,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "how can we understand how can we probe",
      "offset": 3433.52,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "what this model is doing",
      "offset": 3435.16,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "internally yeah so these are all things",
      "offset": 3436.839,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "we were thinking about but it's all",
      "offset": 3438.72,
      "duration": 1.879
    },
    {
      "lang": "en",
      "text": "pretty",
      "offset": 3439.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "exciting it's very cool stuff and I'm",
      "offset": 3440.599,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "glad we were able to connect and uh chat",
      "offset": 3442.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "about it a bit yeah I'm also happy to",
      "offset": 3445.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "feel feel questions has like like write",
      "offset": 3448.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "email or anything like this is like I've",
      "offset": 3451.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "really been thinking a lot about this",
      "offset": 3453.48,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "model and about you know how to do it",
      "offset": 3454.4,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "how to train it and all trials and",
      "offset": 3456.16,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "tribulations so I'm always happy to",
      "offset": 3457.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "answer more questions about this awesome",
      "offset": 3459.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "awesome thanks so much",
      "offset": 3462.359,
      "duration": 4.271
    },
    {
      "lang": "en",
      "text": "yianis yeah thank you for your time",
      "offset": 3463.88,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3466.63,
      "duration": 3.049
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3484.16,
      "duration": 3.19
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.635Z"
}