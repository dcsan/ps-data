{
  "episodeId": "J-g4Kq9CUdc",
  "channelSlug": "@twimlai",
  "title": "Accelerating AI Training and Inference with AWS Trainium2 with Ron Diamant - 720",
  "publishedAt": "2025-02-24T18:35:13.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "s Tropic is an amazing customer to work",
      "offset": 0.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "with project trainer we're building a",
      "offset": 2.24,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "gigantic training cluster that fully",
      "offset": 5.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Embraces the scaling laws that cluster",
      "offset": 8.599,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "will have many hundreds of thousands of",
      "offset": 11.44,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "training to devices that cluster is at",
      "offset": 13.88,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "least 5x larger than antropix previous",
      "offset": 17.279,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "training cluster and they're planning to",
      "offset": 21.48,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "train the largest and most intelligent",
      "offset": 24.119,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "Frontier Model on top of that cluster",
      "offset": 27.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "all right everyone welcome to another",
      "offset": 42.879,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "episode of the twiml AI podcast I am",
      "offset": 44.52,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "your host Sam charington today I'm",
      "offset": 46.92,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "joined by Ron diamont Ron is Chief",
      "offset": 49.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "Architect for trinium at Amazon web",
      "offset": 51.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "services Ron welcome to the podcast",
      "offset": 53.84,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "thanks for having me Sam I'm super",
      "offset": 56.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "excited to have you on the show I'm",
      "offset": 58.719,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "looking forward to our chat you joined",
      "offset": 60.44,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "AWS as part of the anero labs",
      "offset": 62.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "acquisition and we're connecting at a",
      "offset": 65.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pretty big moment for the anera team",
      "offset": 67.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "including the 10th anniversary of that",
      "offset": 69.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "acquisition the train M2 launch at last",
      "offset": 71.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "year's reinvent and some pretty big",
      "offset": 73.92,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "customer announcements with apple and",
      "offset": 76,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "anthropic we'll be digging into all of",
      "offset": 78.28,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "these and more and I'd love to have you",
      "offset": 80.759,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "get us started by sharing a bit about",
      "offset": 83.479,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "your background sure yes so first of all",
      "offset": 85.159,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "thanks again for having me today um I",
      "offset": 87.32,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "joined an nauna Labs uh in 2011 uh just",
      "offset": 89.92,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "a few weeks after it was founded very",
      "offset": 93.439,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "early on um and since then I've been",
      "offset": 95.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "fortunate to to participate in all of",
      "offset": 97.88,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "the different product lines that we",
      "offset": 99.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "built in analum h that includes Nitro",
      "offset": 101.28,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "where we basically offloaded the",
      "offset": 105,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "hypervisor from the host CPU to",
      "offset": 107.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "Dedicated chips and we we reinvented how",
      "offset": 110,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "virtualization in the cloud happens then",
      "offset": 113.88,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "we built graviton our host CP use that",
      "offset": 116.56,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "are Arm based and provide the best",
      "offset": 120.479,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "performance and and price performance in",
      "offset": 122.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the cloud for a variety of cloud",
      "offset": 124.719,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "workloads and in the past eight years or",
      "offset": 127.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so I've been running the architecture",
      "offset": 130.16,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "team for trinium where we build",
      "offset": 132.08,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "dedicated highly performant and highly",
      "offset": 134.84,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "efficient AIML accelerators for deep",
      "offset": 137.599,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "learning workloads and the training of",
      "offset": 141.44,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "Frontier models I was at the reinvent uh",
      "offset": 143.56,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "when those first chips were announced",
      "offset": 148.28,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and even when the the acquisition was",
      "offset": 150.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "announced um and it's hard to believe",
      "offset": 153.36,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "that it's been 10 years already oh yeah",
      "offset": 156.519,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "we we we've been amazingly lucky to to",
      "offset": 159.879,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "get acquired by AWS and get integrated",
      "offset": 163.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "into that team I can tell you personally",
      "offset": 165.519,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that this is the second acquisition that",
      "offset": 168.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I've been through in my career and the",
      "offset": 170.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "first acquisition I won't mention the",
      "offset": 172.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "names but it wasn't as smooth as smooth",
      "offset": 175.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "as an aura labs and AWS",
      "offset": 178.28,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "but uh but AWS aws's culture is like a",
      "offset": 180.68,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "sea of startups and the startups are on",
      "offset": 185.239,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "the one hand very autonomous and",
      "offset": 188.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "progressing to their road maps and",
      "offset": 191.04,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Milestones with minimum dependencies on",
      "offset": 193.76,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "other teams but on the other hand they",
      "offset": 196.159,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "share road maps and share information",
      "offset": 199.159,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "extremely collaboratively and openly uh",
      "offset": 201.599,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "so it's been a blast for us at Anapa",
      "offset": 204.48,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "building chips for internal customers in",
      "offset": 207,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "WS that tell us exactly where the pain",
      "offset": 209.879,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "points are and what they're trying to",
      "offset": 212.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "achieve uh and at the same time keeping",
      "offset": 214.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "our startup culture and moving very",
      "offset": 217.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "quickly to build a devices and and",
      "offset": 218.92,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Solutions in in new grounds so how many",
      "offset": 222.28,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "chips has it been all told since the",
      "offset": 224.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "acquisition uh so since acquisition it's",
      "offset": 226.799,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "been a decade and we've built more than",
      "offset": 229.2,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "10 chips so more than a chip per year H",
      "offset": 231.599,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "it's across three product lines as I",
      "offset": 234.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "mentioned before Nitro graviton and",
      "offset": 236.439,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "trinium and",
      "offset": 238.319,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the each one of these product lines is",
      "offset": 240.4,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "shipping in in a many millions and in",
      "offset": 243.079,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "some cases tens of millions of Parts um",
      "offset": 246.12,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "and almost all of our chips got to these",
      "offset": 249.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "volumes in the very first tap out which",
      "offset": 251.799,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "is quite unique in the semiconductor",
      "offset": 254.56,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "industry tape out is like when a design",
      "offset": 256.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is released in manufacturing oh yeah",
      "offset": 258.759,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "exactly yes that's that's what it is and",
      "offset": 260.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "in the cheap world we we basically work",
      "offset": 262.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "extremely hard before we release the",
      "offset": 265.24,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "design to manufacturing because at that",
      "offset": 267.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "point point you trigger an activity the",
      "offset": 269.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "manufacturing activity that costs tens",
      "offset": 271.36,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "of millions of dollars so we work",
      "offset": 274.28,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "extremely hard to make sure that the",
      "offset": 276.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "device is healthy and well tested and",
      "offset": 278,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "can support scaling into volumes before",
      "offset": 280.919,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "we trigger manufactur one of the things",
      "offset": 283.759,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "that's always struck me in my",
      "offset": 285.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "conversations with folks about Hardware",
      "offset": 286.919,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "in general and chips in particular is",
      "offset": 289.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "really as a result of those lead times",
      "offset": 292.479,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "being so long",
      "offset": 294.56,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "and how important it is is to you know",
      "offset": 297.28,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "get a lot of things right kind of catch",
      "offset": 301.199,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the market and we've seen kind of waves",
      "offset": 303.24,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "of accelerator startups come over the",
      "offset": 305.12,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "past you know 10 years or more um and it",
      "offset": 308.639,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "strikes me that that we're in a really",
      "offset": 313.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "interesting point in time right now",
      "offset": 315.479,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "because we're finally seeing like a",
      "offset": 317.44,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "convergence or you know critical mass at",
      "offset": 320.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "scale of a particular light particular",
      "offset": 323.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "type of workload in the form of llms and",
      "offset": 325.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Transformers where as",
      "offset": 328.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "before it struck me that there were a",
      "offset": 331.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "lot of companies trying to innovate uh",
      "offset": 333.44,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "around a more generalized you know work",
      "offset": 337.68,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "set of workloads in deep learning you",
      "offset": 341.199,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "know so some folks would specialize on",
      "offset": 343.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "convolutional architectures but there",
      "offset": 346.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "wasn't enough volume there to really you",
      "offset": 347.88,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "know scale it the way we're seeing with",
      "offset": 350.36,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "llms does that uh does that resonate",
      "offset": 352.479,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "with you first of all and then like how",
      "offset": 355.28,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "do you think about you know what you're",
      "offset": 357.199,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "doing there in that context yeah it",
      "offset": 360.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "absolutely does um so so I think we're",
      "offset": 362.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "seeing the convergence of two Market",
      "offset": 364.6,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "forces roughly at the same times uh so",
      "offset": 367.199,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "first of all the Transformer",
      "offset": 370.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "architecture took the industry by by",
      "offset": 371.919,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "storm and many many workloads are",
      "offset": 374.24,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "mapping towards some variants of the",
      "offset": 376.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Transformer",
      "offset": 379.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "architecture which means that we can",
      "offset": 380.68,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "dedicate our efforts more towards",
      "offset": 383.479,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "optimizing that architecture as long as",
      "offset": 386.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "we keep in generality and we should",
      "offset": 389.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "probably touch on that more as we talk",
      "offset": 390.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "through the details but the other force",
      "offset": 393.759,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "that we're we're seeing across the",
      "offset": 396.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "industry is the the force of scaling",
      "offset": 398.08,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "laws so we now have about a decade or or",
      "offset": 400.28,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "slightly more than a decade where the",
      "offset": 403.919,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "scaling laws are empirically proving",
      "offset": 405.919,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "themselves again and again and what the",
      "offset": 409.56,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "scaling laws mean is that the more",
      "offset": 411.52,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "compute we put into training of a model",
      "offset": 414.199,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "the better intelligence or accuracy we",
      "offset": 417.039,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "get on the other side now if I think",
      "offset": 419,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "about the the convergent of these two",
      "offset": 421.56,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "forces it means that there are a few",
      "offset": 424.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "companies out there or few players out",
      "offset": 428.4,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "there that are spending a lot of compute",
      "offset": 430.96,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "in order to train Frontier models and",
      "offset": 434.039,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "most of them are deploying some form of",
      "offset": 437.56,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "a transformer architecture with a few",
      "offset": 441.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "tweaks everyone has their own secret",
      "offset": 443.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Source um now that what that means is",
      "offset": 445.68,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "that in instead of kind of spreading our",
      "offset": 448.84,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "efforts in supporting a wide variety of",
      "offset": 451.96,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "workloads we can dedicate our efforts",
      "offset": 455.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "our engineering efforts into being",
      "offset": 457.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "extremely efficient in these massive",
      "offset": 459.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "scale workloads and by doing that we can",
      "offset": 462.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "enable the folks that are training for",
      "offset": 466.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "Frontier models to be even more",
      "offset": 469,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "efficient in their deployments and thus",
      "offset": 471.319,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "spend more compute during training and",
      "offset": 474.44,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "get more intelligent models overall",
      "offset": 477.039,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "now I'm simplifying things to some",
      "offset": 480.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "extent here because the the last thing",
      "offset": 482.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that I think we want to do is build a",
      "offset": 484.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "chip that can only do Transformers I I",
      "offset": 486.759,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "don't think that's Wise by by any",
      "offset": 489.28,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "stretch of the imagination because new",
      "offset": 491.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "innovation will come through and we none",
      "offset": 493.639,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "of us can anticipate what new operators",
      "offset": 496,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "will get invented in the next couple of",
      "offset": 498.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "years but we can focus our engineering",
      "offset": 500.159,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "efforts where it matters the most",
      "offset": 503.08,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "because the majority of the industry is",
      "offset": 506.199,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "driven by few workloads and not",
      "offset": 508.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "thousands of workloads and so when you",
      "offset": 510.479,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "think about architecting a a chip how do",
      "offset": 512.159,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "you balance you know where the workload",
      "offset": 516.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "is today projecting forward into time",
      "offset": 518.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that where the workload and the customer",
      "offset": 520.68,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "will be tomorrow and manage the",
      "offset": 523.08,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "requirements for the the tip that you're",
      "offset": 526.959,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "trying to build yeah definitely so so I",
      "offset": 529.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "think we start from a concept that s may",
      "offset": 532.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sound very simple but I think it's very",
      "offset": 535.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "powerful uh it's actually qu a quote",
      "offset": 537.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "from Jeff Bezos from back in the days",
      "offset": 540.2,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "where people spend a lot of time trying",
      "offset": 542.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "to anticipate how the world is going to",
      "offset": 544.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "change in the next five years but they",
      "offset": 547.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "don't spend enough time thinking about",
      "offset": 550.079,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "what's not going to change in the next",
      "offset": 552.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "five years and I think it's it's quite",
      "offset": 553.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "powerful to to think about what's",
      "offset": 556.519,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "actually not going to change so for",
      "offset": 558.6,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "example we know that folks are going to",
      "offset": 561.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "want more compute performance we know",
      "offset": 563.079,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that they they're going to want a better",
      "offset": 565.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "C structures ER we know that power",
      "offset": 568.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "efficiency is going to be a big deal in",
      "offset": 571.079,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "training and in ml in general and we",
      "offset": 573.2,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "also know that folks will want the",
      "offset": 576.519,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "flexibility to innovate on top of our",
      "offset": 579.48,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "platform H because they don't want to be",
      "offset": 583,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "confined to just one or five uh",
      "offset": 585.2,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "workloads but rather they want the",
      "offset": 588.399,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "ability to invent the new secret sauce",
      "offset": 590.279,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "that will make their models perform",
      "offset": 593,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "better than others so the first thing",
      "offset": 594.56,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "that we do is to make sure that we keep",
      "offset": 597.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "a a we keep our design H very efficient",
      "offset": 600.279,
      "duration": 6.761
    },
    {
      "lang": "en",
      "text": "across these different dimensions um and",
      "offset": 604,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "we can go a we can go in a very long",
      "offset": 607.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "tangent on on what each of these",
      "offset": 609.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Dimensions mean but I'll I'll I'll try",
      "offset": 611.6,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "to keep it short ER for example",
      "offset": 614.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "Performance doesn't only mean compute",
      "offset": 616.959,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "performance performance performance is",
      "offset": 619.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "actually driven by quite a few",
      "offset": 621.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "dimensions of the design so by getting",
      "offset": 623.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "to know the workload we see that H some",
      "offset": 625.88,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "workload",
      "offset": 628.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "are dominated or Bound by our compute",
      "offset": 629.959,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "performance so we make sure to pack a",
      "offset": 632.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "lot of compute in our chips but other",
      "offset": 635.16,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "workloads are bound by memory bandwidth",
      "offset": 637.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for example when we interact with a with",
      "offset": 640.76,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "a chatbot it processes your input",
      "offset": 642.76,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "request your prompt and that portion",
      "offset": 646.32,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "tends to be very compute bound but after",
      "offset": 649.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that it enters what we call an auto",
      "offset": 652.079,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "regressive stage where it generates one",
      "offset": 654.2,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "token or one word after the other and",
      "offset": 657.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "that portion of the workload tends to be",
      "offset": 660.639,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "very memory bandwidth bound so we pack a",
      "offset": 662.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "lot of memory bandwidth into our chips",
      "offset": 665.24,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "we also pack a lot of memory capacity",
      "offset": 668.24,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "and a lot of network uh H Band with into",
      "offset": 670.959,
      "duration": 5.241
    },
    {
      "lang": "en",
      "text": "our chips because other workloads and I",
      "offset": 673.959,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "kind of I'll kind of skip through the",
      "offset": 676.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "details for now other workloads tend to",
      "offset": 678.04,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "be either memory capacity bound or or",
      "offset": 680.279,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "network bound so we make sure that we",
      "offset": 682.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "balance the design across all these",
      "offset": 684.6,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Dimensions but then there's also a",
      "offset": 687.16,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "second H level of design considerations",
      "offset": 689.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "that it is probably what you were",
      "offset": 693.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "referring to with your question which is",
      "offset": 694.56,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "H what actual building blocks do we put",
      "offset": 697.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "into our",
      "offset": 700.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "chips um and for it's it's funny because",
      "offset": 701.72,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "for the very first chip that we built uh",
      "offset": 705.12,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "we started building it before",
      "offset": 707.92,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Transformers existed so uh so we didn't",
      "offset": 709.399,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "even know about the building blocks of a",
      "offset": 713.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "transformer when we designed that ship",
      "offset": 715.6,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "but what we did do back in the days is",
      "offset": 717.839,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "to break a set of popular workloads into",
      "offset": 720.519,
      "duration": 7.081
    },
    {
      "lang": "en",
      "text": "Primitives and then the first goal or",
      "offset": 723.88,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "the first attempt was to make sure that",
      "offset": 727.6,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "we support all the different Primitives",
      "offset": 729.2,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "that serve as building blocks to to",
      "offset": 731.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "supporting all these different workloads",
      "offset": 734.079,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "but then the second step after that was",
      "offset": 736.279,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "to kind of zoom out and try to",
      "offset": 738.519,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "generalize each one of these Primitives",
      "offset": 740.32,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "such that if any new operator will get",
      "offset": 742.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "invented there will be a high chance",
      "offset": 745.959,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "that our gen I Primitives would be able",
      "offset": 748.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to support",
      "offset": 750.6,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "it and then what we do after that just",
      "offset": 751.68,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "two more sentences on this on this topic",
      "offset": 754.079,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "H what we do after that is we take a few",
      "offset": 756.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "workloads that we didn't see before and",
      "offset": 758.6,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "test our hypothesis on these workloads",
      "offset": 761.48,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "we basically try to see whether for",
      "offset": 764.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "example if we if we design the chip to",
      "offset": 766.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "do language to support language",
      "offset": 768.68,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "workloads and vision workloads we test",
      "offset": 770.839,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "ourself on an audio workload and we try",
      "offset": 773.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "to see whether new kernels emerge that",
      "offset": 776.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we don't support yet or or we actually",
      "offset": 779.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "support our generaliz set of currents do",
      "offset": 781.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "support all the different workloads and",
      "offset": 783.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "one of our kind of a",
      "offset": 785.68,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "fun experiences during the design of",
      "offset": 789.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "trainum is that when we were building",
      "offset": 792.56,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the first chip which was named",
      "offset": 794.839,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "inferentia back in the days as I said",
      "offset": 796.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Transformer didn't exist and when the",
      "offset": 799.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Transformer paper came out we basically",
      "offset": 801.399,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "mapped it to our generalized set of",
      "offset": 804.12,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "kernels we saw that it's fully support",
      "offset": 806.56,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "by uh by the building blocks that we",
      "offset": 809.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "designed and it was actually more",
      "offset": 812.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "performant than other workloads that we",
      "offset": 814.16,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "designed before and that we designed the",
      "offset": 816.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "building blocks B based on so that was a",
      "offset": 818.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "happy moment during the the growth of",
      "offset": 820.48,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "this trainum architecture that we've",
      "offset": 823.36,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "been working on how should we think",
      "offset": 825.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "about what is a kernel in this context",
      "offset": 826.44,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "is it a set of functionality for common",
      "offset": 829.72,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "operations like multiply add accumulate",
      "offset": 832.639,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "that kind of thing or is it a higher",
      "offset": 835.6,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "level abstraction",
      "offset": 837.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "that's a good question I actually tend",
      "offset": 839.279,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "to intermix three er three ter terms and",
      "offset": 841,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "we we might want to distinguish between",
      "offset": 844.6,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "them a little bit H one term is what we",
      "offset": 846.079,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "call an",
      "offset": 849.68,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "instruction then there is another term",
      "offset": 850.68,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that is called that that that is an",
      "offset": 853.279,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "operator and the third term is a",
      "offset": 855.959,
      "duration": 7.481
    },
    {
      "lang": "en",
      "text": "kernel um so as a hardware architect we",
      "offset": 858.639,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "care a lot about or I care a lot about",
      "offset": 863.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the instruction set architecture these",
      "offset": 866.8,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "are basically the building blocks of the",
      "offset": 869.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "hardware or you can think about it as",
      "offset": 871.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "the software Hardware contract this",
      "offset": 874,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "could be a matrix multiplication or a a",
      "offset": 876.48,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "nonlinear activation function or maybe",
      "offset": 880.199,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "addition of two tensors to one another",
      "offset": 883.639,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "so these are basically that's the",
      "offset": 886.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "language that the hardware operates with",
      "offset": 888.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and any software that operates the",
      "offset": 891.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "hardware need to map higher level",
      "offset": 892.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "workloads to that instruction set that",
      "offset": 894.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "language",
      "offset": 897.199,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "uh then an operator is a framework level",
      "offset": 899.32,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "uh",
      "offset": 904.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "computation uh that is agnostic of the",
      "offset": 905.199,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "hardware underneath it so that could be",
      "offset": 908.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "a layer normalization operator a a fully",
      "offset": 910.68,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "connected operator which is a matrix",
      "offset": 914.04,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "multiplication followed by a a a",
      "offset": 916.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "nonlinear activation layer um and",
      "offset": 919.199,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "there's a bunch of other",
      "offset": 921.8,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "operators and a kernel tends to be the H",
      "offset": 924.12,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "the mapping between an operator and the",
      "offset": 928.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "instruction set so Bas that aernal",
      "offset": 931.079,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "basically is a is a function that a",
      "offset": 933.8,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "software programmer wrote that",
      "offset": 936.56,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "implements an operator or a set of",
      "offset": 938.48,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "operators and Maps them to the",
      "offset": 941.279,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "instruction set of the hardware so maybe",
      "offset": 943.199,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "before we go further talk a little bit",
      "offset": 945.319,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "about the trading architecture as a",
      "offset": 947.88,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "whole relative to gpus and how uh you",
      "offset": 950.12,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "know similar where are the similarities",
      "offset": 954.399,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and distinctions yeah definitely um so",
      "offset": 956.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "so tranium is a different architecture",
      "offset": 959.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "from a GPU and I'll touch on the",
      "offset": 962.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "differences in a second but we also see",
      "offset": 964.12,
      "duration": 6.519
    },
    {
      "lang": "en",
      "text": "some form of convergence between the two",
      "offset": 967.88,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "architecture so it it would be",
      "offset": 970.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "interesting to compare and contrast",
      "offset": 972.399,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "there so to begin with a a GPU",
      "offset": 974,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "architecture is a massively parallel",
      "offset": 978.04,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "architecture where we have thousands of",
      "offset": 981.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "different cores that are working",
      "offset": 984.319,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "together or to to run the the same",
      "offset": 987,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "program uh the programming model is",
      "offset": 989.519,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "typically called simd a single",
      "offset": 992.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "instruction multiple data basically we",
      "offset": 995.399,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "have a single instruction set that is",
      "offset": 997.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "running on multiple cores and each core",
      "offset": 999.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "is processing a different portion of the",
      "offset": 1002.48,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "of the",
      "offset": 1004.24,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "data um the trainum architecture is",
      "offset": 1005.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "built around a small number of very",
      "offset": 1009.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "powerful cores and very power efficient",
      "offset": 1012.6,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "cores uh we built them around what we",
      "offset": 1015.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "call a systolic array which is a a",
      "offset": 1019.079,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "hardware architecture that optimizes for",
      "offset": 1021.88,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "data reuse such that basically optimizes",
      "offset": 1025.839,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "for data reuse and Energy Efficiency",
      "offset": 1030.039,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "such that we read data into one set of",
      "offset": 1032.679,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "compute elements they perform a",
      "offset": 1037,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "computation and move the data to the",
      "offset": 1039.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "next set of compute elements so it",
      "offset": 1040.88,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "minimizes the amount of memory touches",
      "offset": 1043.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "that we need to do across the when when",
      "offset": 1045.839,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "running workload and trainum and that in",
      "offset": 1048.48,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "turn improves the Energy Efficiency",
      "offset": 1051.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "quite",
      "offset": 1054,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "dramatically now having said that we do",
      "offset": 1054.96,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "see that the two architectures are",
      "offset": 1058.32,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "somewhat converging at least",
      "offset": 1061.16,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "partially uh because trinium is now H",
      "offset": 1063.679,
      "duration": 8.521
    },
    {
      "lang": "en",
      "text": "introducing multiple cores multiple",
      "offset": 1068.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "powerful cores but it but we are growing",
      "offset": 1072.2,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "from two cores to four cores to eight",
      "offset": 1075,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "cores and even more in the future",
      "offset": 1077.08,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "and the GPU landscape starting from the",
      "offset": 1080.08,
      "duration": 6.959
    },
    {
      "lang": "en",
      "text": "Volta 100 Generation by Nvidia started",
      "offset": 1083.44,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "introducing what they call tensor cores",
      "offset": 1087.039,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "which are systolic array based compute",
      "offset": 1090.159,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "units that try to take advantage of data",
      "offset": 1092.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "reuse in order to improve Energy",
      "offset": 1096.2,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Efficiency so even though there's a lot",
      "offset": 1098.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "of there are a lot of differences",
      "offset": 1100.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "between the architectures there's also",
      "offset": 1101.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "some similarities and and they're",
      "offset": 1103.84,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "increasing over time talk about how",
      "offset": 1105.64,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "the user needs to think about these",
      "offset": 1109.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "architectural",
      "offset": 1113.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "differences does the user need to be",
      "offset": 1114.64,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "very aware of it or do the are the",
      "offset": 1118,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "differences managed by whatever",
      "offset": 1120.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Frameworks they're using uh so that's",
      "offset": 1122.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that's a Nuance answer to that question",
      "offset": 1124.84,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "um so there it depends on the type of",
      "offset": 1128.44,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "user H we have lots of users that want",
      "offset": 1130.44,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "their models to run be decently",
      "offset": 1134.4,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "optimized but they're not necessarily",
      "offset": 1137.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "trying to squeeze out every last",
      "offset": 1139.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "percentage of Hardware",
      "offset": 1142.36,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "performance um so for these users they",
      "offset": 1144.159,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "actually don't need to be aware of the",
      "offset": 1147.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "architectural differences they can",
      "offset": 1148.679,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "Implement their model on pytorch or",
      "offset": 1151.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "jacks or any any framework of choice um",
      "offset": 1154.12,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "and then we have what we call the neuron",
      "offset": 1158.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "compiler that Maps between the framework",
      "offset": 1160.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "level implementation and the instruction",
      "offset": 1163.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "set of the hardware and optimizes the",
      "offset": 1165.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "execution as it lowers the obstruction",
      "offset": 1168,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "level for example if you go to hugging",
      "offset": 1170.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "phase today and you look at the you have",
      "offset": 1173.559,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "thousands of models there that natively",
      "offset": 1177.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "map to trinium and they just run",
      "offset": 1179.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "efficiently uh specifically with with",
      "offset": 1182.64,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "hugging face they introduce a a what",
      "offset": 1185.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they call the neuron Optimum library",
      "offset": 1187.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "that basically connects between the",
      "offset": 1190.039,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "Transformer library and neuron so you",
      "offset": 1191.76,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "just get automatic optimizations under",
      "offset": 1193.76,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "the hood you don't need to do much",
      "offset": 1196.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that's one is the SDK for trinium oh",
      "offset": 1198.039,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "yeah I should have said so thanks yes um",
      "offset": 1201.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "and then there's another type of",
      "offset": 1205,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "customers uh typically the folks that",
      "offset": 1207.159,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "are training very large Foundation",
      "offset": 1209.28,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "models and for them every last",
      "offset": 1213,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "percentage of performance matters if you",
      "offset": 1215.96,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "can improve performance by three or 4%",
      "offset": 1219.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "it makes a a world of of a difference I",
      "offset": 1222.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "think that was one of the interesting",
      "offset": 1225.48,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "things that we saw out of the deep seek",
      "offset": 1226.72,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "team is that they talked about going",
      "offset": 1229.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "underneath Cuda and writing assembly",
      "offset": 1232.44,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "code to optimize their models or",
      "offset": 1234.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "something along those lines like it's",
      "offset": 1236.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "for teams operating at a certain degree",
      "offset": 1238.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of scale it becomes very important to be",
      "offset": 1240.84,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "able to do that exactly right and that's",
      "offset": 1242.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "exactly the type of examples that I'm",
      "offset": 1244.159,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "referring to and for these customers H",
      "offset": 1246.08,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "we recently introduced H the the neuron",
      "offset": 1249.84,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "kernel interface nki or we like to call",
      "offset": 1252.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "it Nikki and and Nik",
      "offset": 1255.96,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "delivers basically instruction set",
      "offset": 1259.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "architecture a interface to any",
      "offset": 1262.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "programmer so basically you can come as",
      "offset": 1265.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "an expert customer and say I don't want",
      "offset": 1268.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "to be reliant on the entire software",
      "offset": 1271.559,
      "duration": 5.401
    },
    {
      "lang": "en",
      "text": "stock but rather I want to get direct",
      "offset": 1274.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "access to the bare metal hardware along",
      "offset": 1276.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "with its instruction set and I want to",
      "offset": 1279.36,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "program my operators or kernels and",
      "offset": 1281.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "squeeze the maximum performance out of",
      "offset": 1285.799,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "them we've had quite a few examples of",
      "offset": 1287.6,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "customers including anop entropic and",
      "offset": 1290.72,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "poolside and a few internal customers in",
      "offset": 1293.279,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "AWS as well that use Nikki in order to",
      "offset": 1296,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "really",
      "offset": 1299.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "squeeze performance from the devices how",
      "offset": 1301.039,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "does Cuda compare to what you offer with",
      "offset": 1304.08,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "Nikki for trinium 2 so so I think first",
      "offset": 1307.24,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "of all I think Nvidia did a fantastic",
      "offset": 1311.64,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "job with Cuda so so Props to them and",
      "offset": 1313.279,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "they built a very large user based on",
      "offset": 1316.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "top of K which I think was was a a a",
      "offset": 1318.2,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "significant and important achievement in",
      "offset": 1322.039,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "the industry and at the same time Cuda",
      "offset": 1324.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "is somewhat hard to use H because you",
      "offset": 1328,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "need to map an algorithm which you H",
      "offset": 1330.44,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "think of as a as an atomic computation",
      "offset": 1333.96,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "and you need to break it into hundreds",
      "offset": 1337.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "or sometimes even thousands of cores and",
      "offset": 1340.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "handle all the synchronization that is",
      "offset": 1343.44,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "needed between the cores and that's one",
      "offset": 1345.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "of the reason reasons that folks in the",
      "offset": 1348.08,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "industry try to build obstruction levels",
      "offset": 1350.2,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "on top of Cuda including the the Triton",
      "offset": 1353.72,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "Language by by open",
      "offset": 1357.559,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "a um what we are doing with Nikki is to",
      "offset": 1359.919,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "ER provide with the same level of bare",
      "offset": 1364.76,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "metal access but in a way that we think",
      "offset": 1367.919,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "is much easier to program uh with the",
      "offset": 1370.559,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "Nikki Isa you uh Al sry I didn't mention",
      "offset": 1373.919,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "but uh Nikki actually delivers two name",
      "offset": 1377.72,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "spaces or obstruction levels to the",
      "offset": 1381.76,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "programmer the first one is called Nikki",
      "offset": 1383.919,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "Isa you can literally import nikki. Isa",
      "offset": 1386.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and get that namespace and the second",
      "offset": 1389.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "namespace is nikki.",
      "offset": 1391.6,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "lank um and we we basically are trying",
      "offset": 1393.76,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "to give customers the ability to choose",
      "offset": 1398.12,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "the obstruction level in which they they",
      "offset": 1401.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "want to program the devices with the",
      "offset": 1404.08,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "Niki I substruction level it's it's",
      "offset": 1406.559,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "pretty the name is pretty telling you",
      "offset": 1409.44,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "get full control over the instruction",
      "offset": 1411.919,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "set so you get to program at the",
      "offset": 1414.039,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "instruction level and we believe that",
      "offset": 1416.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "it's easier to program at the Nika level",
      "offset": 1418.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "than in the Cuda level because you know",
      "offset": 1422.12,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "exactly what the hardware is doing and",
      "offset": 1424.4,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "you also are interacting with a small",
      "offset": 1426.799,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "number of large cores so you have less",
      "offset": 1429.919,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "of a communication overhead that you",
      "offset": 1433.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "need to",
      "offset": 1435.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "manage with the Niki L obstruction level",
      "offset": 1436.64,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "we basically raise the obstruction a bit",
      "offset": 1440.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "higher and we provide numpy like",
      "offset": 1442.679,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "semantics which scientists are used to",
      "offset": 1445.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "and and and enjoy in their day-to-day",
      "offset": 1448.44,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "life and that's that's an abstraction",
      "offset": 1451.039,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "level that is useful for quick",
      "offset": 1454.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "experimentation so think about H trying",
      "offset": 1456.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "to invent a new operator you probably",
      "offset": 1459.039,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "don't want to squeeze every bit of",
      "offset": 1461.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "performance in the first goal you just",
      "offset": 1463.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "want it to run relatively quickly see if",
      "offset": 1464.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the operator does what you want to do",
      "offset": 1467.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and if it's if it's important you'll",
      "offset": 1470,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "optimize it later on so that's what the",
      "offset": 1471.84,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "Niki Lang H obstruction level is four so",
      "offset": 1473.799,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "if I kind of try to summarize a bit um I",
      "offset": 1477.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "think Cuda is a phenomenal programming",
      "offset": 1481.039,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "environment and we're trying to provide",
      "offset": 1483.24,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "with more capabilities and easier",
      "offset": 1486.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "interface with the neuron kernel",
      "offset": 1488.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "interface or Nikki language that we",
      "offset": 1490.84,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "provide to customers and we're actually",
      "offset": 1492.6,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "collaborating with quite a few",
      "offset": 1495.039,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "universities that are now teaching the",
      "offset": 1496.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "neuron kernal interface as part of their",
      "offset": 1499.32,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "high performance programming courses and",
      "offset": 1501.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "we're seeing really good feedback from",
      "offset": 1504.159,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "students so far to what degree today in",
      "offset": 1505.76,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "2025 do you think that Cuda still",
      "offset": 1509.159,
      "duration": 8.961
    },
    {
      "lang": "en",
      "text": "represents this uh Moe for NVIDIA that",
      "offset": 1512.559,
      "duration": 8.921
    },
    {
      "lang": "en",
      "text": "companies can't overcome or do you think",
      "offset": 1518.12,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "that you know Nikki or other",
      "offset": 1521.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Technologies or you know the pace of",
      "offset": 1525.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "innovation like some set of factors has",
      "offset": 1527.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "created an environment where Cuda no",
      "offset": 1529.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "longer represents this unsalable Mo for",
      "offset": 1531.72,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "NVIDIA so I think Cuda is a phenomenal",
      "offset": 1535.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "programming environment and it's been",
      "offset": 1539.08,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "very useful and we built we as an",
      "offset": 1541.2,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "industry built a large customer base",
      "offset": 1544.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "around it but I think the world has",
      "offset": 1546.6,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "changed quite a bit in the last 5 to 10",
      "offset": 1549.039,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "years and what we're seeing today is",
      "offset": 1551.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "that there's the Transformer",
      "offset": 1554.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "architecture is Extreme popular and in",
      "offset": 1556.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "order to build a Transformer all you",
      "offset": 1560.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "need is five to 10 operators you don't",
      "offset": 1562.24,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "need to implement 100 or 200 different",
      "offset": 1565.159,
      "duration": 7.041
    },
    {
      "lang": "en",
      "text": "operators and on top of that folks that",
      "offset": 1568.6,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "are training Frontier models are",
      "offset": 1572.2,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "spending lots of er lots",
      "offset": 1574.679,
      "duration": 8.761
    },
    {
      "lang": "en",
      "text": "of dollars on compute infrastructure so",
      "offset": 1579,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "if you take these two considerations",
      "offset": 1583.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "together it's absolutely worthwhile for",
      "offset": 1585.48,
      "duration": 7.799
    },
    {
      "lang": "en",
      "text": "a leading AI labs to spend time in",
      "offset": 1589.2,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "optimizing five to 10 operators to have",
      "offset": 1593.279,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "their Transformer running as efficiently",
      "offset": 1596.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "as possible and they'd be absolutely",
      "offset": 1599.159,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "willing to do it in an alternative",
      "offset": 1601.24,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "programming environment to Cuda which I",
      "offset": 1603.96,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "think uh is a good a Tailwind behind the",
      "offset": 1606.88,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "neuron kernel interface these days so",
      "offset": 1611.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "the folks that really care about",
      "offset": 1613.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "performance and would go to the level of",
      "offset": 1615.679,
      "duration": 8.521
    },
    {
      "lang": "en",
      "text": "Cuda or even lower than Cuda they just",
      "offset": 1619.6,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "want something that's going to allow",
      "offset": 1624.2,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "them to do what they need to do and",
      "offset": 1625.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "gives them the the most bang for the",
      "offset": 1627.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "butt from a performance perspective and",
      "offset": 1629.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the folks that aren't as performance",
      "offset": 1631.96,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "sensitive they",
      "offset": 1633.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "can they can do whatever they want",
      "offset": 1635.279,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "without going to that level just because",
      "offset": 1637.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "of the framework compatibility exactly",
      "offset": 1639.399,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "so so just to uh restate this in",
      "offset": 1641.72,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "slightly different words I think the",
      "offset": 1644.399,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "folks that are training Frontier models",
      "offset": 1646.2,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "are going to chase the best comp",
      "offset": 1648.919,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "computer efficiency even if it requires",
      "offset": 1651.159,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "them to to do extra engineering work and",
      "offset": 1653.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we're seeing that very clearly um and",
      "offset": 1656.32,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "that's that's a the the driving force",
      "offset": 1658.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "behind folks migrating to Trum Trum and",
      "offset": 1661.08,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "also using the neuron kernel interface",
      "offset": 1664.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "and the folks that are H not spending as",
      "offset": 1666.799,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "much on compute will want to stay on the",
      "offset": 1670.2,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "framework level they will not want to",
      "offset": 1672.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "code neither in the Cuda level nor in",
      "offset": 1675.039,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the ne internal interface",
      "offset": 1677.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "level and that's why we built an xcla",
      "offset": 1678.64,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "compiler that allows you to take a model",
      "offset": 1682,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "from pytorch or from Jack and seemly",
      "offset": 1684.36,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "seamlessly map it to train is the xlaa",
      "offset": 1686.72,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "compiler part of the um neuron interface",
      "offset": 1690,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that you mentioned at hugging face is",
      "offset": 1694.6,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "that under the covers or if I'm using a",
      "offset": 1696,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "pytorch model um will I need to run that",
      "offset": 1699.039,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "xla compiler myself both pytorch and",
      "offset": 1702.84,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "Jacks have integrated",
      "offset": 1705.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "integr integrated plugins into the XA",
      "offset": 1708.64,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "compiler the ja framework was actually",
      "offset": 1712.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "built on top of the xcla compiler and",
      "offset": 1714.559,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "the pytorch framework introduced it over",
      "offset": 1717.12,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "time H with a shared project by Google",
      "offset": 1720.32,
      "duration": 6.839
    },
    {
      "lang": "en",
      "text": "meta and and Us in AWS and a few others",
      "offset": 1723.399,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "um what the xcla compiler does is it it",
      "offset": 1727.159,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "basically aims to be a just in time",
      "offset": 1731.64,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "compiler which means that it's pretty",
      "offset": 1734.08,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "seamless to you you run the model from",
      "offset": 1736.32,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "the framework without any a Kno how that",
      "offset": 1739.159,
      "duration": 5.961
    },
    {
      "lang": "en",
      "text": "an xcl compiler is working under the",
      "offset": 1742.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "hood and then for the in the very first",
      "offset": 1745.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "time that the model that the the",
      "offset": 1747.64,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "framework encounters an uncompiled code",
      "offset": 1749.32,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "it will trigger the xcla compiler and",
      "offset": 1752.519,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "map the framework level graph to the",
      "offset": 1755.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "hardware and every subsequent time that",
      "offset": 1757.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "you call the same computation graph it",
      "offset": 1759.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "will map automatically to the previously",
      "offset": 1762.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "done compilation when does tranm make",
      "offset": 1764.6,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "sense and for which types of users is it",
      "offset": 1767.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "primarily targeting the users with these",
      "offset": 1770.399,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "large workloads is it uh meant to",
      "offset": 1772.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "ultimately be an alternative to the GPU",
      "offset": 1775.799,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "for kind of your everyday uh generative",
      "offset": 1779.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "AI workloads how do you think about the",
      "offset": 1782.44,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "the user base in that way so so we",
      "offset": 1785.519,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "generally build trainum to be a a",
      "offset": 1788.519,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "general purpose AI acceleration platform",
      "offset": 1791.399,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "which can serve lots of needs across the",
      "offset": 1795,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "a IML Community but we do H think along",
      "offset": 1798.159,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the lines that you mentioned before and",
      "offset": 1802.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "we kind of we have the following mental",
      "offset": 1804.399,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "model um for the folks that are training",
      "offset": 1806.799,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "Frontier models trainum and especially",
      "offset": 1809.76,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "trainum 2 is highly beneficial because",
      "offset": 1812.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "it provides the highest performance and",
      "offset": 1815.6,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "highest performance efficiency whether",
      "offset": 1818.519,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "you're denominated by by energy or by",
      "offset": 1820.679,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "cost uh so folks that are really need to",
      "offset": 1823.679,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "build large compute class",
      "offset": 1826.6,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "can benefit a ton from migrating to",
      "offset": 1828.519,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "training um on top of that we we partner",
      "offset": 1831.679,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "with a few providers including meta and",
      "offset": 1836.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "hugging face and Ray and a few others to",
      "offset": 1838.919,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "provide with a A list of already",
      "offset": 1842.44,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "supported architectures including",
      "offset": 1845.519,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "Transformers and the diffusion",
      "offset": 1848.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "Transformers and stable diffusions",
      "offset": 1850.08,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "models so folks that are using already",
      "offset": 1852.36,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "existing model architectures",
      "offset": 1856.36,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "can very well ramp on Trum 2 with",
      "offset": 1858.519,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "minimal efforts on their side they they",
      "offset": 1861.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "run the model at the framework level and",
      "offset": 1864,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "just trust the neuron software stack to",
      "offset": 1867.12,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "do the rest and map it to the",
      "offset": 1870.279,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "device and and I would also say that the",
      "offset": 1872.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "folks that might want to wait a little",
      "offset": 1875.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "bit before migrating to Trum are the",
      "offset": 1877.32,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "folks that are inventing new models but",
      "offset": 1879.919,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "are not using large clusters so for",
      "offset": 1882.639,
      "duration": 7.481
    },
    {
      "lang": "en",
      "text": "those folks you might not want to take",
      "offset": 1886.679,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "the burden of migrating to new",
      "offset": 1890.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "architecture uh for optimizing uh for a",
      "offset": 1892.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "relatively low level of cost so these",
      "offset": 1896.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "folks might want to wait a little bit",
      "offset": 1898.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "until the trinium infrastructure matures",
      "offset": 1900.6,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "a bit and the migration becomes simpler",
      "offset": 1903.12,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "and simpler over time so in the near",
      "offset": 1905.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "term if I'm in a research environment",
      "offset": 1907.88,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "for example experimenting with novel",
      "offset": 1910.96,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "architectures the burden to ensure that",
      "offset": 1913.279,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "those architectures work efficiently on",
      "offset": 1915.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Cranium would fall on me since it I'm",
      "offset": 1917.88,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "not able to take advantage of a popular",
      "offset": 1920.559,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "abstraction and that might be someone",
      "offset": 1922.559,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "who you know won't reap the full benefit",
      "offset": 1924.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "even though there there are caveats to",
      "offset": 1927.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that that we we might want to we we",
      "offset": 1929.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "might discuss so uh shum does come with",
      "offset": 1932.12,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "a couple of capabilities that just don't",
      "offset": 1936.12,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "exist in other Hardware out there one of",
      "offset": 1939.679,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "these capabilities for example is four",
      "offset": 1942.559,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "to 16 sparsity so an in sparcity",
      "offset": 1945,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "optimization that allows you to to scale",
      "offset": 1948.84,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "H to to speed up your compute by up to",
      "offset": 1952.36,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "4X um so if you want to to H try to make",
      "offset": 1955.44,
      "duration": 8.839
    },
    {
      "lang": "en",
      "text": "take advantage of this capability and 4x",
      "offset": 1960.679,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "speed up is something that that seems",
      "offset": 1964.279,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "quite compelling to you you might want",
      "offset": 1967,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "to try trainum as well but because",
      "offset": 1969.36,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "you're kind of trying to break new",
      "offset": 1972,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "grounds the way to take advantage of",
      "offset": 1973.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "these capabilities would be through the",
      "offset": 1977.2,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "neuron kernel interface or Nikki",
      "offset": 1978.84,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "programming environment that we",
      "offset": 1981.679,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "described before so we've talked quite a",
      "offset": 1982.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "bit about trainum but not about like how",
      "offset": 1984.919,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it's offered to customers right you",
      "offset": 1987.279,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "can't just go buy a tranium chip you're",
      "offset": 1989.799,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "getting these through AWS instances and",
      "offset": 1991.639,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "that was part of the big news at",
      "offset": 1994.2,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "reinvent was the General availability of",
      "offset": 1995.84,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "the trn2 instances can you talk a little",
      "offset": 1999.039,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "bit about um you know how they're",
      "offset": 2001.96,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "packaged at the instance level and I",
      "offset": 2004.36,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "think they also announcements around",
      "offset": 2007.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Ultra servers and Ultra clusters what",
      "offset": 2009.679,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "what do all these things mean yeah",
      "offset": 2012.08,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "definitely uh so when we talk about the",
      "offset": 2014.039,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "trinium to chip or when talk about Trum",
      "offset": 2017.039,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "2 we mean the Trum 2 chip it's something",
      "offset": 2020.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that looks roughly like that and we we",
      "offset": 2022.6,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "actually had the chips uh in our labs",
      "offset": 2025.88,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "for for quite a while now H but what we",
      "offset": 2028.399,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "announce the three invent is General",
      "offset": 2031.799,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "availability of the training two chips",
      "offset": 2034.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "via server",
      "offset": 2036.84,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "that we can rent by the hour or or in",
      "offset": 2038.279,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "any other constellation and these are",
      "offset": 2040.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "called the tier and two instances or tn2",
      "offset": 2043.039,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "servers each trn2 server packs a one",
      "offset": 2046.72,
      "duration": 9.199
    },
    {
      "lang": "en",
      "text": "host CPU and 16 trinium 2 devices so",
      "offset": 2051.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that's one of the advantages with",
      "offset": 2055.919,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "trinium 2 they're very cost and energy",
      "offset": 2057.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "efficient which means that we can pack",
      "offset": 2060.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "more of them in a single server without",
      "offset": 2062.72,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "burdening either the the cost of the",
      "offset": 2065.24,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "server or the energy consumption of the",
      "offset": 2067.399,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "server um now training to the the TN two",
      "offset": 2070.399,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "servers are the most powerful AIML",
      "offset": 2074.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "servers that we have in",
      "offset": 2077,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "AWS um which which means that that's one",
      "offset": 2078.8,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "way to consume Trum 2 you can rent these",
      "offset": 2082.119,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "servers again either rent by the hour or",
      "offset": 2085.72,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "rent as a lower a longer term engagement",
      "offset": 2088.879,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "and just run your workload on the train",
      "offset": 2092.44,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "to servers available in AWS so that's",
      "offset": 2095.079,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "one one entry point let's call it",
      "offset": 2098.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "another entry point is through managed",
      "offset": 2101.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "services so we have a couple of managed",
      "offset": 2103.88,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "services in AWS including AWS Bedrock",
      "offset": 2106.32,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "which is an an an ml API provider that",
      "offset": 2109.68,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "supports various models including Cloe",
      "offset": 2114.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and Lama and dipic and",
      "offset": 2116.839,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "Nova um so for when you use Bedrock we",
      "offset": 2119.04,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "actually map many of these models to",
      "offset": 2124.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Trum under the Hood in order to improve",
      "offset": 2126.68,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "the speed and cost structure of these",
      "offset": 2129.44,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "services so that's another way that you",
      "offset": 2132.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "might be consuming uh tranium to chips",
      "offset": 2135.079,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "without even uh noticing that and maybe",
      "offset": 2137.88,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "a third way to consume trainum Trum",
      "offset": 2141.48,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "chips is via a a complete",
      "offset": 2144.32,
      "duration": 8.36
    },
    {
      "lang": "en",
      "text": "Services by an Amazon and other teams so",
      "offset": 2148.8,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "when you for for example you might have",
      "offset": 2152.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "noticed that when you go to amazon.com",
      "offset": 2155,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you have an AI shopping assistant called",
      "offset": 2157.2,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "Rufus that gives you recommendations and",
      "offset": 2159.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and gives you summarization of different",
      "offset": 2162.359,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "products so Rufus also uses a fleet of",
      "offset": 2164.44,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "training devices under the hood again in",
      "offset": 2167.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "order to improve performance scaling and",
      "offset": 2170.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "cost trck so throughout this",
      "offset": 2173.04,
      "duration": 3.559
    },
    {
      "lang": "en",
      "text": "conversation we've talked generally",
      "offset": 2174.599,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "about performance um but there were some",
      "offset": 2176.599,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "specific stats tossed out at the at",
      "offset": 2180.079,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "reinvent with regard to the tranium 2",
      "offset": 2183.96,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "instance in particular uh but generally",
      "offset": 2187.079,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "speaking how do you think about the",
      "offset": 2189.839,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "performance advantages of trinium",
      "offset": 2191.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "relative to Alternatives yeah definitely",
      "offset": 2194.44,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "so uh the trn2 instances provide with",
      "offset": 2197.599,
      "duration": 8.841
    },
    {
      "lang": "en",
      "text": "21 Peta flops of compute performance and",
      "offset": 2202.64,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "that's dense compute without making use",
      "offset": 2206.44,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "of sparcity or Forex that 803 Peta flops",
      "offset": 2208.4,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "of sparse compute performance in a",
      "offset": 2213.44,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "single server um which is about 1.3x",
      "offset": 2215.319,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "faster than any other uh compute",
      "offset": 2219.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "platform in AWS for the dense",
      "offset": 2222.44,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "computation and",
      "offset": 2225.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "2.6x faster than any other",
      "offset": 2227.319,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "comput AIML accelerated platform for",
      "offset": 2229.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "sparse",
      "offset": 2233.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "computations H we also provide with 46",
      "offset": 2234.839,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "terabyte per second of hbm bandwidth",
      "offset": 2238.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "which is a massive amount of memory",
      "offset": 2241.599,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "bandwidth that's close to 2x H faster",
      "offset": 2243.16,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "than than the next in line instance um",
      "offset": 2246.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and we also provide with 3.2 terabit per",
      "offset": 2249.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "second of network connectivity to",
      "offset": 2252.68,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "thousands of other servers via an ultra",
      "offset": 2255.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "cluster network uh which we can talk",
      "offset": 2259.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "more about now all of that was the trn2",
      "offset": 2261.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "servers we also announced that reinvent",
      "offset": 2265.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the trn2 ultra servers which are",
      "offset": 2268.44,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "essentially four tn2 servers that are",
      "offset": 2271.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "tightly integrated with one another and",
      "offset": 2274.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "you can B basically think about it as",
      "offset": 2277,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "the entire spec is increased by Forex",
      "offset": 2278.839,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "compared to a single tier and two ser",
      "offset": 2282.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "and then the ultra clusters are some",
      "offset": 2284.28,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "number of ultra servers presumably so",
      "offset": 2286.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "the the ultra clusters connect thousands",
      "offset": 2290.16,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "of ultra servers with one another with a",
      "offset": 2293.56,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "what we call internally the 10p 10U",
      "offset": 2298,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "Network so that's a a network that",
      "offset": 2300.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "allows you to connect any server to any",
      "offset": 2304.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "other server with in the ultra cluster",
      "offset": 2306.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "between thousands of ultra servers with",
      "offset": 2308.56,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "a 10 pabit per second of network and",
      "offset": 2311.48,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "less than 10 microsc of latency so",
      "offset": 2316,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that's the reason for the name 10 p is",
      "offset": 2318.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the 10 pabit yeah exactly practically",
      "offset": 2320.319,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "speaking can I go into my AWS console",
      "offset": 2322.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "and make sure my credit card is in there",
      "offset": 2326.56,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "and spin up an ultra cluster or so so",
      "offset": 2328.359,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "not exactly so these H super computers",
      "offset": 2332.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "are extremely expensive they're they're",
      "offset": 2335.319,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "gigantic and we we basically manage an",
      "offset": 2338.16,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "ultra cluster in a very different way",
      "offset": 2341.079,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "than that than we manage general purpose",
      "offset": 2343.079,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "compute platforms so you would need to",
      "offset": 2345.8,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "to communicate with your AWS support",
      "offset": 2348.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "person and and schedule an ultra cluster",
      "offset": 2350.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "for your needs um I suspected that that",
      "offset": 2353.079,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "was the",
      "offset": 2355.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "answer uh tranium in the name is",
      "offset": 2357.44,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "training and and we've talked thus far",
      "offset": 2361.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "about kind of these large training",
      "offset": 2364.2,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "workloads but uh trinium is also used",
      "offset": 2365.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for inference can you talk a little bit",
      "offset": 2369.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "about the use of trinium for inference",
      "offset": 2371.64,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "and um how that compares relative to",
      "offset": 2374.2,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "inferentia and where you see inference",
      "offset": 2378.72,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "workloads on trainum going and when we",
      "offset": 2382.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "started designing INF fren and Trum and",
      "offset": 2384.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "we're talking about eight years back",
      "offset": 2387.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "we've seen somewhat of a different",
      "offset": 2389.8,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "requirement for inference workloads and",
      "offset": 2393.2,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "for training work training workloads ER",
      "offset": 2395.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "inference workloads back in the days",
      "offset": 2397.88,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "required less memory bandwidth very",
      "offset": 2399.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different from today we'll talk about it",
      "offset": 2402.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "in a second um and it required a",
      "offset": 2403.96,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "slightly a smaller set of operators it",
      "offset": 2407.28,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "was very focused on Matrix",
      "offset": 2411.04,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "multiplications convolutions and some",
      "offset": 2413.119,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "nonlinearities",
      "offset": 2415.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "uh but as the Transformer architecture",
      "offset": 2416.359,
      "duration": 6.361
    },
    {
      "lang": "en",
      "text": "picked up and specifically a Transformer",
      "offset": 2420,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "decoder architecture picked up what",
      "offset": 2422.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "we've seen and this is quite a few years",
      "offset": 2425.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "back now uh is that the the inference",
      "offset": 2427.88,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "workload is actually broken down to two",
      "offset": 2431.4,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "very different workloads I touched on it",
      "offset": 2434.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "before the first step of the workload is",
      "offset": 2436.359,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "what we call the prompt processing or",
      "offset": 2439.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "sometimes we call it prefill where we",
      "offset": 2441.44,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "take a user query and we process the",
      "offset": 2444,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "entire query at once and a good way to",
      "offset": 2447.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "think about it is that we read all the",
      "offset": 2450.079,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "model parameters from memory but then we",
      "offset": 2451.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "get to amortize these red bites or red",
      "offset": 2453.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "weights from memory we get to amortise",
      "offset": 2456.72,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "them over the computation that that is",
      "offset": 2459.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "required to to compute the entire prompt",
      "offset": 2462.079,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "or typically hundreds or maybe thousands",
      "offset": 2465.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "of of tokens or thousands of words but",
      "offset": 2468.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "then a Transformer decoder enters the",
      "offset": 2472.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "second phase of the workload when it is",
      "offset": 2474.64,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "auto regressive which means that it",
      "offset": 2477.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "generates one token or one word at the",
      "offset": 2479.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "time it basically generates a token and",
      "offset": 2481.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "then fits the token in an auto",
      "offset": 2483.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "regressive man manner into the input",
      "offset": 2485.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and generates the next token this ends",
      "offset": 2487.76,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "up being very memory bound which breaks",
      "offset": 2490.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "the assumptions that we had for these",
      "offset": 2494.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "workloads from about 10 years ago so",
      "offset": 2495.599,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "quite a few years ago we realized that a",
      "offset": 2499.68,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "serving models or or performing",
      "offset": 2502.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "inference on top of trainum is actually",
      "offset": 2504.68,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "quite efficient today um and we and",
      "offset": 2507.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "we're seeing that H the majority of our",
      "offset": 2511.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "inference serving is done on training",
      "offset": 2515.24,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "devices today uh so naming uh might have",
      "offset": 2517.24,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "not been ideal but we're definitely",
      "offset": 2522.119,
      "duration": 6.281
    },
    {
      "lang": "en",
      "text": "using trainum to to serve both large",
      "offset": 2524.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "cluster training as well as inference uh",
      "offset": 2528.4,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "workers a number of customers were",
      "offset": 2531.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "mentioned at the reinvent launch uh",
      "offset": 2535.16,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "including Adobe poolside data bricks and",
      "offset": 2538.52,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "Qualcomm uh you also had uh",
      "offset": 2542.2,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "representative from Apple and their",
      "offset": 2545.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "machine learning engineering team",
      "offset": 2547.359,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "talking about their early experiences",
      "offset": 2548.839,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "with tranium 2 and mentioned that they",
      "offset": 2551.24,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "were expecting to see significant like",
      "offset": 2553.64,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "50% efficiency gains based on their",
      "offset": 2556.079,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "evaluations um can you talk a little bit",
      "offset": 2560.04,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "about um you know generally what you've",
      "offset": 2562.8,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "seen from customers thus far yeah um so",
      "offset": 2565.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "first of all customers are extremely",
      "offset": 2568.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "excited about Trum 2 as as are we",
      "offset": 2570.76,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "because it it provides with a couple of",
      "offset": 2573.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "clear advantages over Alternatives in",
      "offset": 2576.599,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the market right now the first over is",
      "offset": 2578.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "just raw specs the raw specs are better",
      "offset": 2581.599,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "than alternative Hardware available in",
      "offset": 2583.92,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "the market but on top of that H the",
      "offset": 2586.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "efficiency and the path to getting",
      "offset": 2589.8,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "efficiency is also uh is also better",
      "offset": 2592.16,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "than than alternative offerings out",
      "offset": 2596.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there and when I talk about the path to",
      "offset": 2598.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "efficiency it's worthwhile talking about",
      "offset": 2601.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "two metrics that we tend to measure",
      "offset": 2603.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ourselves according to and the first one",
      "offset": 2605.359,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "is called mfu or model flops utilization",
      "offset": 2607.76,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "and the second one is called mbu or",
      "offset": 2611.599,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "memory bandwidth utilization and the",
      "offset": 2614.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "idea with both of these metrics is H",
      "offset": 2617.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "that we give ourselves a score on H how",
      "offset": 2619.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "much of the entitlement of the hardware",
      "offset": 2624.16,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "of how much of the r specs are we able",
      "offset": 2627,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "to achieve when we're running a real",
      "offset": 2629.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "worklow H so what we've seen with",
      "offset": 2632.28,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "trainum 2 is that when we're running",
      "offset": 2634.839,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "micro benchmarks we can almost entirely",
      "offset": 2637.28,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "saturate the memory bandwith just get",
      "offset": 2640.079,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "above 90% of the the spec memory B",
      "offset": 2642.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "bandwidth of the device which is quite",
      "offset": 2645.28,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "unique it's not easy to achieve but even",
      "offset": 2647.079,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "when we run endtoend inference workloads",
      "offset": 2650.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "and there's a bunch of other uh",
      "offset": 2652.319,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "considerations other than just reading",
      "offset": 2654.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "from the memory as fast as you can we",
      "offset": 2657.24,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "still can get to 60 70 and Beyond a",
      "offset": 2659.4,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "percent mbu which is quite an an",
      "offset": 2662.8,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "impressive ER number and I can kind of",
      "offset": 2665.559,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "quote similar numbers for the mfu land",
      "offset": 2669,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "or for the compute land of the of the a",
      "offset": 2672.119,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "entitlement",
      "offset": 2675.88,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "scores now we've been extremely",
      "offset": 2677.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "extremely lucky to collaborate with the",
      "offset": 2680.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "customers that you mentioned ER we're",
      "offset": 2683.079,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "learning a ton from their engineers and",
      "offset": 2685.8,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "one of the things that we've done",
      "offset": 2688.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "together is uh to build a a set of",
      "offset": 2690.119,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "models on top of Nikki on top of our a",
      "offset": 2694.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "neuron interal interface and the",
      "offset": 2697.28,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "benefits go both ways here on the one",
      "offset": 2700.4,
      "duration": 7.919
    },
    {
      "lang": "en",
      "text": "hand we H try to help a the the our",
      "offset": 2703.76,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "customers to fully optimize performance",
      "offset": 2708.319,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "and get the maximum from the the devices",
      "offset": 2710.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that they're paying for and on the other",
      "offset": 2712.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "hand they get to tell us where the sharp",
      "offset": 2714.76,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "edges are and what we need to improve in",
      "offset": 2718,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "Niki such that we build a sustainable",
      "offset": 2719.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and easy to use programming environment",
      "offset": 2722.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "that will H be with us for for quite a",
      "offset": 2725.359,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "few years down the road another",
      "offset": 2727.359,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "interesting customer announcement from",
      "offset": 2730.079,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "reinvent was anthropic who in addition",
      "offset": 2731.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to talking about many of the things that",
      "offset": 2734.24,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "they're doing with the Claud models on",
      "offset": 2736.24,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "AWS talked about this project for near",
      "offset": 2738.68,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "which is uh large uh superc computer",
      "offset": 2741.96,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "cluster based on the tranium 2 instances",
      "offset": 2745.16,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "can you talk a little bit about um you",
      "offset": 2748.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "know what what can you share about that",
      "offset": 2751.559,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "uh project and the size and scale what",
      "offset": 2753.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "they're hoping to achieve with it yeah",
      "offset": 2757.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "so uh s Tropic is a is an amazing",
      "offset": 2759.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "customer to work with h they're the most",
      "offset": 2761.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "talented AI lab that I've ever",
      "offset": 2764.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "interacted with we're super fortunate to",
      "offset": 2766.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "work with them H with project trainer",
      "offset": 2769,
      "duration": 6.599
    },
    {
      "lang": "en",
      "text": "we're building a gigantic training",
      "offset": 2772.44,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "cluster that fully Embraces the scaling",
      "offset": 2775.599,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "laws H what we've said publicly is that",
      "offset": 2778.319,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that cluster will have many hundreds of",
      "offset": 2780.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "thousands of training to devices but we",
      "offset": 2783.359,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "didn't disclose the the exact",
      "offset": 2785.88,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "number uh but that cluster is H at least",
      "offset": 2788.28,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "5x larger than an Tropics previous",
      "offset": 2792.52,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "training cluster ER and they're planning",
      "offset": 2796.2,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "to to train the the largest and most",
      "offset": 2799.16,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "intelligent Frontier Model on top of",
      "offset": 2802.88,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "that cluster ER that work has already",
      "offset": 2805.68,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "started I've been working with them uh",
      "offset": 2808.52,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "quite closely and I'm super excited",
      "offset": 2811.64,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "about what they have in store for us uh",
      "offset": 2813.92,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for General audience the entropic team",
      "offset": 2816.559,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "is the the AI lab behind the",
      "offset": 2818.96,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "clo AI model and if you haven't tried it",
      "offset": 2821.839,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "I highly recommend it when you're",
      "offset": 2825.119,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "building at the scale of hundreds of",
      "offset": 2826.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "thousands of chips what additional",
      "offset": 2827.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "considerations Beyond some of the things",
      "offset": 2831.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that we've talked about uh come into",
      "offset": 2832.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "play I've got to imagine there's all of",
      "offset": 2834.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "the traditional data center Cooling and",
      "offset": 2837.28,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "you know power issues",
      "offset": 2841.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "networking um can you talk a little bit",
      "offset": 2843.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "about those definitely yes um so when",
      "offset": 2845.64,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "when you're building a that scale",
      "offset": 2848.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "there's a a whole list of considerations",
      "offset": 2850.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that come into play the first one is is",
      "offset": 2853.72,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "just architectural ER where we need to",
      "offset": 2856.4,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "rethink how we Shard the model in order",
      "offset": 2859.92,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "to fit a gigantic Model A on a cluster",
      "offset": 2863.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "of that scale now we all we all are",
      "offset": 2867.24,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "familiar with tensor parallelism",
      "offset": 2870.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "pipeline parallelism data expert",
      "offset": 2871.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "sequence parallelism all these",
      "offset": 2874,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "techniques are are are quite well known",
      "offset": 2876,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "at this point but when you H push the",
      "offset": 2878.72,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "boundary to a cluster of that scale you",
      "offset": 2882.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "also work against what we call the",
      "offset": 2885.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "global minib batch constraint so",
      "offset": 2887.28,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "basically there's a a maximum number of",
      "offset": 2889.96,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "examples that the model can train can",
      "offset": 2893.72,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "can work through before updating the",
      "offset": 2896.64,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "weights and beyond that number scaling",
      "offset": 2899.28,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "is is significantly diminished so",
      "offset": 2902.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "basically you add more compute but you",
      "offset": 2905.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "don't don't get better speed UPS the",
      "offset": 2907.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "model will just need the same number",
      "offset": 2909.88,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "roughly the same number of steps in",
      "offset": 2912.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "order to convert so the first thing to",
      "offset": 2913.839,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "do is to H decide on the sharding",
      "offset": 2916.52,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "strategy in a way that optimizes across",
      "offset": 2920.319,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "multiple Dimensions from system",
      "offset": 2923.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "performance and all the way to the",
      "offset": 2925.48,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "science or or mini",
      "offset": 2927.24,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "constraints h on top of that when you",
      "offset": 2930.319,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "train at that a a scale",
      "offset": 2933.04,
      "duration": 7.759
    },
    {
      "lang": "en",
      "text": "you also need to pay lots of attention",
      "offset": 2937.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "uh to error detection and error recovery",
      "offset": 2940.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "because even if we and we do we pay a",
      "offset": 2944,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "ton of attention to making these trainum",
      "offset": 2946.4,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "two devices extremely robust and I can I",
      "offset": 2948.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "can spend a full hour talking about all",
      "offset": 2951.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the techniques that we employ there but",
      "offset": 2953.4,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "even after doing that when you train on",
      "offset": 2955.96,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "hundreds of thousands of devices every",
      "offset": 2958.319,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "once in a while a device will fail and",
      "offset": 2960.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you need to recover from that H so it's",
      "offset": 2963.64,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "really critical without starting your",
      "offset": 2966.119,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "training job that you started two months",
      "offset": 2968.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "ago all over again oh 100% yes so the",
      "offset": 2970.88,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "the first order H set of optimizations",
      "offset": 2975.28,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "are to identify the failure really",
      "offset": 2979.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "quickly so you don't want to identify",
      "offset": 2981.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the failure after 10 minutes or after an",
      "offset": 2984.2,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "hour when you kind of wasted a lot of",
      "offset": 2987.119,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "compute um and you also want to be able",
      "offset": 2990.319,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "to replace a node and recover extremely",
      "offset": 2993.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "quickly but this these are kind of table",
      "offset": 2995.68,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "sticks and there are also more advanced",
      "offset": 2998.04,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "optimizations where you actually don't",
      "offset": 3001.48,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "recover from a checkpoint but rather can",
      "offset": 3004.079,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "dynamically change the set of nodes that",
      "offset": 3007.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "are participating in a computation and",
      "offset": 3010.559,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "all of that goes into every large scale",
      "offset": 3013.24,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "training gr that we do uh without going",
      "offset": 3016.04,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "into specific techniques by certain",
      "offset": 3019.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "customers it's interesting that uh all",
      "offset": 3021.559,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "of your responses this was primarily",
      "offset": 3025.52,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "architectural and software related as",
      "offset": 3028.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "opposed to uh where I thought you'd have",
      "offset": 3031.4,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "challenges is you know the topology and",
      "offset": 3034.52,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "network and things like that I imagine",
      "offset": 3038.799,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the implication being that it AWS is",
      "offset": 3041.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "scale you figured a lot of that out",
      "offset": 3043.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "already yeah I mean I consider the the",
      "offset": 3045,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "networking and and power optimizations",
      "offset": 3047.559,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "that we do as table stickes we we need",
      "offset": 3050.359,
      "duration": 7.161
    },
    {
      "lang": "en",
      "text": "to give our customers an extreme smooth",
      "offset": 3053.839,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "experience uh when from that perspective",
      "offset": 3057.52,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "because that's our responsibility that's",
      "offset": 3061.119,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "not that's not our customers",
      "offset": 3062.799,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "responsibility and that's ER and that's",
      "offset": 3065.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "where aws's",
      "offset": 3068.52,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "reputation lies and that's where that's",
      "offset": 3070.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "the reason that people come to AWS to",
      "offset": 3073.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "train large models H but after doing all",
      "offset": 3075.079,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "of that uh we still need to to",
      "offset": 3078.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "acknowledge the the laws of physics even",
      "offset": 3081.559,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "if every single device can fail like at",
      "offset": 3084.119,
      "duration": 8.161
    },
    {
      "lang": "en",
      "text": "0.1% of percentage annually uh we still",
      "offset": 3087.4,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "need to create systems around the",
      "offset": 3092.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "cluster in order to deal with these",
      "offset": 3094.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "failures and when you're operating a",
      "offset": 3096.72,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "cluster at that scale are you using the",
      "offset": 3098.68,
      "duration": 8.679
    },
    {
      "lang": "en",
      "text": "traditional AWS Primitives to to to do",
      "offset": 3102.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "it are you using the same console or you",
      "offset": 3107.359,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "using cloud formation and things like",
      "offset": 3109.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "that or are you getting maybe more of a",
      "offset": 3110.839,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "raw you know set of instances that you",
      "offset": 3114.319,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "are operating yourself uh with your own",
      "offset": 3116.799,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "tooling so different customers have uh",
      "offset": 3119.16,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "make different choices here uh we've",
      "offset": 3121.839,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "seen quite a few customers that are",
      "offset": 3124.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "leveraging what we call hyperpod which",
      "offset": 3126.319,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "is another AWS service for managing a",
      "offset": 3129.24,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "large training clusters and it provides",
      "offset": 3132.359,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "you with a with f with f detection and",
      "offset": 3135.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "recovery and and many other er er",
      "offset": 3138.88,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "services like that uh but other",
      "offset": 3142.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "customers decide to just get get their",
      "offset": 3144.88,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "raw instances and maybe run a kubernetes",
      "offset": 3146.799,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "cluster on top of it and manage it",
      "offset": 3149.68,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "themselves um and we're non opinionated",
      "offset": 3152.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "here we we would support customers in in",
      "offset": 3155.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "whichever H place they want to meet us",
      "offset": 3158,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "there are a couple things that you",
      "offset": 3160.64,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "mentioned earlier that I wanted to",
      "offset": 3161.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "follow up on one of those was your",
      "offset": 3163.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "comments around",
      "offset": 3166.599,
      "duration": 3.401
    },
    {
      "lang": "en",
      "text": "sparsity uh you talked a little bit",
      "offset": 3168,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "about how trainum to provide some",
      "offset": 3170,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "advantages relative to sparcity but can",
      "offset": 3172.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you talk a little bit about how those",
      "offset": 3174.559,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "are used and what's required to take",
      "offset": 3176.88,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "advantage of them one of the things that",
      "offset": 3178.839,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "we've seen over the last couple of years",
      "offset": 3180.599,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "is that model intelligence improves with",
      "offset": 3183.24,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the number of parameters and with",
      "offset": 3186.119,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "compute that is invested during training",
      "offset": 3188,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "but at the same time we we're already",
      "offset": 3190.48,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "spending a ton of compute energy and and",
      "offset": 3193,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "spend in order to train these models so",
      "offset": 3196.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "one of the challenges that we we're",
      "offset": 3199.4,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "seeing is how to support larger models",
      "offset": 3201,
      "duration": 6.799
    },
    {
      "lang": "en",
      "text": "without increasing the amount of compute",
      "offset": 3205.28,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "and the amount of energy spent in a",
      "offset": 3207.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "proportional Manner and one of the nice",
      "offset": 3210.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "ways to achieve that is what we call",
      "offset": 3213.079,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "sparcity uh where we basically grow the",
      "offset": 3215.72,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "amount of parameters in the model but",
      "offset": 3219.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "take a take some assumption or or or",
      "offset": 3222,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "actually constrain the model to have a",
      "offset": 3225.16,
      "duration": 6.679
    },
    {
      "lang": "en",
      "text": "certain percentage of zeroed parameters",
      "offset": 3228.119,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "and there's nice a nice research out",
      "offset": 3231.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "there which I'll mention in a sec",
      "offset": 3234.799,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "ER on on the the gains that can be had",
      "offset": 3237,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "with such sparsity",
      "offset": 3240.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "techniques um but as of today the the",
      "offset": 3242.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "hardware available in the market doesn't",
      "offset": 3245.28,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "support a lot of sparcity",
      "offset": 3247.76,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "acceleration H we have various gpus that",
      "offset": 3250.359,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "support what we call two to four",
      "offset": 3253.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "sparcity and when I say two to four it",
      "offset": 3255.799,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "means that for out of every four",
      "offset": 3257.96,
      "duration": 4.599
    },
    {
      "lang": "en",
      "text": "elements two elements are nonzero and",
      "offset": 3259.68,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "the other are",
      "offset": 3262.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "zeros um so we and and on top of that we",
      "offset": 3264.359,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "we also saw that the speed up that you",
      "offset": 3267.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "get from sparity is not linear so you",
      "offset": 3270.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "would expect that two to four sparcity",
      "offset": 3272.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "will give you 2x speed up but what we're",
      "offset": 3274.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "seeing is",
      "offset": 3276.799,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that that that's actually somewhat lower",
      "offset": 3278,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "than that the speed ups are somewhat",
      "offset": 3282.079,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "lower than",
      "offset": 3283.599,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "2x uh so we we decided to tackle that",
      "offset": 3285.16,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "with the training to architecture and",
      "offset": 3288.68,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "we've built in a a sparsity capability",
      "offset": 3290.64,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "that we call 4 to 16",
      "offset": 3293.839,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "uh so you you should not two things here",
      "offset": 3296.599,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "first of all the the the potential speed",
      "offset": 3299.04,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "up is much much higher than than 2x it's",
      "offset": 3301.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "actually 4X because we have four nonzero",
      "offset": 3305.52,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "element out of every 16 elements and we",
      "offset": 3307.88,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "also have a a extensive benchmarking of",
      "offset": 3310.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the hardware that shows that we can",
      "offset": 3314.119,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "actually get the 4X speed so it's not",
      "offset": 3316.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "only spec numbers it's actually",
      "offset": 3319.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "achievable in",
      "offset": 3320.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "benchmarks now when we introduce this",
      "offset": 3322.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "capability to our science team H the",
      "offset": 3325.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "first thing that they wanted to do",
      "offset": 3328.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "actually surprised me a little bit but",
      "offset": 3329.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "it it makes tons of sense they didn't",
      "offset": 3331.839,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "want to try the the 4 to6 sparsity as a",
      "offset": 3334.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "first step but rather the 4 to8 sparsity",
      "offset": 3336.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "as a first step and the rationale behind",
      "offset": 3339.16,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "it is that now they can Benchmark 2 to",
      "offset": 3342.16,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "four sparsity by other Hardware",
      "offset": 3345.079,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "platforms with 4 to8 sparsity which",
      "offset": 3346.839,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "provides the same spec speed ups but",
      "offset": 3350.079,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "provides you with a better flexibility",
      "offset": 3353.4,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "in the sparsity fatter CU if you think",
      "offset": 3355.4,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "about it a little bit you'll you'll see",
      "offset": 3357.599,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "that 4 to8 sparsity is more flexible",
      "offset": 3359.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "than 2 to four",
      "offset": 3361.079,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "sparcity um and what we've what we've",
      "offset": 3362.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "done is we took a set of models",
      "offset": 3365.76,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "specifically we used llama 3.1 8B 70b",
      "offset": 3368.119,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "and 45b so across various sizes and we",
      "offset": 3371.96,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "sparsify these models to either 2 to",
      "offset": 3375.4,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "four sparcity or 4 to8 sparcity and we",
      "offset": 3378.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "saw that across the board the 4 to8",
      "offset": 3381.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "sparcity was more accurate so that was",
      "offset": 3382.92,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "already a win",
      "offset": 3385.44,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and we also saw that we we're getting",
      "offset": 3386.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the 2X and 4x speed 2x speed UPS in this",
      "offset": 3388.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "case and 4x speed up in 428 4216",
      "offset": 3391.079,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "spity uh with for the for the gem",
      "offset": 3394.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "portion of the",
      "offset": 3397.799,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "work now the second thing that we did",
      "offset": 3399.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "which was quite exciting was to then",
      "offset": 3401.839,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "compare the accuracy of the sparsified",
      "offset": 3405.16,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "model with the accuracy of the original",
      "offset": 3407.72,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "dense model and we've seen that H we've",
      "offset": 3411.4,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "been able to recover to fully recover",
      "offset": 3414.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the accuracy of the model with the",
      "offset": 3417.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "sparer presentation and there's a paper",
      "offset": 3419.2,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "from a team in Amazon that is going to",
      "offset": 3422.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "come out soon to to discuss the the",
      "offset": 3425,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "techniques that we're used to achieve",
      "offset": 3427.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "that meaning the techniques that were",
      "offset": 3428.799,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "used to sparsify the models exactly yeah",
      "offset": 3430.44,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "so if you if after applying sparsity you",
      "offset": 3433.799,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have um reached parody from a you know",
      "offset": 3436.319,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "accuracy performance perspective is it",
      "offset": 3439.799,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "efficiency that you've gained or is it",
      "offset": 3442.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "um you know latency because you're doing",
      "offset": 3445.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "a greater amount of effective flops like",
      "offset": 3447.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "what is the ultimate Advantage it's",
      "offset": 3449.799,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "actually all of the above so so",
      "offset": 3452.079,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "basically during inference uh we get to",
      "offset": 3454.16,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "read sparse weights and we won't read",
      "offset": 3457.599,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the the zeros we only only read the",
      "offset": 3459.72,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "nonzero weights so a token generation",
      "offset": 3461.76,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "becomes linearly faster or close to",
      "offset": 3465.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "linearly faster H we also improve",
      "offset": 3467.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "latency or or what typically called time",
      "offset": 3470.16,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "to First token because we we skip all",
      "offset": 3472.52,
      "duration": 6.039
    },
    {
      "lang": "en",
      "text": "the computation for the zero",
      "offset": 3475.559,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "parameters and during training you can",
      "offset": 3478.559,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "actually decide how you use the gains",
      "offset": 3480.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "you can either cut your spend by about",
      "offset": 3483.16,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "half or you can just throw more compute",
      "offset": 3485.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "at the problem and get a more",
      "offset": 3488.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "intelligent model uh so for folks that",
      "offset": 3490.52,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "really manage to H to recover the full",
      "offset": 3493.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "accuracy of the model this is almost",
      "offset": 3496.48,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "like jumping a generation in the",
      "offset": 3498.839,
      "duration": 3.881
    },
    {
      "lang": "en",
      "text": "hardware and training with the hardware",
      "offset": 3500.52,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "that will come up in 1. in a year and a",
      "offset": 3502.72,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "half like trinium three one way that you",
      "offset": 3505.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "can think about mixture of experts is as",
      "offset": 3508.359,
      "duration": 5.321
    },
    {
      "lang": "en",
      "text": "a kind of a sparse Network architecture",
      "offset": 3511.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "have you done anything with M Moes oh I",
      "offset": 3513.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "I 100% agree with you so I think the",
      "offset": 3516,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "community is looking into deploying",
      "offset": 3518.2,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "sparcity in different ways and the way",
      "offset": 3520.359,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "that I look at things is that structured",
      "offset": 3523.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "sparsity is a form of fine grain",
      "offset": 3526.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "sparsity and mixture of experts is",
      "offset": 3528.48,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "exactly as you said a coarse H grain",
      "offset": 3531.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "level of",
      "offset": 3533.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sparcity and we're definitely working",
      "offset": 3534.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "with quite a few customers that are",
      "offset": 3537.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "deploying a a mixture of",
      "offset": 3539.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "experts",
      "offset": 3542.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "models ER one that I can definitely name",
      "offset": 3544.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because it's public at this point is",
      "offset": 3547.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "data bricks with their dprx Moe",
      "offset": 3549.119,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "model um and we've been working quite a",
      "offset": 3552.24,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "bit to optimize uh the trinium to and",
      "offset": 3555.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "the the neuron stack uh to support mix",
      "offset": 3559.76,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "effect for ER models including efficient",
      "offset": 3562.72,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "all to all communication Primitives",
      "offset": 3566.44,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "including Dynamic compute and including",
      "offset": 3568.839,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "a Advanced sharding Techniques for",
      "offset": 3572.48,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "expert parallelism versus sequence",
      "offset": 3575.24,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "parallelism one of the things that we've",
      "offset": 3577.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "seen lately especially with dips is that",
      "offset": 3579.319,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "there's a gradual migration from a small",
      "offset": 3582.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "number of large experts to a large",
      "offset": 3585.4,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "number of small experts and that",
      "offset": 3588.359,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "actually shifts the the system",
      "offset": 3590.96,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "optimization tradeoffs quite a bit and",
      "offset": 3593.44,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "pushes us more towards expert Paralis uh",
      "offset": 3596.039,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "techniques that we've been working to",
      "offset": 3599.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "optimize quite significantly in when we",
      "offset": 3601.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "talked about some of the work with",
      "offset": 3604.24,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "developing new kernels and supporting",
      "offset": 3606.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "new architectures are there any examples",
      "offset": 3610.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that come to mind for that yeah we we",
      "offset": 3612.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "have a ton of examp we have too many to",
      "offset": 3614.079,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "mention but uh most of them are are",
      "offset": 3615.599,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "customers that are building their own",
      "offset": 3618.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "unique secret Source operators on top of",
      "offset": 3622,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Premium but one example that uh that",
      "offset": 3624.24,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we've shared broadly and is super fun",
      "offset": 3627.119,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "you can read about it online we actually",
      "offset": 3629.76,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "go in in quite a bit of depth into how",
      "offset": 3631.16,
      "duration": 5.959
    },
    {
      "lang": "en",
      "text": "this was H built H is what an internal",
      "offset": 3633.799,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "team did with the Mamba 2 Network so so",
      "offset": 3637.119,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "just to give a little bit of background",
      "offset": 3641,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Mamba 2 is a state space model which is",
      "offset": 3643.44,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "a state space models are a family of",
      "offset": 3647.2,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "models that are a tweak to the",
      "offset": 3649.079,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "Transformers architecture that replaces",
      "offset": 3652.319,
      "duration": 7.201
    },
    {
      "lang": "en",
      "text": "the the attention operator in a way that",
      "offset": 3655.64,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "can support very long sequence length of",
      "offset": 3659.52,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "millions of",
      "offset": 3663.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "tokens H but one of the challenges with",
      "offset": 3664.839,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "the with State space models is that",
      "offset": 3667.44,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "they're inherently sequential which",
      "offset": 3669.52,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "means that you need to pay really close",
      "offset": 3672.119,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "attention to Performance optimization to",
      "offset": 3674.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "get them to perform H so we have an",
      "offset": 3675.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "internal team that is optimizing these H",
      "offset": 3678.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "State space models they identified a",
      "offset": 3680.96,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "kernel that was a bottleneck for them",
      "offset": 3684.28,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "when running on top of trinium so they",
      "offset": 3686.599,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "just took that kernel and migrated it to",
      "offset": 3688.799,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "a to a Nikki",
      "offset": 3692.28,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "implementation H that kernel was sped up",
      "offset": 3694.039,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "by",
      "offset": 3697.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "17x and they did it with 77 lines of",
      "offset": 3698,
      "duration": 6.119
    },
    {
      "lang": "en",
      "text": "Nikki code it's all online wow yeah it's",
      "offset": 3700.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "all all online we were super happy with",
      "offset": 3704.119,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "that achievement and they ended up uh",
      "offset": 3706,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "providing really good performance for",
      "offset": 3709.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "their Mamba 2 implementation well we'll",
      "offset": 3711.119,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "dig up that link and include it in the",
      "offset": 3713.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "show notes and also link to my interview",
      "offset": 3715.319,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "with Albert goo who was one of the",
      "offset": 3718.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "researchers who uh created Mambu so one",
      "offset": 3719.68,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "of the other things that came out of the",
      "offset": 3722.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "tranium to Keynote or Mt garin's keynote",
      "offset": 3726.119,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "at the last reinvent I should say was",
      "offset": 3728.72,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "that we should expect tranium 3 to be",
      "offset": 3731.839,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "released sometime uh late this year how",
      "offset": 3735.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "are you thinking about evolving the",
      "offset": 3739.359,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "trainum architecture to you know meet",
      "offset": 3741.359,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "new challenges and in needs and um you",
      "offset": 3744.839,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "know be positioned for the way that uh",
      "offset": 3748.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "generative AI workloads are evolving",
      "offset": 3752.039,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "yeah so H so training 3 is in very",
      "offset": 3754.72,
      "duration": 6.639
    },
    {
      "lang": "en",
      "text": "Advanced uh development stages I'll try",
      "offset": 3757.839,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "to avoid talking about the exact stage",
      "offset": 3761.359,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "er er the exact stage and we're actually",
      "offset": 3764,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "already working on on trainum four so",
      "offset": 3767.359,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "we're working on a couple of generations",
      "offset": 3769.48,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "ahead and we're definitely in terms of",
      "offset": 3773.079,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "how we're thinking about evolving the",
      "offset": 3776.839,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "trainum architecture and making sure",
      "offset": 3779.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "that it provides the maximum value to",
      "offset": 3782,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "our customers ER there's a couple of",
      "offset": 3783.64,
      "duration": 6.719
    },
    {
      "lang": "en",
      "text": "tenets that uh that we work through the",
      "offset": 3787.2,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "first one is is very straightforward H",
      "offset": 3790.359,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "we we tightly collaborate with a set of",
      "offset": 3792.96,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "lead customers including the ones that",
      "offset": 3796.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "we mentioned before and Tropic poolside",
      "offset": 3798.559,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "and and many others um and we work with",
      "offset": 3800.88,
      "duration": 8.159
    },
    {
      "lang": "en",
      "text": "them to Def find the H the the next",
      "offset": 3804.96,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "steps of the architecture Evolution and",
      "offset": 3809.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "what what would it require to support",
      "offset": 3811.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the next generation of model",
      "offset": 3814.4,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "architecture and the next next",
      "offset": 3815.76,
      "duration": 3.079
    },
    {
      "lang": "en",
      "text": "generation of model architectures",
      "offset": 3816.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "including a couple of H Primitives that",
      "offset": 3818.839,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "we bake into the architecture that just",
      "offset": 3821.839,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "does don't exist in in any other",
      "offset": 3824.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "Hardware that is the market today but on",
      "offset": 3827.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "top of that we we actually ER build on",
      "offset": 3829.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "two tenets in Anapa Labs that I think",
      "offset": 3833.039,
      "duration": 5.161
    },
    {
      "lang": "en",
      "text": "are paying a lot of dividends uh the",
      "offset": 3835.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "first one is what we call the the",
      "offset": 3838.2,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "Builder Builder operator approach where",
      "offset": 3839.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "every person that builds the training",
      "offset": 3843.52,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "devices also operates them in the fleet",
      "offset": 3846.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "in the giant clusters that were were",
      "offset": 3849.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "building and that gives us a really good",
      "offset": 3852.079,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "flywheel where H the the builders that",
      "offset": 3855.24,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "are that are ER that are designing the",
      "offset": 3858.839,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "chip are Al are the ones that know exact",
      "offset": 3862.52,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "exactly how to optimize the chip how to",
      "offset": 3865.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "optimize its power",
      "offset": 3867.92,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "point power operating point and many",
      "offset": 3869.48,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "other considerations and this is really",
      "offset": 3872.799,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "critical because they can solve problems",
      "offset": 3875.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "faster than anyone else but they also",
      "offset": 3877.64,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "highly incentivized to build a an easy",
      "offset": 3880,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "to operate system for the Next",
      "offset": 3882.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Generation so we kind of get a flywheel",
      "offset": 3884.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "there and the other tenet that we H that",
      "offset": 3886.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "we pay a lot of attention to is the",
      "offset": 3889.4,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "designer Optimizer so folks on my team",
      "offset": 3892.079,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "that are AR icting the the Next",
      "offset": 3895.039,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Generation also spend a lot of time with",
      "offset": 3897.44,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "customers and with the software teams to",
      "offset": 3900.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "optimize existing works and get we get",
      "offset": 3902.359,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "two benefits there they know the",
      "offset": 3905.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "hardware the best so they can optimize",
      "offset": 3906.559,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "it but they also H feel firsthand when",
      "offset": 3908.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the where the sharp edges are and they",
      "offset": 3911.799,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "build improvements towards the Next",
      "offset": 3913.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Generation so we get a very nice",
      "offset": 3915.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "flywheel from these two tenants so any",
      "offset": 3917.24,
      "duration": 5.879
    },
    {
      "lang": "en",
      "text": "specific predictions about where the the",
      "offset": 3919.88,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "architecture goes uh I'll give you the",
      "offset": 3923.119,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the easy predictions there there's going",
      "offset": 3925.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "to be way more comput better faster",
      "offset": 3928.119,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "cheaper yes exactly H but on top of that",
      "offset": 3930.119,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "I I I definitely see us going more",
      "offset": 3933.839,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "towards more compact data types I think",
      "offset": 3936.559,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "that's very clear these days and uh and",
      "offset": 3939.079,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "I I see us going",
      "offset": 3942.799,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "towards a a few dedicated optimizations",
      "offset": 3944.68,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "that I won't touch on right now but",
      "offset": 3948.559,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "we're planning to talk about them at the",
      "offset": 3951.119,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "the next stream awesome details to come",
      "offset": 3953.319,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "um yes well Ron thanks so much for",
      "offset": 3955.72,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "taking the time to jump on and share a",
      "offset": 3958.599,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "bit about what you've been working on uh",
      "offset": 3961.039,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "both immediately and over the past 10",
      "offset": 3963.72,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "years we went back a little bit uh but",
      "offset": 3966.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it's been it's been great catching up on",
      "offset": 3968.72,
      "duration": 3.399
    },
    {
      "lang": "en",
      "text": "all this thank you s I enjoy the",
      "offset": 3970.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "conversation thanks for having me thank",
      "offset": 3972.119,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "you",
      "offset": 3974.599,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 3993.08,
      "duration": 3.19
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:24.668Z"
}