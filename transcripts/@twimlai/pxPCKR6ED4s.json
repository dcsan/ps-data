{
  "episodeId": "pxPCKR6ED4s",
  "channelSlug": "@twimlai",
  "title": "Grokking, Generalization Collapse, and Dynamics of Training Deep Neural Nets [Charles Martin] - 734",
  "publishedAt": "2025-06-05T00:32:17.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "So, think of it like baking a cake. When",
      "offset": 0.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you bake a cake, you you have the",
      "offset": 2.32,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "temperature, you watch the cake. Imagine",
      "offset": 3.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a cake has a lot of layers. Well, if the",
      "offset": 5.12,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "oven's too hot, some layers are going to",
      "offset": 7.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "burn and the inside's not going to get",
      "offset": 9.519,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "cooked cuz you don't get good, you know,",
      "offset": 11.48,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "conduction through the heat. So, what",
      "offset": 13.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "you want, even when you bake a cake, you",
      "offset": 15.759,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "still have to be careful to adjust the",
      "offset": 17.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "temperature, right? To make sure that",
      "offset": 18.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the layers all cook at the same way. Uh,",
      "offset": 20.56,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "and it's the same idea in a",
      "offset": 22.96,
      "duration": 5.5
    },
    {
      "lang": "en",
      "text": "model. All",
      "offset": 26.119,
      "duration": 11.271
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 28.46,
      "duration": 8.93
    },
    {
      "lang": "en",
      "text": "right, everyone. Welcome to another",
      "offset": 37.84,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "episode of the TwiML AI podcast. I am",
      "offset": 39.52,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "your host, Sam Cherington. Today, I'm",
      "offset": 41.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "joined by Charles Martin. Charles is the",
      "offset": 44.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "founder of Calculation Consulting.",
      "offset": 46.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Before we get going, be sure to take a",
      "offset": 48.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "moment to hit that subscribe button",
      "offset": 50.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "wherever you're listening to today's",
      "offset": 51.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "show. Charles, I am super excited to",
      "offset": 53.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "have you on the podcast. We've been",
      "offset": 56.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "working on this one for quite a while",
      "offset": 57.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "now. Great. Thanks a lot, Sam. I'm I'm",
      "offset": 59.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "really happy to be here. This is a great",
      "offset": 61.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "show. Appreciate it. I appreciate it.",
      "offset": 63.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "You know, to get things started, I'd",
      "offset": 66.08,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "love to have you share a little bit",
      "offset": 67.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "about your background and what you've",
      "offset": 69.439,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "been up to recently with our audience.",
      "offset": 71.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Sure thing. So, you know, I have a I'm",
      "offset": 73.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "an AI researcher. Um, I did my PhD at",
      "offset": 75.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "University of Chicago. You may know my",
      "offset": 78.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "more famous classmate John Jumper who",
      "offset": 80.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "just won the Nobel Prize for AlphaFold.",
      "offset": 82.24,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "I then was an NSF postoc. Uh I I my",
      "offset": 84.32,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "other um you may know another classmate",
      "offset": 87.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "of mine Jurgen Schmidhuber who basically",
      "offset": 90.08,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "claims to have invented everything else.",
      "offset": 92.64,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "So I did",
      "offset": 94.799,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "my so I've been doing this for a very",
      "offset": 96.439,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "long time. I really got I mean I I've",
      "offset": 99.04,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "been doing working in industry you know",
      "offset": 101.04,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I left grad school and postto went into",
      "offset": 102.32,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "industry. I've been doing consulting",
      "offset": 103.92,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "work doing building AI and machine",
      "offset": 105.119,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "learning solutions for companies",
      "offset": 107.399,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "forever. I I did Arvar was acquired by",
      "offset": 109.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Google. I did e-How first billion dollar",
      "offset": 111.6,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "IPO since Google and probably the",
      "offset": 113.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "biggest crash. Um I was a quant on Wall",
      "offset": 115.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Street. I was also been scientific",
      "offset": 118.24,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "adviser to Larry Page's family. So and",
      "offset": 120.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and during the course of that I about 10",
      "offset": 122.719,
      "duration": 2.601
    },
    {
      "lang": "en",
      "text": "years",
      "offset": 124.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "ago I decided to get back into uh AI",
      "offset": 125.32,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "research working with my friend Michael",
      "offset": 129.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "Mahoney at UC Berkeley very informally",
      "offset": 131.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and we've been working on this project",
      "offset": 133.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "called uh what I call the Weight Watcher",
      "offset": 134.8,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "project which is a large which is an",
      "offset": 137.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "open source tool. We have almost 200,000",
      "offset": 138.879,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "downloads which is designed to help",
      "offset": 140.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "people who are monitoring training and",
      "offset": 142.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "fine-tuning their own AI models. And all",
      "offset": 145.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "of this has been just really a passion",
      "offset": 147.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "project to get back into research using",
      "offset": 150.319,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "techniques from theoretical physics and",
      "offset": 152.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "chemistry to understand how these models",
      "offset": 154.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "work and how to help my clients. And",
      "offset": 156.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you've been at this for a while. Uh I",
      "offset": 158.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "think we first got to know one another",
      "offset": 161.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "in the the deep learning era. Uh now",
      "offset": 163.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "we're all talking about these large",
      "offset": 166,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "language models. How does this stuff",
      "offset": 167.68,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "apply to LLMs? Well, it it actually",
      "offset": 169.12,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "works really well. Um I I designed I did",
      "offset": 171.519,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "theory. uh but as a theorist I you know",
      "offset": 174.64,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "I work with engineers so I know that",
      "offset": 176.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "theory you know they say you know in",
      "offset": 178.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "theory practice and theory are the same",
      "offset": 180.319,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "and in practice they're different right",
      "offset": 182.4,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "so I started I actually got into this I",
      "offset": 185.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "had a client in all places of Slovenia",
      "offset": 188.159,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "and we were they were making like fake",
      "offset": 190.64,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "text for things like weight loss",
      "offset": 193.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "articles Amazon reviews things like that",
      "offset": 196.04,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "so we were gen this is before even like",
      "offset": 198.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "open AI stuff came out uh and we're",
      "offset": 200.159,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "generating this text I realized I'm",
      "offset": 202.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "generating like 50,000 pieces of text a",
      "offset": 203.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "day. I have no way to know if what I'm",
      "offset": 205.84,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "generating is any good or not. And I",
      "offset": 209.04,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "can't hire 500 people to evaluate it,",
      "offset": 211.519,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you know, so it would blow my cost. Like",
      "offset": 214.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I've got to get a theory. I've got to",
      "offset": 216.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "invent some kind of theory that would",
      "offset": 218,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "let me know whether these AI models are",
      "offset": 220.08,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "working correctly or not. And I started",
      "offset": 221.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "working on my spare time and working on,",
      "offset": 224.08,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "you know, kind of cracking the books",
      "offset": 225.68,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "and, you know, acting like a scientist",
      "offset": 226.959,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "again. And and that's where it's come",
      "offset": 228.959,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "up. So it turns out it applies really",
      "offset": 230.879,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "well to LLMs because they're really big",
      "offset": 232.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "and they have lots and lots of layers",
      "offset": 235.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "and this is basically theory which",
      "offset": 238,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "analyzes the weights in the layers. It's",
      "offset": 239.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "weight watcher so it looks at the layer",
      "offset": 241.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "weight matrices and so the more layers",
      "offset": 243.84,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "you have the bigger the model the better",
      "offset": 245.92,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "the theory works. It's kind of",
      "offset": 247.439,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "interesting to me that even in that, you",
      "offset": 248.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know, practical context, so working with",
      "offset": 250.72,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "a client that has a job to be done as",
      "offset": 253.2,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "opposed to in academia, the approach you",
      "offset": 254.959,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "took was a theoretical approach as",
      "offset": 257.199,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "opposed to a more, you know, let's call",
      "offset": 258.799,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it a statistical course like the",
      "offset": 261.359,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "application of evals as a practice as",
      "offset": 262.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "opposed to um you know a theoretical",
      "offset": 264.56,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "take on it. I I have I did all this",
      "offset": 268.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "stuff in the 90s and I have this",
      "offset": 270.639,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "background in theoretical physics and",
      "offset": 271.84,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "chemistry. So and and of course I was at",
      "offset": 273.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Quantum Wall Street. So some of the",
      "offset": 275.199,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "techniques I'm using actually are quant",
      "offset": 276.96,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "techniques that we used on Wall Street",
      "offset": 278.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to predict the markets. So I knew how to",
      "offset": 280.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "apply these techniques to real systems",
      "offset": 283.12,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "because we use that's how we that's what",
      "offset": 286,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "we do. We do portfolio theory. What's an",
      "offset": 287.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "example of uh a technique that and how",
      "offset": 289.44,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "is it used on Wall Street and how are",
      "offset": 292.32,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you using it in your project? So in",
      "offset": 293.919,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "portfol when you're like at a big place",
      "offset": 296.479,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "I was at Black Rockck you know that's",
      "offset": 298,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "the 800 pound gorilla on Wall Street and",
      "offset": 299.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we we have you know the the group I was",
      "offset": 300.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "in you couldn't even trade in the group",
      "offset": 303.04,
      "duration": 3.999
    },
    {
      "lang": "en",
      "text": "unless you had $200 million right that",
      "offset": 304.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "was the scale so it was a big portfolio",
      "offset": 307.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and you have these big portfolios and",
      "offset": 309.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you have to figure out signal versus",
      "offset": 311.039,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "noise where's the signal in the",
      "offset": 313.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "portfolio versus noise you're trying to",
      "offset": 315.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "reallocate how much Google how much",
      "offset": 316.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Apple how much GM how much you know that",
      "offset": 318.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "kind of stuff in the portfolio and so",
      "offset": 320.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you have to decide signal versus noise",
      "offset": 322.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and so you use something called random",
      "offset": 324,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "matrix theory to do that and you can",
      "offset": 325.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "detect the signal and the noise and it's",
      "offset": 327.28,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "important that you not peak at the stock",
      "offset": 329.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "market data you not peak at things",
      "offset": 333.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "because you'll you'll overfit yourself",
      "offset": 334.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to the history so there's two things",
      "offset": 337.039,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "going I'm using random matrix there is",
      "offset": 339.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "what we do and I'm making sure not to",
      "offset": 340.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "look at the data because if you look at",
      "offset": 342.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the data you'll fool yourself right",
      "offset": 344.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you'll think you know so that idea is",
      "offset": 346.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "actually essentially you can think of",
      "offset": 349.039,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "weight watcher as and trying to find the",
      "offset": 350.24,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "signal from noise in a model by looking",
      "offset": 352.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "at the individual weight matrices which",
      "offset": 355.759,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "are like little portfolios. They're like",
      "offset": 357.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the portfolios you would form when",
      "offset": 359.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you're doing portfolio theory. And I",
      "offset": 360.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "knew about some of the theory because we",
      "offset": 362.88,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you we do we do theory when you're at",
      "offset": 364.96,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "quant. And so it turned out there were",
      "offset": 366.6,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "some very interesting and and they're",
      "offset": 368.639,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "interesting scientific properties. You",
      "offset": 370.08,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "know, as a scientist, I started studying",
      "offset": 371.52,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "this stuff and I go, you know, what I",
      "offset": 372.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "did was I just took the models that",
      "offset": 374.479,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "people had trained. They these open",
      "offset": 376,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "source models, right? Today we have",
      "offset": 377.36,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "hugging face. There are a million models",
      "offset": 378.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "on hugging face. When I started there",
      "offset": 380.24,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "were less than a hundred open source",
      "offset": 382.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "models. I started looking at them,",
      "offset": 383.72,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "right? What do they look like? which",
      "offset": 386,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what are the what do the good ones look",
      "offset": 387.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "like versus the bad ones? Just like when",
      "offset": 389.12,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "you're a quant, what do good companies",
      "offset": 391.44,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "look like versus bad companies? And you",
      "offset": 393.759,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "analyze their properties. And it turned",
      "offset": 395.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "out there was some interesting science",
      "offset": 398.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "behind this. Um, and one of the guys I",
      "offset": 400,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "worked with, the guy who's the head of",
      "offset": 403.28,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "futures and equities at Black Rockck was",
      "offset": 404.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a theoretical physicist. And you know,",
      "offset": 406.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "he showed me some of the stuff they were",
      "offset": 408.16,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "doing. You know, we can apply this to",
      "offset": 409.52,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "AI. And it just turned out to work, you",
      "offset": 411.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know, and I just kept digging and",
      "offset": 413.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "digging and digging. it just worked",
      "offset": 415.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "better and better and better and then I",
      "offset": 416.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "discovered there's some some deep",
      "offset": 418.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "science in this. So that's where we",
      "offset": 420.319,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "ended up. And so traditionally in the ML",
      "offset": 422.24,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "AI world, the way to overcome that, you",
      "offset": 426.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "know, overfitting on the past data is to",
      "offset": 430.16,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "split it into test and train and to only",
      "offset": 432.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "look at part of it. Are you uh you're",
      "offset": 434.88,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "not looking at any of it? No. No. And in",
      "offset": 438.639,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "fact, you don't do that in physics.",
      "offset": 441.039,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "Yeah. Like when you do analysis, you you",
      "offset": 442.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you I have a very I have like a 100 I",
      "offset": 444.4,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "have this 120 long 20 page paper",
      "offset": 446.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "theoretical physics explains how we do",
      "offset": 449.84,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "things but it's it's actually different",
      "offset": 451.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "right we actually what you're doing is",
      "offset": 453.039,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you're looking at it turns out the",
      "offset": 455.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "weight matrices when you train a model",
      "offset": 457.52,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "really really well the weight matrices",
      "offset": 460.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have these universal property I call",
      "offset": 463.36,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "them signatures of emergence",
      "offset": 464.88,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "and they actually this this idea",
      "offset": 467.4,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "actually comes from I actually because I",
      "offset": 469.759,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "did AI in the '90s I know something",
      "offset": 471.52,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "about neuroscience. It turns out these",
      "offset": 473.039,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "signatures are very similar to the",
      "offset": 474.879,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "signatures you see when studying spiking",
      "offset": 476.56,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "neurons and it turns out spiking neurons",
      "offset": 479.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "exhibit something called parallel",
      "offset": 481.759,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "structure and they have these sort of",
      "offset": 483.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "universal properties and we knew about",
      "offset": 484.8,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "them and sort of the original tool is I",
      "offset": 486.319,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "took these tools from computational",
      "offset": 488.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "neuroscience and I just applied them to",
      "offset": 489.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the layers of weight matrix and it turns",
      "offset": 491.52,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "out they have the same signals. So it",
      "offset": 492.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "turns out that the spatial temporal",
      "offset": 495.36,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "correlations in the weight matrices in",
      "offset": 497.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the layers in the neurons are very",
      "offset": 499.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "similar to what you see in actual",
      "offset": 501.44,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "neurons like that you would cult you",
      "offset": 503.199,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "take a lab you culture them you grow",
      "offset": 505.599,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "them in the lab and you look and there",
      "offset": 506.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "it's the same thing and and this work",
      "offset": 508.56,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "was pioneered actually by um some",
      "offset": 510.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "physicist in the 90s in particular a",
      "offset": 512.479,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "friend of you know some guys a guy named",
      "offset": 513.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Jack Cow University of Chicago has done",
      "offset": 515.599,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "a lot of work on this he's sort of",
      "offset": 517.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "invented a lot of this stuff so it turns",
      "offset": 518.08,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "out this stuff works um and it it's",
      "offset": 520,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "actually very simple",
      "offset": 522.399,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "If you train a model, you're training a",
      "offset": 524,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "layer, right? But most people think of",
      "offset": 526.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "training the model as I minimize the",
      "offset": 527.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "training accuracy. But you minimize the",
      "offset": 529.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "error, right? Or you minimize the error,",
      "offset": 531.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "right? Accuracy of the error. Either",
      "offset": 533.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Yeah. Excuse me. You minimize the error,",
      "offset": 535.2,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "not the accuracy, right? Maximize the",
      "offset": 537.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "accuracy, right? Right. Right. Right.",
      "offset": 538.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "So, think of it like baking a cake. When",
      "offset": 541.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you bake a cake, you you have the",
      "offset": 543.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "temperature, you watch the cake. Imagine",
      "offset": 544.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a cake has a lot of layers. Well, if the",
      "offset": 546,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "oven's too hot, some layers are going to",
      "offset": 548.16,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "burn and the inside's not going to get",
      "offset": 550.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "cooked because you don't get good, you",
      "offset": 552.36,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "know, conduction through the heat. So,",
      "offset": 554.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what you want, even when you bake a",
      "offset": 556.48,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "cake, you still have to be careful to",
      "offset": 557.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "adjust the temperature right to make",
      "offset": 559.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "sure that the layers all cook at the",
      "offset": 561.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "same way. Uh, and it's the same idea in",
      "offset": 562.88,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a model is that if you know, if if",
      "offset": 565.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "you're training like for example, if you",
      "offset": 568.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "turn the learning rate up too high, it",
      "offset": 569.519,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "turns out some of the layers overfit. So",
      "offset": 571.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "they become they they they become too",
      "offset": 574.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "there's too much information in them and",
      "offset": 575.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "they just overfit and all of a sudden",
      "offset": 578,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "they stop generalizing and and we have a",
      "offset": 580,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "very interesting paper that I've done",
      "offset": 582.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "with a fellow who came out of uh just",
      "offset": 584.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "getting his master's degree now super",
      "offset": 586.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "smart guy and he he was looking at this",
      "offset": 587.68,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "and he said well what do I take this",
      "offset": 589.2,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "whole groing problem and it turns out if",
      "offset": 590.399,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "you take a simple model and you train it",
      "offset": 591.92,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "for a very very long period of time you",
      "offset": 594.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "even if you don't turn the learning rate",
      "offset": 597.279,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "crazy up high you'll see that it starts",
      "offset": 598.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to overfit and we can detect this Um,",
      "offset": 600.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "and the signature of overfitting. Um,",
      "offset": 603.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know, really the reason I started",
      "offset": 606.64,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "thinking is because, you know, if I were",
      "offset": 608,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "if I were trading in the stock market",
      "offset": 609.279,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and I were making an AI model, what's",
      "offset": 611.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the one thing you don't want to do? You",
      "offset": 613.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "don't want to overfit to the history.",
      "offset": 615.6,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Over fit on the history. Yeah. Because,",
      "offset": 616.959,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "you know, and I, believe it or not, when",
      "offset": 618.24,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I was at Black Rockck, we saw guys doing",
      "offset": 619.279,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "this. I'm like, what are you doing? You",
      "offset": 620.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know, like you can't you can't even do",
      "offset": 622.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this stuff. Well, that's the classic",
      "offset": 624.32,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "thing. You learn a little bit of machine",
      "offset": 625.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "learning. You say, let me download all",
      "offset": 627.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of the historical data and train a model",
      "offset": 628.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "on it.",
      "offset": 630.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Yeah. Hey, you got like here's a guy a",
      "offset": 632.56,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "PhD from electrical engineering from",
      "offset": 633.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "Stanford. I'm like what are you guys",
      "offset": 635.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "doing? You know, but but this is a",
      "offset": 636.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "classic thing and so it's very critical",
      "offset": 638.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "to understand when the overfitting",
      "offset": 640.72,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "occurs and so that you know having been",
      "offset": 642.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "a quant on Wall Street are really really",
      "offset": 643.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "sensitive to this kind of stuff. And it",
      "offset": 646.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "turns out there's some ideas from",
      "offset": 648.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "physics that are like phase transitions",
      "offset": 650.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and where you and the stuff that was we",
      "offset": 652,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "knew this stuff happened in the 90s like",
      "offset": 653.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we understood the theory. Um the thing",
      "offset": 655.68,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is we're physicists. The physicists and",
      "offset": 657.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "the computer scientists didn't really",
      "offset": 659.36,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "talk that much.",
      "offset": 660.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So you know that so we have these",
      "offset": 662.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "theories from physics that we know we",
      "offset": 663.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "can detect this stuff but how do you",
      "offset": 665.36,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "apply it and that's what we're trying to",
      "offset": 667.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "figure out how to do and it turns out",
      "offset": 669.839,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that yeah you could detect when the",
      "offset": 671.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "model overfits and when it's under fit",
      "offset": 672.959,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "and you can see it in the layers it's",
      "offset": 674.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "just like you could see this layer is",
      "offset": 676.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "burnt it it it it absorbed too much",
      "offset": 678,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "information and absorbed so much",
      "offset": 680.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "information from the training data it it",
      "offset": 682.399,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "it got stuck and it doesn't learn",
      "offset": 684.48,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "anything just it's overfit right um and",
      "offset": 686.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "then there are cases where the layers",
      "offset": 689.279,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "are underfit",
      "offset": 690.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "like they're too the bottle's too big or",
      "offset": 692,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "something's going on. You know, the",
      "offset": 694.56,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "layers just didn't really learn",
      "offset": 695.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "anything. So, we we can detect that very",
      "offset": 697.04,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "easily. So, we'll dig into this paper.",
      "offset": 699.76,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "Uh before we do that,",
      "offset": 702.399,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "um a contextual context setting question",
      "offset": 705.079,
      "duration": 6.041
    },
    {
      "lang": "en",
      "text": "and then a possible rabbit hole. So the",
      "offset": 709.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "context setting question is uh one",
      "offset": 711.12,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "application of all this is uh",
      "offset": 713.68,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "fine-tuning models and",
      "offset": 717.24,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "um when we were chatting before you were",
      "offset": 720.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "talking about how fine-tuning is really",
      "offset": 722.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "difficult and a lot of folks get it",
      "offset": 723.76,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "wrong and find it really frustrating.",
      "offset": 725.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Other folks I hear from say, &quot;Oh,",
      "offset": 727.279,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "fine-tuning's so easy right now.&quot; Uh,",
      "offset": 728.88,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "compared to, you know, a few years ago,",
      "offset": 731.36,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "like a like uh kind of square that",
      "offset": 734.399,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "circle for me. And talk a little bit",
      "offset": 737.839,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "about your specific experience with",
      "offset": 739.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "regards to fine-tuning. So, if you you",
      "offset": 741.12,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "look at what people are doing, there's a",
      "offset": 743.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "report by Kenzie McKenzie that said",
      "offset": 744.399,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "maybe 9% of companies doing AI are",
      "offset": 746.8,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "fine-tuning. So, the rest are probably",
      "offset": 749.32,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "doing prompt engineering. And and the",
      "offset": 751.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "question becomes you know look some",
      "offset": 753.519,
      "duration": 2.841
    },
    {
      "lang": "en",
      "text": "people most people do I'd say out of",
      "offset": 754.72,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "those probably of those only 9% are",
      "offset": 756.36,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "really doing full fine-tuning on very",
      "offset": 760.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "large data sets. Yeah, you could do",
      "offset": 762.639,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "something like Laura which called low",
      "offset": 764.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "low rank adaptation and you can kind of",
      "offset": 765.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "tweak the model to you know if you want",
      "offset": 768.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "to get a JSON output well or you're",
      "offset": 770.079,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "trying to get markdown output you just",
      "offset": 772,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "tweak the model a little bit to change",
      "offset": 773.519,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "its outputs you can kind of steer it a",
      "offset": 774.959,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "little bit right but if you want to add",
      "offset": 777.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a huge amount of data to your model and",
      "offset": 779.839,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "you want the model to learn from that",
      "offset": 782.88,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "data while still you know keeping its",
      "offset": 784.8,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "its own knowledge it turns out that it's",
      "offset": 788.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it's actually quite hard. It's hard for",
      "offset": 790.399,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a number of reasons. fun because I've",
      "offset": 791.92,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "worked in",
      "offset": 793.519,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "enterprises it it's hard to get good",
      "offset": 795.16,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "data in the enterprise and that's part",
      "offset": 797.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "of it is just I I you know and I I just",
      "offset": 799.519,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "talked to a client the other a couple",
      "offset": 801.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "weeks ago we said yeah we had we had to",
      "offset": 803.839,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "we had our model running for a year in",
      "offset": 805.2,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "production we had to dump it and start",
      "offset": 806.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "over because we didn't realize it was",
      "offset": 808.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "broken because something happened in the",
      "offset": 809.839,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "data pipeline the data pipeline stopped",
      "offset": 811.839,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "you know the somebody changed a column",
      "offset": 814.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "table or they changed something and it",
      "offset": 816.32,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "screwed up the model and you know doing",
      "offset": 817.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "I've worked with projects I've worked",
      "offset": 819.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "with GoDaddy and eBay and Walmart and",
      "offset": 821.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "these things happen all the time. You",
      "offset": 824.56,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "know, you're in a production",
      "offset": 826.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "environment, the data changes, you don't",
      "offset": 826.959,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "know it changed where they told you, you",
      "offset": 828.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "know, three months later the model's cra",
      "offset": 831.279,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "the model's not working. What happened?",
      "offset": 833.92,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "So, you know, this is the problem with",
      "offset": 835.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "fine-tuning or you know, because you",
      "offset": 838.079,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "know you're or even training your own",
      "offset": 840,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "model from scratch that the data goes",
      "offset": 841.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "crazy and you don't know it. Um, and",
      "offset": 843.24,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "that's probably and then the thing is",
      "offset": 846,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "you have how do you prepare good data",
      "offset": 847.279,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "sets? data sets are they're duplicates",
      "offset": 848.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and there's noise and there's problems",
      "offset": 850.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "and so in a in a real environment a",
      "offset": 853.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "production environment a big in a legacy",
      "offset": 855.279,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "company this is very hard um so yeah if",
      "offset": 857.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you're just training on a very small",
      "offset": 860.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "data set and you can look at the data",
      "offset": 861.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "yourself and curate it you could",
      "offset": 863.68,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "probably get the data right but when",
      "offset": 865.12,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "you're in a you know production",
      "offset": 866.72,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "environment with I' I've worked like you",
      "offset": 867.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know Walmart millions of examples on the",
      "offset": 869.279,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "clickstream you know from the from the",
      "offset": 871.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "search engine you can't curate it",
      "offset": 872.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "yourself you need you need tools which",
      "offset": 874.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "let you look at the model and say did",
      "offset": 876.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "something go",
      "offset": 878.399,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "um it's very hard because you don't you",
      "offset": 880,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "just don't know you don't have",
      "offset": 881.92,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "visibility. The the other thing that",
      "offset": 882.88,
      "duration": 3.89
    },
    {
      "lang": "en",
      "text": "makes fine-tuning hard is just that",
      "offset": 884.8,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 886.77,
      "duration": 4.99
    },
    {
      "lang": "en",
      "text": "um you know you never really know is is",
      "offset": 888.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "is whether you're evaluating the model",
      "offset": 891.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "correctly. There are there are a hundred",
      "offset": 894.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "different evals and you know there's all",
      "offset": 896.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this controversy and I remember llama 4.",
      "offset": 899.04,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "Oh, they they let it put lama four out",
      "offset": 901.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "books. They cooked the books, right?",
      "offset": 903.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "They snuck it in. It turned out now the",
      "offset": 905.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "and then it turns out that the the LMC",
      "offset": 907.76,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "guys, you know, they they've been",
      "offset": 910.32,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "secretly h, you know, handing over the",
      "offset": 912.639,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "answers, like 25% of the answers to",
      "offset": 914.959,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "Google and that's the coher paper that",
      "offset": 916.959,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "came out a couple weeks ago. Yes. Yeah.",
      "offset": 918.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "The coher I gave a talk at coheri a few",
      "offset": 920.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "weeks ago. Yeah. So, so they've been",
      "offset": 923.12,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "cooking the book. So, you don't really",
      "offset": 924.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know what the model can do, right? So,",
      "offset": 925.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "don't say that though because they just",
      "offset": 928.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "raised a ton of money in there, you",
      "offset": 929.76,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "know, uh, you know, it's well, you know,",
      "offset": 931.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "they got they're trying to figure out,",
      "offset": 934.88,
      "duration": 1.759
    },
    {
      "lang": "en",
      "text": "right? They're trying to figure out",
      "offset": 935.92,
      "duration": 2.479
    },
    {
      "lang": "en",
      "text": "what's going on. So, look, I work with",
      "offset": 936.639,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "real clients and my clients when my",
      "offset": 938.399,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "models don't work, they don't pay me.",
      "offset": 940,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "Like, you",
      "offset": 942.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "know, so I, you know, I, you know, it it",
      "offset": 944.12,
      "duration": 4.519
    },
    {
      "lang": "en",
      "text": "reminds me of the difference. Do you",
      "offset": 947.68,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "ever see the old movie Ghostbusters?",
      "offset": 948.639,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Uhhuh. I remember they they're getting",
      "offset": 950.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "ready to leave the university and and uh",
      "offset": 952.079,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "I think it was um I can't remember which",
      "offset": 954.72,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "one of them said he goes look man you",
      "offset": 956.48,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "you've spent your whole life in",
      "offset": 958.399,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "academics you've never been in the real",
      "offset": 959.759,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "world they expect",
      "offset": 961.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "results so I'm coming from that",
      "offset": 964.12,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "perspective like so you know it's yes of",
      "offset": 966.24,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "course the tool the problem that's",
      "offset": 969.519,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "happened is that in the industry is that",
      "offset": 971.199,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "the tooling has gotten easier and with",
      "offset": 972.639,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "the tooling getting",
      "offset": 975.04,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "easier there's just there's a lower bar",
      "offset": 976.6,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "to entry everybody's trying to do things",
      "offset": 979.199,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "and so there's a much wider variance in",
      "offset": 981.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "what's going on and so we see that I",
      "offset": 983.44,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "don't see a lot of people if they are",
      "offset": 986.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "fine-tuning you know there's a lot of",
      "offset": 987.519,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "problems things break and the goal of",
      "offset": 989.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this product was to figure out how to",
      "offset": 991.36,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "make fine-tuning work really well and to",
      "offset": 994,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "detect problems in production so if",
      "offset": 996.639,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "you're retraining a model every day",
      "offset": 998.24,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "every you know I worked in search engine",
      "offset": 999.92,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "retrain every hour but you might retrain",
      "offset": 1001.6,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "your model once a month maybe once a",
      "offset": 1003.759,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "week once a month you want to know you",
      "offset": 1005.519,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "want to make sure things didn't drift",
      "offset": 1007.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "and go crazy. And this technology is",
      "offset": 1008.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "designed to help you find those kinds of",
      "offset": 1011.279,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "problems that you to find problems you",
      "offset": 1013.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can't find using any other technique.",
      "offset": 1015.279,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "But one of the things you said uh that",
      "offset": 1017.6,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "was pretty interesting was",
      "offset": 1019.44,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "uh kind of the suggestion that there are",
      "offset": 1022.279,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "different types or degrees of",
      "offset": 1025.079,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "fine-tuning. Like there's a you know",
      "offset": 1027.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "surface level fine-tuning and a deeper",
      "offset": 1030.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "fine-tuning is, you know, I'm going to",
      "offset": 1032.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "force fit your cake analogy. Like you've",
      "offset": 1034.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "got a vanilla cake. Do you want just",
      "offset": 1037.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "like chocolate icing or do you want",
      "offset": 1038.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "chocolate at the bottom layer or you",
      "offset": 1040.64,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "want to stick a layer in between? That's",
      "offset": 1042.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "exactly right. Yeah. If you're just",
      "offset": 1044.079,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "putting sprinkles on the cake, it's not",
      "offset": 1045.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "a big deal. Okay. But if you're trying",
      "offset": 1046.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "to, you know, stick something in the",
      "offset": 1048.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "middle, you know, you and I've not",
      "offset": 1050,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "really seen like any like concrete, you",
      "offset": 1052.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "know, taxonomy or elucidation of like",
      "offset": 1056.24,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "degrees of fine-tuning and or what",
      "offset": 1058.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "characterizes an easy finetune, what",
      "offset": 1060.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "characterizes a hard fine tune. You have",
      "offset": 1062.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "models that are instruction fine-tuned.",
      "offset": 1064.24,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "Okay. So instruction fine-tuning for us",
      "offset": 1066.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the our technology is really you're",
      "offset": 1068.799,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "doing instruction fine-tuning on a on a",
      "offset": 1070.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "100 thousand examples or a million",
      "offset": 1071.919,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "examples. We were working, for example,",
      "offset": 1073.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "with a group in Poland and they're",
      "offset": 1074.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "trying to do instruction fine-tuning on",
      "offset": 1076.64,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "a model to try to adapt it to the Polish",
      "offset": 1079.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "language and we found there were some",
      "offset": 1081.08,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "funny things going on like they were",
      "offset": 1083.12,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "they're trying to do there's a model",
      "offset": 1084.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "called solar which is quite good and",
      "offset": 1085.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they were trying to adapt the solar",
      "offset": 1088.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "technology to their model called bio and",
      "offset": 1090.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the model's not bad right but something",
      "offset": 1091.76,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "happened inside the fine-tuning and the",
      "offset": 1094.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "model training that somehow whatever",
      "offset": 1095.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "they did the weights and the weight",
      "offset": 1098.16,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "matrices and the layers doesn't and by",
      "offset": 1100.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the way this is all published in city",
      "offset": 1101.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "publication it it doesn't look like",
      "offset": 1103.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "solar. Like something went wrong and we",
      "offset": 1105.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "couldn't figure out what it was. Like",
      "offset": 1107.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "what what did you guys do, you know, cuz",
      "offset": 1108.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you're trying to replicate the engine.",
      "offset": 1110.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "You want to keep what you like about the",
      "offset": 1113.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "base model, but add some behavior or add",
      "offset": 1114.96,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "some knowledge or add something uh at",
      "offset": 1117.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the same time. And it sounds like they",
      "offset": 1120.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "broke something fundamental. They broke",
      "offset": 1122.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "something. And the thing is, you're",
      "offset": 1124.559,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "trying to follow an instruction that",
      "offset": 1125.919,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "somebody else has given you, but you",
      "offset": 1128.32,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don't really know what's like you're",
      "offset": 1130.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "trying to do something a little",
      "offset": 1132.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "different and it's difficult to make the",
      "offset": 1133.52,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "recipe exact, right? You changed",
      "offset": 1136.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "something a little bit and now you know",
      "offset": 1138.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you used a different kind of flour, a",
      "offset": 1140.64,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "different kind of oil, a different kind",
      "offset": 1142.24,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "of butter. Something went wrong and you",
      "offset": 1143.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "don't know why. And and this stuff is so",
      "offset": 1145.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "e ephemeral, you know, it's so opaque.",
      "offset": 1147.76,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "You know, whatever you do on one data",
      "offset": 1151.12,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "set might not work on another data set",
      "offset": 1152.799,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "for whatever reason. We don't know why.",
      "offset": 1154.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "And that's what we see happening is that",
      "offset": 1156.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they tried to replicate the training",
      "offset": 1158.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "process exactly, but when you look at",
      "offset": 1159.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the models, they're quite different.",
      "offset": 1161.84,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "What what did why did what happened? And",
      "offset": 1163.12,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "so quite different in terms of the the",
      "offset": 1165.679,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "weight behavior and performance or the",
      "offset": 1169.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "Okay. Some from the perspective of your",
      "offset": 1172.32,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "Yeah. So, one thing that this tool does",
      "offset": 1175.28,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "is it makes clear some set of",
      "offset": 1178.559,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "differences between these two, you know,",
      "offset": 1180.679,
      "duration": 4.201
    },
    {
      "lang": "en",
      "text": "what they started with and what they",
      "offset": 1183.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "ended up with. And the problem is that",
      "offset": 1184.88,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "you don't know what the problems are",
      "offset": 1187.039,
      "duration": 2.481
    },
    {
      "lang": "en",
      "text": "going to be until you go into",
      "offset": 1188.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "production. People start doing things.",
      "offset": 1189.52,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "You know, there are a million things you",
      "offset": 1191.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "could do. You're trying to, like I said,",
      "offset": 1192.799,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "you're generating fake texts. You're",
      "offset": 1194,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "answering questions. You're doing",
      "offset": 1195.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "things. You don't know how to respond.",
      "offset": 1196.64,
      "duration": 6.919
    },
    {
      "lang": "en",
      "text": "So, we're already a client of yours and",
      "offset": 1200.32,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "like it was natural to use Weight",
      "offset": 1203.559,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "Watcher like were they use weight using",
      "offset": 1207.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Weight Watchers from the beginning and",
      "offset": 1209.2,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "so it was clear what No, no. They came",
      "offset": 1210.72,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in later. What happens? They're doing it",
      "offset": 1212.32,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "they train the model. What did they see",
      "offset": 1213.84,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "then? Yeah. What did they see that said",
      "offset": 1215.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "we need help with this? The layer Weight",
      "offset": 1217.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Watcher gives you a layer quality",
      "offset": 1219.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "metric. So if I have a model with a",
      "offset": 1221.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "thousand layers and you have some of",
      "offset": 1223.44,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "these models now have a thousand layers",
      "offset": 1225.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "right but you have a thousand layers",
      "offset": 1226.799,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "maybe a 100 layers in your model every",
      "offset": 1228.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "layer gets a quality metric what's the",
      "offset": 1230.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "score on the layer and that score should",
      "offset": 1232.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "be somewhere between two and four two",
      "offset": 1234.559,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "and six in that range two say two to",
      "offset": 1236.08,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "five now they now what you find when you",
      "offset": 1240.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "fine-tune is you can look at the",
      "offset": 1242.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "fine-tuning update so I train the model",
      "offset": 1244.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "here's the update I want to look at the",
      "offset": 1246.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "update if you've train if the update",
      "offset": 1247.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "should show good quality scores between",
      "offset": 1250.48,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "two and four, two and five. If you look",
      "offset": 1253.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "at the instruction fine-tuning updates",
      "offset": 1255.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "of Llama, Quinn, Falcon, Mistral, they",
      "offset": 1257.28,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "all",
      "offset": 1260.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "show reasonably good layer qualities.",
      "offset": 1261.159,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "You look at these, even solar, their",
      "offset": 1264.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "instruction fine-tuning for some reason",
      "offset": 1266.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "had a large number of layers that seem",
      "offset": 1269.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "to be underfit. The layer quality was",
      "offset": 1270.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "too high. I should say quality. The",
      "offset": 1272.96,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "quality was low in the sense that the",
      "offset": 1274.72,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "the metric was above six, maybe nine,",
      "offset": 1276.72,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "maybe 10, maybe 15, way too high, which",
      "offset": 1280.24,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "indicates the layer is almost random. So",
      "offset": 1282.64,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "what happened that those layers for some",
      "offset": 1285.039,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "reason did not learn any",
      "offset": 1287.039,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "information or if they learned it, they",
      "offset": 1289.799,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "learned it very weakly. There's only a",
      "offset": 1291.919,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "very small amount of information those",
      "offset": 1293.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "layers learned. And and that's, you",
      "offset": 1295.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "know, you're wasting a lot of compute.",
      "offset": 1297.679,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you know, you're spending all I mean,",
      "offset": 1300,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "they're running on a supercomputer in",
      "offset": 1301.2,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "Poland. So, you know, they're they're",
      "offset": 1302.4,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "spending all this compute time, energy",
      "offset": 1303.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "to figure out what's going on. And",
      "offset": 1305.679,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "that's what the tool is telling you. You",
      "offset": 1307.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "know, there there's a step before that,",
      "offset": 1309.12,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "what was the thing that they were",
      "offset": 1311.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "observing in the training process that",
      "offset": 1313,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "that caused them to call you? Was it",
      "offset": 1316.559,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "just Well, performance or they're Yeah.",
      "offset": 1318.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah. They're just trying to get the",
      "offset": 1321.679,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "performance up. They're trying different",
      "offset": 1322.799,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "things. They're again, they don't really",
      "offset": 1324.159,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "know what to Yeah. You read a paper,",
      "offset": 1326.679,
      "duration": 4.921
    },
    {
      "lang": "en",
      "text": "there's instructions, there's code, but",
      "offset": 1329.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "when you run on your data set, it does",
      "offset": 1331.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "something different. So, we read this",
      "offset": 1333.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "paper. Based on everything we read, we",
      "offset": 1335.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "should be able to take our data, apply,",
      "offset": 1337.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "you know, a fine-tuning approach to it",
      "offset": 1339.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and see performance like this. But we're",
      "offset": 1342.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "seeing performance down here. Can you",
      "offset": 1343.76,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "help us figure out what's wrong, right?",
      "offset": 1346,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "And so, one of the things they they're",
      "offset": 1347.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "trying one of the things people do now",
      "offset": 1348.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "is on these big models, they're doing",
      "offset": 1350.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "all sorts of it used to be you take a",
      "offset": 1352.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "model, you train it, right? Now people",
      "offset": 1354.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "are trying to like they tried to",
      "offset": 1356.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "replicate part of the model. Here's part",
      "offset": 1357.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "of the model and we're going to take",
      "offset": 1359.2,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "these layers and replicate them and",
      "offset": 1360.4,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "stick them up here. Right? And then",
      "offset": 1361.919,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "we're going to take two models. We're",
      "offset": 1364.159,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "going to merge them together. Right? So",
      "offset": 1365.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "they do all these funny things, right?",
      "offset": 1367.2,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And oh, and then people say, &quot;Oh, you",
      "offset": 1369.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "can replicate layers. You can merge",
      "offset": 1371.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "models together.&quot; And it's kind of like",
      "offset": 1373.12,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that's a little strange, you know? And",
      "offset": 1375.679,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it turns out it gives goofy results. And",
      "offset": 1377.679,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that's the problem, right? And you don't",
      "offset": 1380.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "know did you do it correctly or not.",
      "offset": 1381.84,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "They just don't know. You just don't",
      "offset": 1383.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "know because you don't really know like",
      "offset": 1385.28,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "the recipe doesn't have enough detail",
      "offset": 1387.679,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "and it depends on the ingredients which",
      "offset": 1390.159,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "are you know which change they're not",
      "offset": 1392.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "stable. So that's what makes this stuff",
      "offset": 1394.559,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "so hard and and similar to fine-tuning",
      "offset": 1396.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like if you don't have the exact data",
      "offset": 1398.919,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and maybe the hyperparameters were wrong",
      "offset": 1401.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "like maybe our maybe the hyperparameters",
      "offset": 1403.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "for this data set are different from the",
      "offset": 1406.4,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "hyperparameters for our data set. How do",
      "offset": 1408.4,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we select the hyperparameters? How do we",
      "offset": 1411.039,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "select the learning rates for each",
      "offset": 1413.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "layer? Should we have dropout? Should we",
      "offset": 1415.039,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "not have dropout? Should we have weight",
      "offset": 1416.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "decay? Should we not have weight decay?",
      "offset": 1418,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "These are the open questions and it",
      "offset": 1419.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "changes from data set to data set. How",
      "offset": 1421.36,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "would you know? And and this is the kind",
      "offset": 1423.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "of thing that we found that it was very",
      "offset": 1425.679,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "difficult to try to and especially",
      "offset": 1427.44,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because you're running on a supercomput.",
      "offset": 1429.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "You could run it once right on the",
      "offset": 1430.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "computer. It's it runs for a few weeks.",
      "offset": 1433.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "You come back, you know, you're not",
      "offset": 1435.52,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Google. You can't run it a 100 times on",
      "offset": 1437.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "their million node cluster. I mentioned",
      "offset": 1439.679,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "that I had a possible degression rabbit",
      "offset": 1442.88,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "hole and that is uh when you were",
      "offset": 1445.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "describing the spiking nature of the",
      "offset": 1448.159,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "neural networks uh it made me think of",
      "offset": 1451.44,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like Na the Jeff Hawkins stuff like do",
      "offset": 1453.679,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "you have you come across that like I've",
      "offset": 1456.88,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "met them at the conferences I like what",
      "offset": 1458.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "they're doing you know it's uh look",
      "offset": 1459.84,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "the idea of modeling spiking neurons was",
      "offset": 1464.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "developed by a guy named Jack there were",
      "offset": 1468,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "sort of two or three people at the time",
      "offset": 1469.6,
      "duration": 2.36
    },
    {
      "lang": "en",
      "text": "in the",
      "offset": 1470.88,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "1960s. Uh, and one of them is Jack",
      "offset": 1471.96,
      "duration": 3.959
    },
    {
      "lang": "en",
      "text": "Cowan. If you follow Schmid Huber on",
      "offset": 1474.08,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Twitter, you know, he was complaining",
      "offset": 1475.919,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "that Hotfield got the Nobel Prize and he",
      "offset": 1477.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "said, &quot;You deserve the Nobel Prize.&quot; You",
      "offset": 1479.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "know, these guys, you know, what are you",
      "offset": 1481.12,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "going on Twitter? He's also, you know,",
      "offset": 1482.72,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "what are you going on Twitter saying",
      "offset": 1483.84,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "stuff like this, you know? I mean, come",
      "offset": 1484.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "on, man. You know, I mean, that's",
      "offset": 1486.08,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "something you stay for a bar when you're",
      "offset": 1487.76,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "drunk, not when you're on Twitter. Um,",
      "offset": 1489.2,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "but during the the late60s, people",
      "offset": 1493.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "started modeling neurons. You'd go in",
      "offset": 1495.039,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the lab and you would model the spike",
      "offset": 1496.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "electrical activity and try to come up",
      "offset": 1498.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "with models for the electrical response",
      "offset": 1500.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "or neurochemical response of a neuron. I",
      "offset": 1502,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "was a theoretical chemist that's what we",
      "offset": 1504.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "do is what theoretical chemistry does.",
      "offset": 1505.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "So it turns out that um you know people",
      "offset": 1507.36,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "have like models of spiking neurons and",
      "offset": 1510.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "there are people I mean but I mean",
      "offset": 1514,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "actual neurons like like in a lab you",
      "offset": 1515.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "would take the neurons you grow them in",
      "offset": 1518.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "a lab and you put electrodes in you",
      "offset": 1519.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "watch how they spike right? you know,",
      "offset": 1522.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "some Elon Musk kind of thing. He's gonna",
      "offset": 1524.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "try to, you know, how they know",
      "offset": 1525.919,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Neurolink. Yeah. Yeah. How it works.",
      "offset": 1527.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "That started by growing neurons in a",
      "offset": 1529.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "lab, right? That's where it comes from.",
      "offset": 1531.36,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "That that was like the early work on",
      "offset": 1533.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "this. So, you know, there actually is a",
      "offset": 1534.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "deep connection between that stuff and",
      "offset": 1537.84,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "AI people. Yeah. And the models we have",
      "offset": 1539.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "today were developed to try to explain",
      "offset": 1542.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "these spiking neurons and they",
      "offset": 1544.88,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "eventually became sort of computer",
      "offset": 1546.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "sciency models and we run them on, you",
      "offset": 1547.919,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "know, GPUs. uh but there's a lot of",
      "offset": 1550.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "theory behind that and we and there's a",
      "offset": 1552.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "lot of experimental observations and you",
      "offset": 1554.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "can observe what I call signatures of",
      "offset": 1556.32,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "emergence these signatures there there's",
      "offset": 1559.12,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "a book by a guy named um a late",
      "offset": 1561.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "physicist perbach and he invented a",
      "offset": 1564.559,
      "duration": 5.281
    },
    {
      "lang": "en",
      "text": "theory called self-organized criticality",
      "offset": 1567.2,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and it's this idea that systems",
      "offset": 1569.84,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "self-organize to a critical point a",
      "offset": 1572.36,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "critical point between order and",
      "offset": 1575.039,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "chaos and you can see the signature of",
      "offset": 1577.559,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this critical point between order and",
      "offset": 1580.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "chaos inside many physical systems. You,",
      "offset": 1582.279,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "you know, in particular things like",
      "offset": 1586.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "avalanches is, you know, the snow is",
      "offset": 1587.96,
      "duration": 3.719
    },
    {
      "lang": "en",
      "text": "falling on the mountain. All of a",
      "offset": 1590.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "sudden, it collapses and there's this",
      "offset": 1591.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "point right before there's this this",
      "offset": 1593.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "point right before the critical point",
      "offset": 1595.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "when it collapses where it's in this",
      "offset": 1596.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "sort of semi-stable state of",
      "offset": 1598.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "self-organized criticality. That's",
      "offset": 1600.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "what's going on inside the brain. That's",
      "offset": 1602.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "a theory. It's it's called the um the",
      "offset": 1604.799,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "critical brain hypothesis. And so we can",
      "offset": 1607.6,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "see these signatures of criticality, I",
      "offset": 1610.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "call them signatures of emergence of of",
      "offset": 1612,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "AI inside actual neurons. And it turns",
      "offset": 1614.08,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "out our experimental work shows that the",
      "offset": 1617.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "closer the quality I have this layer",
      "offset": 1619.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "quality metric, there's a sweet spot at",
      "offset": 1622,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "two, there's a value of two. And when",
      "offset": 1624.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "all the layers reach two, we think that",
      "offset": 1627.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the model is perfectly optimized. And",
      "offset": 1630.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the goal of the theory is to try to get",
      "offset": 1632.72,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "your model so all the layers reach two.",
      "offset": 1634.48,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "And that's what we're trying to do. try",
      "offset": 1637.279,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "to develop technology to do that. But",
      "offset": 1638.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you know here you can and that's sort of",
      "offset": 1640.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "we and we see it in you know just really",
      "offset": 1642.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "basic experiments on things like",
      "offset": 1645.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "grocking and double descent where really",
      "offset": 1646.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you can really flush it all out. But you",
      "offset": 1648.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know you have to you got to you know we",
      "offset": 1650.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "try to study small models and then apply",
      "offset": 1652.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the results to larger models and then we",
      "offset": 1654.72,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "go back and try to figure out what's",
      "offset": 1656.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "going on and that's what we've been",
      "offset": 1658.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "doing and and it's been fairly",
      "offset": 1660.48,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "successful. There's a lot of open",
      "offset": 1661.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "questions, but you know the idea of is I",
      "offset": 1662.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "give people so that that's what the",
      "offset": 1665.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "spiking neurons are talking about is",
      "offset": 1667.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "just that you can measure these",
      "offset": 1668.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "signatures and you can see them in the",
      "offset": 1670.32,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "actual models you're training and you",
      "offset": 1672.32,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "can use them you can use them make",
      "offset": 1675.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "better models. So you've mentioned",
      "offset": 1676.399,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "groing a couple of times. Uh what's",
      "offset": 1678.799,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "grocking? Grocking is a",
      "offset": 1681.44,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "phenomenon where if you take a model and",
      "offset": 1683.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "you train it for you know some small",
      "offset": 1687.6,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "data set it it it tends to reproduce the",
      "offset": 1689.6,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "training it has perfect training",
      "offset": 1693.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "accuracy zero error and people think",
      "offset": 1695.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that it's somehow memorizing the",
      "offset": 1697.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "training set and then and it has almost",
      "offset": 1699.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "no test and the test error is like as",
      "offset": 1702.72,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "big as it could be. It's horrible,",
      "offset": 1704.96,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "right? And then all of a",
      "offset": 1706.24,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "sudden it it very quickly learns how to",
      "offset": 1708.36,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "generalize. And so the training accuracy",
      "offset": 1711.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "stays high but then the test accuracy",
      "offset": 1714.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "gets high. So my test accur might get",
      "offset": 1716,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like 85% you know 90%. Not super high",
      "offset": 1717.44,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "but it gets high very high for a small",
      "offset": 1720.399,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "data set.",
      "offset": 1722.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "That's something that people in the",
      "offset": 1725.24,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "theoretically is very odd because it's",
      "offset": 1728,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "odd that you would get a a model that",
      "offset": 1730.399,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "can describe the training data",
      "offset": 1732.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "perfectly. you think well it must be",
      "offset": 1734.48,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "over fit it must have memorized the data",
      "offset": 1736.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and then all of a sudden it's able to",
      "offset": 1738.799,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "generalize. So the kind of",
      "offset": 1740.159,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "nonlinearity in the learning is what's",
      "offset": 1743.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "interesting about yeah a phase",
      "offset": 1746.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "transition really it just goes from not",
      "offset": 1747.84,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "being able to generalize at all to being",
      "offset": 1749.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "able to generalize extremely well and",
      "offset": 1751.12,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "happens very and it happens just sort of",
      "offset": 1752.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "suddenly and that's called you the gro",
      "offset": 1754.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "is to understand something so they call",
      "offset": 1757.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it groing. Yeah. And then generalization",
      "offset": 1758.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "collapse. Generalization collapse",
      "offset": 1761.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "something we detected. What we define is",
      "offset": 1763.52,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "if you continue training all of a sudden",
      "offset": 1765.12,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "it stops generalizing. It starts going",
      "offset": 1767.039,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "down again. So it still has memorize the",
      "offset": 1768.559,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "training data in some sense that the",
      "offset": 1771.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "training accuracy is perfect. But it it",
      "offset": 1773.12,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "it it starts to learn it generalizes and",
      "offset": 1776,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "all of a sudden and it stops",
      "offset": 1779.039,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "generalizing and it starts going down",
      "offset": 1780.279,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "and it might go down to like you know",
      "offset": 1782.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "instead of like you know an 10% accuracy",
      "offset": 1783.76,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "would be random it might go down to like",
      "offset": 1786.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "50% accuracy. So it doesn't get ter it",
      "offset": 1788.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "doesn't get it it does a little bit of",
      "offset": 1790.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "learning but it's confused. It's like in",
      "offset": 1792.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "this state of confusion deep confusion.",
      "offset": 1795.12,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "Got it. And these terms come from the",
      "offset": 1798.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "title of the paper that we've referred",
      "offset": 1800.399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "to here. Grocking and generalization",
      "offset": 1802.48,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "collapse insights from HTSR theory",
      "offset": 1804.559,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "and HTSR theory. Heavy tailed",
      "offset": 1809,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "self-regularization. You got to have an",
      "offset": 1812.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "acronym in in science. Every theory",
      "offset": 1813.6,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "needs an acronym, right? So ACSR that's",
      "offset": 1815.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the theory. Uh heavy tailed self work I",
      "offset": 1818.159,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "published with Michael Mahoney back in",
      "offset": 1820.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "2021. Uh so wow it's been 5 years. Wow",
      "offset": 1822.76,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "it's hard to believe it's that long. Uh",
      "offset": 1825.679,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "so this is the theory behind Weight",
      "offset": 1827.2,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "Watcher. This is why Weight Watcher",
      "offset": 1828.559,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "works. Part part of the theory. The um",
      "offset": 1829.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "so what we discovered is that when",
      "offset": 1832.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "people think about a model memorizing",
      "offset": 1834.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "they think oh the training accuracy is",
      "offset": 1837.2,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "perfect it must be memorizing. No, no,",
      "offset": 1838.799,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "we actually and it turns out that you",
      "offset": 1841.919,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "know we discovered this other phase of",
      "offset": 1844,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "memorizing which is more like",
      "offset": 1846.52,
      "duration": 4.759
    },
    {
      "lang": "en",
      "text": "confusion and so there's a state of",
      "offset": 1848.919,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "memorization and there's a state of",
      "offset": 1851.279,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "confusion and they're very different and",
      "offset": 1852.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you can detect them using Weight",
      "offset": 1855.679,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Watcher. Both states appear to reproduce",
      "offset": 1856.88,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the training accuracy perfectly. Why",
      "offset": 1859.279,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "would confusion reproduce the training",
      "offset": 1861.279,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "uh data perfectly? Okay. So what's",
      "offset": 1864.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "happening is what's what's happening in",
      "offset": 1867.12,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "these",
      "offset": 1869.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "models. It it turns out that in the",
      "offset": 1870.12,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "state before grocking we call it the",
      "offset": 1873.84,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "preg groing",
      "offset": 1875.6,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "phase training accuracy is perfect. Why",
      "offset": 1877.08,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "can't it generalize? It turns out some",
      "offset": 1880.799,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "of the layers have good",
      "offset": 1883.36,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "quality but some of the layers have very",
      "offset": 1885.32,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "bad quality. So what's happened is",
      "offset": 1887.919,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "you've learned the training data but the",
      "offset": 1890.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "layer that learned the training data the",
      "offset": 1892.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "layer that that needs to learn how to",
      "offset": 1895.44,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the layer that's most important for",
      "offset": 1897.679,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "generalizing hasn't converted. So the",
      "offset": 1899.279,
      "duration": 6.601
    },
    {
      "lang": "en",
      "text": "idea is that it's not that you are",
      "offset": 1901.519,
      "duration": 8.241
    },
    {
      "lang": "en",
      "text": "memorizing before and then you're you've",
      "offset": 1905.88,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "switched from like this thing called",
      "offset": 1909.76,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "memorizing to this thing called uh",
      "offset": 1911.36,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "generalization. uh it's more that part",
      "offset": 1915.24,
      "duration": 7.559
    },
    {
      "lang": "en",
      "text": "of your network is sufficiently",
      "offset": 1919.36,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "memorized or not sufficiently memorized",
      "offset": 1922.799,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "but sufficiently learned right it's",
      "offset": 1925.44,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "converged uh and other critical parts",
      "offset": 1927.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "have not yet and",
      "offset": 1930.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "uh that's why you don't and that those",
      "offset": 1933.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's required for the generalization",
      "offset": 1935.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "yes now there's sort of a somebody has",
      "offset": 1937.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "suggested I saw a paper recently that",
      "offset": 1940.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "said the reason it it it doesn't learn",
      "offset": 1941.76,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "there's like a numerical instability",
      "offset": 1944.24,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "in the",
      "offset": 1946.399,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "softmax. And so you have and so it just",
      "offset": 1947.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "it just gets numerically unstable and it",
      "offset": 1949.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "just kind of it's trying to learn but it",
      "offset": 1952.24,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "just can't find its way and you have to",
      "offset": 1954,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "train it for a long period of time until",
      "offset": 1955.919,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "it eventually jumps out of this jumps",
      "offset": 1957.279,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "over the hill and",
      "offset": 1959.039,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "learns. Confusion or what we call you",
      "offset": 1961.159,
      "duration": 3.721
    },
    {
      "lang": "en",
      "text": "know we call overfitting and weight",
      "offset": 1963.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "washer like a confusion is that now the",
      "offset": 1964.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "layers learn too much information. So",
      "offset": 1967.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "they're overcon converged, they're over",
      "offset": 1970,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "they're they're they're they're",
      "offset": 1971.519,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "overbaked, right? They're burned, you",
      "offset": 1972.559,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "know, they're cooked, they're",
      "offset": 1974.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "overcooked. So they've learned too much",
      "offset": 1975.6,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "information and now they're they're",
      "offset": 1977.76,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "stuck and all they know is what and and",
      "offset": 1979.919,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "they can't they get confused as to",
      "offset": 1982.24,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "what's going on because they they",
      "offset": 1983.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they've learned so much that now they",
      "offset": 1985.679,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "can't generalize. So it's a different",
      "offset": 1988.159,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "thing. So in one case we see the layers",
      "offset": 1990.64,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "some of the layers converge and going",
      "offset": 1993.039,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "down and some of them going to stay up",
      "offset": 1995.679,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "here and in the other case they're all",
      "offset": 1997.159,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "way down here like they're all the way",
      "offset": 1999.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "down they're all way too low and what",
      "offset": 2000.96,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "you want is you want them out here in",
      "offset": 2002.48,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "the safe range and and that's what we're",
      "offset": 2003.679,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "seeing and that's and the thing that",
      "offset": 2005.919,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "that could happen in a real model in the",
      "offset": 2007.919,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "real world if you're fine-tuning you",
      "offset": 2010.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "know you're training that you'll see",
      "offset": 2011.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "some of the layers are say they're down",
      "offset": 2013.76,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "here they're good but some of the layers",
      "offset": 2015.6,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "are up here they haven't learned",
      "offset": 2017.6,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "anything yet they're stuck and if If you",
      "offset": 2018.64,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "go too long, they all fall down. You you",
      "offset": 2020.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "burn the system. You burn it. So I to",
      "offset": 2023.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "me, I like the analogy of baking a cake",
      "offset": 2026.24,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because it's like if you turn the oven",
      "offset": 2028,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "up or you leave it in the oven too long,",
      "offset": 2029.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it the whole thing will burn, right? It",
      "offset": 2031.679,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "just it'll cook. It'll overcake. And so",
      "offset": 2034.08,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "something has gone wrong. There's some",
      "offset": 2036.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "numerical instability in the solver.",
      "offset": 2038.32,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "Maybe the softmax is off. Maybe there's",
      "offset": 2040.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "some goofiness in the training data.",
      "offset": 2042.559,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Something went wrong that's preventing",
      "offset": 2044.48,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "the model from converging. And we can",
      "offset": 2046.399,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "see that. And that's a different",
      "offset": 2048.879,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "phenomenon than",
      "offset": 2050.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "overfitting. And the goal is to try how",
      "offset": 2052.919,
      "duration": 4.361
    },
    {
      "lang": "en",
      "text": "how do you fix it? Yeah. Yeah. So, a",
      "offset": 2054.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "couple of things jump out at me as",
      "offset": 2057.28,
      "duration": 5.639
    },
    {
      "lang": "en",
      "text": "interesting here. One is this idea of",
      "offset": 2058.96,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "like I don't know, really just kind",
      "offset": 2062.919,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "of thinking about this as layers, I",
      "offset": 2065.72,
      "duration": 4.919
    },
    {
      "lang": "en",
      "text": "think, and really kind of locking in on",
      "offset": 2069.119,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "that, I think, is interesting. and then",
      "offset": 2070.639,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "that each of these layers can be",
      "offset": 2072.48,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "independently",
      "offset": 2074.24,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "uh underfit, overfit or you know in the",
      "offset": 2076.639,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "the target zone. Um that's interesting.",
      "offset": 2079.919,
      "duration": 8.521
    },
    {
      "lang": "en",
      "text": "Uh but then that kind of leads me to",
      "offset": 2084,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "um and I'm this is kind of a lead into",
      "offset": 2088.44,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "like to what degree have you looked at",
      "offset": 2091.679,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "these things or is this like you know",
      "offset": 2093.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "next uh steps here? But",
      "offset": 2095.359,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "like you know when I think about what um",
      "offset": 2097.8,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "I think about like datacentric AI or",
      "offset": 2103.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "like the focus on data curation as a way",
      "offset": 2105.28,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "to get models to perform better more",
      "offset": 2109.119,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "efficiently. You can also think of it in",
      "offset": 2112.28,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "this context as like can I construct a",
      "offset": 2114.8,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "incremental training data set that",
      "offset": 2118.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "targets the deficiencies of a particular",
      "offset": 2120.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "layer or set of layers. Yeah. Yeah. I I",
      "offset": 2123.04,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "think so. I mean that that's the kind we",
      "offset": 2126.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "haven't looked deeply at changing the",
      "offset": 2128,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "data set as much as changing the",
      "offset": 2129.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "learning rates on the layers or making a",
      "offset": 2131.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "new kind of regularizer. But I think",
      "offset": 2133.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "absolutely that there's something and",
      "offset": 2135.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "you because there's numerical",
      "offset": 2137.52,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "instabilities data right my my take on",
      "offset": 2138.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "dataentric AI is that these guys went",
      "offset": 2141.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "into industry and tried to apply their",
      "offset": 2144.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "their stuff and then they realize that",
      "offset": 2146.24,
      "duration": 3.879
    },
    {
      "lang": "en",
      "text": "it's just a",
      "offset": 2148.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "mess, right? Like I've been doing this",
      "offset": 2150.119,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "for 25 years. I tell you right now, you",
      "offset": 2152.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "know, you're I can't get an SVM working",
      "offset": 2154.56,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "in production. You think you're going to",
      "offset": 2156.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "get an AI model working? You have no",
      "offset": 2157.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "idea what's going on in these companies,",
      "offset": 2159.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "right? So, that to me is like, you know,",
      "offset": 2161.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "that's just they didn't understand like",
      "offset": 2163.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "their business model, what they were",
      "offset": 2165.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "selling. Um, and you know, it's it's",
      "offset": 2167.2,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "that kind of thing where",
      "offset": 2169.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um trying to figure out, you know, how",
      "offset": 2171.8,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "to get, you know, how to target the data",
      "offset": 2174,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "correctly. That that's exactly one of",
      "offset": 2175.599,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "the things we'd like to know. for",
      "offset": 2177.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "example, is, you know, you know, this is",
      "offset": 2178.48,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "stuff we'd like to get into more if we",
      "offset": 2181.2,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "could, you know, is",
      "offset": 2182.64,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "um can we figure out which parts of the",
      "offset": 2184.839,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "data are being learned and which parts",
      "offset": 2187.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "are not, right? And now you should be",
      "offset": 2188.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "able to see you pass the data through",
      "offset": 2191.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "and you see which neurons light up and",
      "offset": 2192.8,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "which ones don't. Those are the kind of",
      "offset": 2194.32,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "things we'd like to do. You know, doing",
      "offset": 2195.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "an ablation study on a model to figure",
      "offset": 2197.68,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "out where in your data set is it",
      "offset": 2199.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "insufficient, where what part of the",
      "offset": 2201.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "data do you need to fill in? maybe like",
      "offset": 2203.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "part of your data set is not fully",
      "offset": 2205.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "filled in and if you added more data",
      "offset": 2207.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there maybe even fake data you know",
      "offset": 2209.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "autogenerate data you might be able to",
      "offset": 2210.64,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "improve training stuff like that those",
      "offset": 2212.079,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "are things we'd like to understand",
      "offset": 2214,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "better um and you know we're trying to",
      "offset": 2215.2,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "you know and of course I'm trying to",
      "offset": 2217.52,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "raise funding actually right now to do",
      "offset": 2218.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "that to develop some of this technology",
      "offset": 2220,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "um and and I think it's just you know",
      "offset": 2222.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "these are things that require um just a",
      "offset": 2224.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "lot of experimentation to figure out but",
      "offset": 2228.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "we we definitely see it now like there's",
      "offset": 2230.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "a paper that came out just just this",
      "offset": 2232.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "week from Stanford by Chris Manning",
      "offset": 2234.56,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "about how if you look at some of these",
      "offset": 2236.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "large models like llama or Quinn that a",
      "offset": 2238.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "bunch of the layers are not learn",
      "offset": 2240.56,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "they're looking at how the residuals",
      "offset": 2242.079,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "flow through the layers and they can see",
      "offset": 2243.359,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that some layers are not really",
      "offset": 2245.359,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "converging some layers are not learning",
      "offset": 2247.599,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "and like yeah we told you that like",
      "offset": 2249.04,
      "duration": 3.799
    },
    {
      "lang": "en",
      "text": "three four years ago like it's on the",
      "offset": 2250.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "website yeah I wish they' used the tool",
      "offset": 2252.839,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "and stuff but you know it's a verifi",
      "offset": 2255.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "it's a validation of what we've been",
      "offset": 2256.8,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "saying for some time is that our",
      "offset": 2258.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "technology can detect this without",
      "offset": 2260.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "seeing the data the the other and was",
      "offset": 2262.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "there is there a onetoone relationship",
      "offset": 2265.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "between the characteristic that they",
      "offset": 2268,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "were talking about and I'm not sure your",
      "offset": 2270.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "characteristic we're starting to just",
      "offset": 2273.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "dig into it now it it is we we've seen",
      "offset": 2274.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "cases like we have a paper that came out",
      "offset": 2277.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "where I guess the broader point is like",
      "offset": 2279.599,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "this sounds like a really interesting",
      "offset": 2282.8,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "way to to characterize individual layers",
      "offset": 2284.24,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "of models but there probably a ton of",
      "offset": 2286.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "different ways to well yeah there lot",
      "offset": 2288.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the the main thing that we can do that",
      "offset": 2290.88,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "no one else can do is We we know the",
      "offset": 2292.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "cutoffs. We know the bottom we know the",
      "offset": 2294.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "lower bound is two and the upper bound",
      "offset": 2296.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "is like six. So there any you can use",
      "offset": 2298.48,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "any model. You can measure the entropy",
      "offset": 2301.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "the distance. You do whatever you want",
      "offset": 2303.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "that all you can see you know if you",
      "offset": 2305.119,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "take two models are those practical",
      "offset": 2307.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "bounds or theory. Yeah those are",
      "offset": 2309.359,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "practical bounds. Yeah they they work in",
      "offset": 2310.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "production. Yeah it really does work.",
      "offset": 2312.24,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Were are they empirical or are they",
      "offset": 2315.68,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "based on some theoretical analysis like",
      "offset": 2317.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "mathematical analysis? Combination of",
      "offset": 2320.24,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "both. They're based on a combination.",
      "offset": 2322.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "There's some new theory coming out. Uh",
      "offset": 2323.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "it uses techniques from theoretical",
      "offset": 2326.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "physics called reormalization group. And",
      "offset": 2327.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "so it's a combination of theory and",
      "offset": 2329.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "experiment going together. You know, we",
      "offset": 2331.04,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "have theories that show there's a",
      "offset": 2332.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "boundary at two. We actually have two",
      "offset": 2334.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "different metrics for the lower bound.",
      "offset": 2336.16,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "You can and they have to line up. And",
      "offset": 2337.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "when they line up, then we know. So",
      "offset": 2338.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's there's some deep theory like I",
      "offset": 2340.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "got a 100page theoretical physics paper",
      "offset": 2342.88,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "on this thing to really justify it. But",
      "offset": 2344.88,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you see it empirically as well. And this",
      "offset": 2347.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "Grocking paper in particular is an",
      "offset": 2348.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "important paper for us. You know, we",
      "offset": 2351.359,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "wrote this nice 10-page paper to really",
      "offset": 2352.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "show, look, it works perfectly. And and",
      "offset": 2354.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "for me, the other thing is that I had",
      "offset": 2356.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "somebody else do it with the tool. So,",
      "offset": 2358.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's independently verified, right? You",
      "offset": 2360.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "know, I you know, so these are real",
      "offset": 2363.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things you can use in production. I",
      "offset": 2365.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "mean, they're they're they're real. Um",
      "offset": 2366.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "the the challenge, as you say, is you",
      "offset": 2369.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "have to analyze the data, you know, and",
      "offset": 2370.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's it's always tough. I mean, from a",
      "offset": 2372.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "consulting perspective, look, I I did a",
      "offset": 2374.72,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "large project with Walmart. I tried to",
      "offset": 2376.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "help them, you know, fix their search",
      "offset": 2377.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "engine, you know, and you find out, oh,",
      "offset": 2379.76,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "you know, you need to deploy your",
      "offset": 2381.04,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "product on our Jupyter notebook serving",
      "offset": 2382.44,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "system. Okay. But you're a consultant.",
      "offset": 2386.079,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "You're not allowed to have access to our",
      "offset": 2389.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Jupyter Nervix servings because we put",
      "offset": 2390.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "our all our customer data there,",
      "offset": 2392.96,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "including all their credit card numbers,",
      "offset": 2394.8,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and you can't have access to that. Okay?",
      "offset": 2396.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "You know, what do you want me to do",
      "offset": 2399.76,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "here? You know, I mean, that's very",
      "offset": 2400.8,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "common. the same thing. I did a project",
      "offset": 2402.72,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "years ago, almost 20 years ago for",
      "offset": 2404.88,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "France Telecom and at the end of the and",
      "offset": 2407.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "we had to only use fake data because I'm",
      "offset": 2409.599,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "not an EU citizen. Since I'm not an EU",
      "offset": 2411.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "citizen, I can't access personal data.",
      "offset": 2413.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "And so what happens in these big",
      "offset": 2416.64,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "companies is that, you know, they'll",
      "offset": 2418,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "stuff all the data on Hadoop into one",
      "offset": 2419.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "system. And then it turns out they have",
      "offset": 2421.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "compliance issues, GDPR, CCPA, you know,",
      "offset": 2422.88,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "you you you can't access things. So it's",
      "offset": 2426.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "a huge problem. And so part of Weight",
      "offset": 2429.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Watcher is that I wanted to build a tool",
      "offset": 2431.28,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "that I didn't need access to any data",
      "offset": 2433.119,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "because nobody ever lets me look at it",
      "offset": 2435.119,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "anyway.",
      "offset": 2436.839,
      "duration": 5.481
    },
    {
      "lang": "en",
      "text": "So, so it's a problem and these are real",
      "offset": 2439.64,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "production problems, you know, uh, and",
      "offset": 2442.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so part of, you know, trying now to",
      "offset": 2444.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "convince people, you know, it's it's an",
      "offset": 2446.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "easier cell to give someone a tool and",
      "offset": 2448.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "analyze the model because, you know, I",
      "offset": 2450.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "mean, in a sense, I don't have the",
      "offset": 2452,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "tokenizer, so I don't really I mean, I",
      "offset": 2453.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "could kind of reverse engineer the",
      "offset": 2455.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "model. It's kind of hard, but getting",
      "offset": 2456.72,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "customer data and trying to look at",
      "offset": 2458.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "customer data is much harder. You know,",
      "offset": 2460.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it's a different ask. Um, it's a",
      "offset": 2462.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "different ask in the org. Um, there are",
      "offset": 2464.8,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "compliance issues around it. So, these",
      "offset": 2467.52,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "are things that, you know, we'd like to",
      "offset": 2469.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "try to do as a product level, but that",
      "offset": 2470.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "takes some time to do. Yeah, this is",
      "offset": 2472.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this is a herculan effort to try to get",
      "offset": 2474.4,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "around a very pragmatic organizational",
      "offset": 2476.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "problem. It's very hard. You know, I I I",
      "offset": 2479.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "always tell people, you know, if you",
      "offset": 2482.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to do something, you've got to I",
      "offset": 2484.72,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "have a friend who's who's figured out a",
      "offset": 2486.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "way to like automate all of the",
      "offset": 2488,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "marketing in his company. And he's just",
      "offset": 2489.68,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "he's totally obsessed with prompt",
      "offset": 2491.44,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "engineering. He's got this and I and he",
      "offset": 2492.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "says because the last and they won't let",
      "offset": 2494.24,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "him do a poll he can't do a poll request",
      "offset": 2495.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because he you have to get you have to",
      "offset": 2498.079,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "get someone to sign off on the poll",
      "offset": 2499.2,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "request. And so we built the whole thing",
      "offset": 2500.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "in Google Sheets, you know, and Google",
      "offset": 2501.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "Drive. And it's just this, you know,",
      "offset": 2504.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "something an accountant would build, you",
      "offset": 2506.96,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "know, and um but I said, &quot;The last thing",
      "offset": 2508.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you want to do is be able to do a poll",
      "offset": 2510.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "request because you'll get into the",
      "offset": 2512.4,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "engineering or once you're in the",
      "offset": 2513.839,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "engineering or you have to follow their",
      "offset": 2515.359,
      "duration": 1.96
    },
    {
      "lang": "en",
      "text": "best",
      "offset": 2516.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "practices and and those best practices",
      "offset": 2517.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "may not work for what you're trying to",
      "offset": 2519.599,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "do. They're they're best practices.",
      "offset": 2520.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "They're just not best for you.&quot; Um and",
      "offset": 2522.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "then a lot of that we see a lot of this",
      "offset": 2524.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "here is as these kinds of tools, you",
      "offset": 2526.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know, to run in in an org. Um, part of",
      "offset": 2527.599,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "like I say like why fine-tuning is hard.",
      "offset": 2530.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Well, I I've been I've had people come,",
      "offset": 2531.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "we want you to build a model for us, but",
      "offset": 2534.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "and we want the model and we want you to",
      "offset": 2536.319,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "design this model for us, but you're not",
      "offset": 2538.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "allowed to ever look at the data for any",
      "offset": 2540.4,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reason because it's a compliant. I'm not",
      "offset": 2543.04,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "going to do how can I possibly get it to",
      "offset": 2545.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "work? I don't know how it would work,",
      "offset": 2547.76,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "but you know, so there's those kinds of",
      "offset": 2549.28,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "things are real and they exist in big",
      "offset": 2551.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "orgs. And so, um, you know, looking at",
      "offset": 2552.64,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "data and peeking, we certainly would",
      "offset": 2555.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "like to look at and peek at what's going",
      "offset": 2557.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "on, uh, with the tool. And this one of",
      "offset": 2558.56,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "the things we're trying to to figure out",
      "offset": 2560.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "how to how to do that in a compliant way",
      "offset": 2561.839,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "with the in the paper you benchmark the",
      "offset": 2564.72,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "results against a few different",
      "offset": 2569.119,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "approaches. Activation sparity, absolute",
      "offset": 2571.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "weight entropy, absolute local local",
      "offset": 2574.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "circuit complexity like talk a little",
      "offset": 2577.68,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "bit about the prior work. So in this",
      "offset": 2579.28,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "paper um you know there are people at",
      "offset": 2582.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Google deep mind and people at anthropic",
      "offset": 2584.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "which are trying to find out metrics to",
      "offset": 2587.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "figure out what's going on in groin and",
      "offset": 2589.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and you know we've picked sort of the",
      "offset": 2591.92,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "four top ones that people have in this",
      "offset": 2593.2,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in the theory and what we find is that",
      "offset": 2594.96,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "none of them can detect this third phase",
      "offset": 2598.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "of anti-rocket none of these metrics",
      "offset": 2599.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "they have I mean they can kind of detect",
      "offset": 2602.16,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "groing like they can see the phase",
      "offset": 2604,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "transition but",
      "offset": 2605.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "because there's a threshold they don't",
      "offset": 2607.319,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "know what the thresholds are they don't",
      "offset": 2609.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "know that anti-groing or this sort of",
      "offset": 2611.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "confusion phase exists the",
      "offset": 2613.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "generalization collapse and their",
      "offset": 2614.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "technique even if they could detect it",
      "offset": 2616.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "it's not clear what you know when it's",
      "offset": 2618.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "happening and when it isn't. So that's",
      "offset": 2621.359,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "sort of the part of the paper is not",
      "offset": 2622.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "only can we detect this new phase but",
      "offset": 2624,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "none of the existing proposed metrics",
      "offset": 2626,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "can detect it only our tool can detect",
      "offset": 2628.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "it. So, so we can just you know we pick",
      "offset": 2631.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "you know the top ones deep mind and",
      "offset": 2633.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "anthropic right those are the big ones",
      "offset": 2635.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "right and so that and that's sort of the",
      "offset": 2636.56,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "point is that whatever you know even",
      "offset": 2638.48,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "like they have this thing called circuit",
      "offset": 2639.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "complexity where you're trying you know",
      "offset": 2640.64,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "this is a big thing that's come out of",
      "offset": 2642.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "anthropic recently the circuits how do",
      "offset": 2643.359,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you know the circuit is overfit can't",
      "offset": 2645.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "tell can't tell does you know and and it",
      "offset": 2647.359,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "may see that it has higher complexity or",
      "offset": 2650.4,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "like people do compression another say",
      "offset": 2653.119,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "oh if you compress the quality of a",
      "offset": 2655.599,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "model is correlated to how much each",
      "offset": 2657.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "layer can be compressed",
      "offset": 2660.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Okay, that's true. No question about it.",
      "offset": 2662.56,
      "duration": 5.559
    },
    {
      "lang": "en",
      "text": "Question is, what about if you",
      "offset": 2665.44,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "overcompress? If you overcompress, you",
      "offset": 2668.119,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "over fit. That's the third phase. They",
      "offset": 2670.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can't detect that. So, that's what we're",
      "offset": 2671.76,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "able to do with the theory is that we",
      "offset": 2674.079,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "can detect this overfitting phase. And",
      "offset": 2676,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "you know, this is something that",
      "offset": 2678.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "um I'll give you an example of where it",
      "offset": 2681.16,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "might and it's not always bad, but let",
      "offset": 2683.44,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "me give you an example where you might",
      "offset": 2684.88,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "actually want to do this. We've looked",
      "offset": 2685.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "at models that are like segment segment",
      "offset": 2687.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "anything models the SAM models from",
      "offset": 2689.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Facebook that allow you to do zero shot",
      "offset": 2691.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "vision learning right in other words it",
      "offset": 2693.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "can it can it turns out if you look at",
      "offset": 2695.359,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "those a lot a lot of the early layers in",
      "offset": 2697.76,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the SAM models are overfitit according",
      "offset": 2701.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "to our theory so here's my early as in",
      "offset": 2703.52,
      "duration": 6.079
    },
    {
      "lang": "en",
      "text": "base yes closer to well the they're",
      "offset": 2706.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "closer to the data so you think of that",
      "offset": 2709.599,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "early mean that you know the closer the",
      "offset": 2711.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "data the earlier the model is closer to",
      "offset": 2712.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "the label the later the layer is so the",
      "offset": 2714.079,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "what What I think is happening is that",
      "offset": 2716.72,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "there are these primitive features in",
      "offset": 2718.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "all of vision, you know, lines, line",
      "offset": 2719.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "segments and little circles and things",
      "offset": 2722.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like that that our visual system picks",
      "offset": 2724.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "up. I think what's happening in the",
      "offset": 2726.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "segment anything models is that they",
      "offset": 2728.8,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "look at a large number of natural images",
      "offset": 2730.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "and they memorize these",
      "offset": 2732.64,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "abstract features, primitive features in",
      "offset": 2735.4,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "the data and that's why they're able to",
      "offset": 2738.079,
      "duration": 3.961
    },
    {
      "lang": "en",
      "text": "perform good",
      "offset": 2740.4,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "zeroot zero shot learning because most",
      "offset": 2742.04,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "natural images, you know, in the natural",
      "offset": 2745.04,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "environment things are pretty much all",
      "offset": 2746.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "the same. You know, tree is a tree is a",
      "offset": 2748.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "tree. Um, and you know, it could detect",
      "offset": 2749.52,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "what a tree is because it can see, oh,",
      "offset": 2751.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "that looks like a tree because it has",
      "offset": 2753.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "the same primitive features. You know, a",
      "offset": 2754.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "tree in China looks like a tree in the",
      "offset": 2756.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "US. Although a pine tree is not the same",
      "offset": 2758.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "as, you know, whatever. Maybe a cherry",
      "offset": 2760.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "blossom in Japan is not the same as a",
      "offset": 2762.4,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "pine tree, but it's primitive features",
      "offset": 2764.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "are close enough that it can segment",
      "offset": 2766.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "them and detect it as a tree. And that's",
      "offset": 2768.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what I think is going on. So, it's not",
      "offset": 2770.48,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "necessarily that overfitting is always",
      "offset": 2771.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "bad. It might be something good. And so,",
      "offset": 2773.2,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "if you're trying to build a zero shot",
      "offset": 2775.599,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "learning model, you might want to",
      "offset": 2777.359,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "optimize for this kind of overfitting in",
      "offset": 2778.88,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "the early",
      "offset": 2780.8,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "layers. Now, I give you an example where",
      "offset": 2781.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "it might be bad. We've looked at models",
      "offset": 2783.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "like llama guard, you know, these guard",
      "offset": 2785.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "model guardrail models. Guard rails show",
      "offset": 2788.48,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the opposite behavior. The the layers",
      "offset": 2790.72,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "near the labels is seem seem to be",
      "offset": 2793.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "overfit. And so what I think is",
      "offset": 2796.28,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "happening and again these are this is",
      "offset": 2798.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "all conjectural. I haven't gone and done",
      "offset": 2799.52,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "ablation studies. This is all",
      "offset": 2800.96,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "conjectural. But I think what's",
      "offset": 2802.079,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "happening in in the guard models is that",
      "offset": 2803.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "it's overfitting to you know whatever",
      "offset": 2805.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the examples you're giving to try to",
      "offset": 2808.56,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "build the guard rail. And which is why",
      "offset": 2810.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "they can be you know you can get around",
      "offset": 2812.16,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "them. you can get deeper into the model",
      "offset": 2813.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and find the more abstract thing to tell",
      "offset": 2815.599,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the model to think about. It's able to",
      "offset": 2817.599,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "go around the",
      "offset": 2819.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "guardrail. So you may be that if you're",
      "offset": 2820.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "trying to build guardrails for models,",
      "offset": 2823.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "you may need to have and these are these",
      "offset": 2825.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "are the instruction fine-tuned",
      "offset": 2827.04,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "components on top of llama. So if you're",
      "offset": 2828.319,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "trying to instruction fine-tune a model",
      "offset": 2830.319,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "to give it a guardrail, you may want to",
      "offset": 2831.839,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have the overfitting go very very deep",
      "offset": 2833.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "into the back layers near the data to",
      "offset": 2836.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "prevent that kind of backdooring.",
      "offset": 2838.48,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "So, so we have cases where overfitting",
      "offset": 2842.16,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "is not necessarily bad, but you but",
      "offset": 2843.839,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "knowing what it is and how to detect it",
      "offset": 2846.4,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "um is what we're trying to do with with",
      "offset": 2848.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the tool. And so the idea is you we so",
      "offset": 2850.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "these are examples we've worked out and",
      "offset": 2852.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "sort of the goal is look, we have an",
      "offset": 2854.319,
      "duration": 3.481
    },
    {
      "lang": "en",
      "text": "open source tool that's 200,000",
      "offset": 2855.76,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "downloads. I give it to you, try it out,",
      "offset": 2857.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "then talk to me about what you're doing.",
      "offset": 2861.2,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "We'll see if we can figure out, you",
      "offset": 2862.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "know, what's going on, how to make it",
      "offset": 2863.68,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "work work for you. That that's",
      "offset": 2864.8,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "essentially the Weight Watcher",
      "offset": 2866.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "project. And when you compare against",
      "offset": 2868.68,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "these other methods, are you strictly",
      "offset": 2871.359,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "comparing against the ability to predict",
      "offset": 2875.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "this thing that you made up or are there",
      "offset": 2877.839,
      "duration": 6.441
    },
    {
      "lang": "en",
      "text": "more like objective metrics that uh that",
      "offset": 2880,
      "duration": 8.079
    },
    {
      "lang": "en",
      "text": "the other uh labs have published? I'll",
      "offset": 2884.28,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "give you one that was very surprising.",
      "offset": 2888.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Okay. A couple years ago, I took a look",
      "offset": 2890.079,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "at all the existing base models, you",
      "offset": 2892.319,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "know, llama and and you know, um, you",
      "offset": 2894.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "know, whatever was back the was hot back",
      "offset": 2896.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "then, you know, this stuff. And I",
      "offset": 2899.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "compared the average alpha quality",
      "offset": 2901.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "metric to the hallucination metric.",
      "offset": 2903.28,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Okay. And it turns out according to our",
      "offset": 2906.8,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "theory, the closer you are to",
      "offset": 2909.2,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "optimality, the more the model",
      "offset": 2910.68,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "hallucinates.",
      "offset": 2912.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Interesting. Yeah, that's kind of like",
      "offset": 2914.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what you would think that a thing that",
      "offset": 2917.72,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "hallucinates more is not optimal in some",
      "offset": 2919.92,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "way. No, we're creative. Right. Right.",
      "offset": 2921.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Right. People are like, &quot;Oh, we don't",
      "offset": 2924.4,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "want them on, but other people No,",
      "offset": 2925.599,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "hallucination is a feature. It's a In",
      "offset": 2926.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "fact, it was so amazing. I gave a TED",
      "offset": 2929.52,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "talk on it. I mean, it was incredible",
      "offset": 2930.96,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "that like like you think about talking",
      "offset": 2932.16,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "to a child, like my um you know, my",
      "offset": 2934.24,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "three-year-old niece, right? She'll just",
      "offset": 2936.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "make stuff up. It sounds good. I'll make",
      "offset": 2938.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "up a little story. It sounds good. I'll",
      "offset": 2940.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "tell you, right? That's what they do.",
      "offset": 2942,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "That's what children do. They're",
      "offset": 2943.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "creative, right? they're exploring",
      "offset": 2944.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "still. So these models, they seem to",
      "offset": 2945.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "have this this hallucination ability",
      "offset": 2947.839,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "seems to be related to the optimal",
      "offset": 2950.4,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "convergence",
      "offset": 2953.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "properties. And so that's an example of",
      "offset": 2954.68,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "where we looked at that was like, wow,",
      "offset": 2956.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "that's really cool. Um, and you know,",
      "offset": 2958.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "maybe it means that certain layers are",
      "offset": 2960.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "contributing more to the hallucinations",
      "offset": 2962.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "than others, right? And and there's a",
      "offset": 2963.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and there's a trade-off between, you",
      "offset": 2966.24,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "know, thinking inside the box, thinking",
      "offset": 2967.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "outside the box, right? And I always",
      "offset": 2969.599,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "say, you know, people want models and",
      "offset": 2971.44,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "not hallucinate. In other words, you",
      "offset": 2972.64,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "don't want them to be so creative,",
      "offset": 2973.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "right? You don't want them going off",
      "offset": 2975.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "into the tangent. Maybe you want them",
      "offset": 2977.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "really to memorize the data and not be",
      "offset": 2979.359,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "so good at generalizing. And that tells",
      "offset": 2981.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "you something about what the layer",
      "offset": 2984.48,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "should look like. And that's the kind of",
      "offset": 2985.92,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "thing we've been able to figure out um",
      "offset": 2987.76,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "just using the theory and and and",
      "offset": 2990.559,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "comparing to, you know, some things",
      "offset": 2992.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "other people are doing that hopefully I",
      "offset": 2993.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I don't think people are um I don't",
      "offset": 2995.68,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "think they're gaming the maybe they're",
      "offset": 2997.76,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "gaming the hallucination metric, but it",
      "offset": 2999.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "would be the other way. trying to",
      "offset": 3000.559,
      "duration": 2.921
    },
    {
      "lang": "en",
      "text": "convince people that it's not",
      "offset": 3002.16,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "hallucinating. But we we definitely",
      "offset": 3003.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "that's the kind of stuff we're seeing.",
      "offset": 3005.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And by the way, I think ours is the that",
      "offset": 3006.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "was the only metric of all the metrics",
      "offset": 3008.559,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "that was the only one that actually",
      "offset": 3010.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "correlated. Like the other stuff didn't",
      "offset": 3011.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "really it was just sort of random. Like",
      "offset": 3013.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "just random stuff. All these other Yeah.",
      "offset": 3015.2,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "The other eval are not really",
      "offset": 3018,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "correlated. like these other eval people",
      "offset": 3019.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "come up with they they don't seem to you",
      "offset": 3022.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "know whatever they're doing is is not as",
      "offset": 3024.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "correlated with um the quality metrics",
      "offset": 3026.88,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "you have as the",
      "offset": 3029.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "hallucination I I I think hallucination",
      "offset": 3031.24,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "is actually testing something",
      "offset": 3033.599,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "fundamental about the model and how it's",
      "offset": 3034.8,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "trained and the other eval seem to be",
      "offset": 3036.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "just maybe overfitit to the data in some",
      "offset": 3039.64,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "way you know very specific to the data",
      "offset": 3042.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "you're using so you test it on one data",
      "offset": 3044.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "set you see one thing you get different",
      "offset": 3046.559,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "data set you get something",
      "offset": 3048,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "Mhm.",
      "offset": 3050.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Do you think of this as",
      "offset": 3052.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "uh meant or mechanistic interpretability",
      "offset": 3055.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "or is that a tool that you're using or I",
      "offset": 3059.04,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "don't I don't is that an academic thing",
      "offset": 3061.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and you're trying to solve problems.",
      "offset": 3063.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. It's an academic. I don't",
      "offset": 3065.119,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "actually read any I there's some people",
      "offset": 3066.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "ask me stuff like that. I don't know",
      "offset": 3068.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "what are they doing, you know? I don't I",
      "offset": 3069.839,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "don't look deeply at what they're doing",
      "offset": 3071.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "because uh you know I I know what I want",
      "offset": 3073.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "to I know I know theoretical physics. I",
      "offset": 3075.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "know what I want to do. Um like that",
      "offset": 3077.359,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that's what we had that's was nice to",
      "offset": 3079.839,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "have someone come and collaborate with",
      "offset": 3081.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "me you know my uh you know because we",
      "offset": 3082.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "wrote this groing paper um and you know",
      "offset": 3084.319,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "someone else I wouldn't you know go out",
      "offset": 3087.839,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "and find all these other metrics and see",
      "offset": 3089.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "what they can do the mechanistic",
      "offset": 3090.88,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "interpretability stuff I I think I you",
      "offset": 3092.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "know my impression is that it hasn't",
      "offset": 3094.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "gone anywhere right like there's like",
      "offset": 3096.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "you know half a dozen things they've",
      "offset": 3099.28,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "done and what practical impact has it",
      "offset": 3100.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "had on what anybody's doing I don't know",
      "offset": 3103.839,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "I maybe internally in anthropic it's",
      "offset": 3106.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "useful, right? Because they're doing",
      "offset": 3109.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "things specifically for their model, but",
      "offset": 3110.559,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "a lot of it seemed very specific to",
      "offset": 3112.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "specific data sets and specific models",
      "offset": 3116,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and not something you can really apply.",
      "offset": 3118.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "You know, if I'm fine-tuning a model",
      "offset": 3120.72,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for, you know, Home Depot or someone",
      "offset": 3122.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "like that, that's that's an Atlanta",
      "offset": 3124.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "company, Home Depot, how do I how do I",
      "offset": 3126.48,
      "duration": 2.839
    },
    {
      "lang": "en",
      "text": "use",
      "offset": 3128.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "it? I don't know. So, it hasn't it",
      "offset": 3129.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "hasn't been something that we've looked",
      "offset": 3132.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "at in any depth. I'm happy to",
      "offset": 3133.76,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "collaborate with anyone doing it. I'm",
      "offset": 3135.44,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "happy to kind of compare what we're",
      "offset": 3136.8,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "doing to them, but we have so many",
      "offset": 3138,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "things on our plates that we just",
      "offset": 3139.68,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "haven't looked deeply at",
      "offset": 3142,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "it. It's so it's the same thing.",
      "offset": 3144.119,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "Sometimes people ask me, well, how is",
      "offset": 3146.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "your theory related to like reproducing",
      "offset": 3147.92,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "Colonel Hilbert's basis? Some",
      "offset": 3149.76,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "ac I colonel is something from physics.",
      "offset": 3155,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "What do you guys I just don't know. I",
      "offset": 3157.68,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "mean, it's you know, if you're doing it,",
      "offset": 3159.119,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "you let you tell me. I I'll explain to",
      "offset": 3160.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you what I'm doing. I'll explain to you",
      "offset": 3162.64,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "what I'm doing and you tell me how it's",
      "offset": 3164.72,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "similar to what you're doing. I I don't",
      "offset": 3166.4,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "know what you're doing. The problem is",
      "offset": 3167.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "there's just so much going on. Yeah.",
      "offset": 3169.119,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "Every day you wake up and you know there",
      "offset": 3170.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "are a thousand new papers that have been",
      "offset": 3173.92,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you know I got 100 ML papers being",
      "offset": 3175.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "published every day. So how do you keep",
      "offset": 3177.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "up with everything? Right. How do you",
      "offset": 3179.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "keep up with everything? I I rely on",
      "offset": 3181.92,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "other people to come to me with",
      "offset": 3184.16,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "interesting stuff. Okay. You know I go",
      "offset": 3185.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "on I I if you know that's it. You know I",
      "offset": 3187.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "go on Twitter. I spent a lot of time on",
      "offset": 3190.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "Twitter trying to see what what are",
      "offset": 3191.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "people talking about on Twitter and try",
      "offset": 3193.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "to follow good people and listen to",
      "offset": 3194.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "podcasts like this and try to keep up.",
      "offset": 3196.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "But, you know, it's everything's moving",
      "offset": 3198.8,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "a thousand. You know, you're moving at,",
      "offset": 3200.319,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "you know, a couple hundred miles an",
      "offset": 3202.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "hour. So, you're just doing the best you",
      "offset": 3203.64,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "can. I I sort of lucked in that, you",
      "offset": 3205.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "know, I I did this stuff in the",
      "offset": 3209.44,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "90s and all the guys I worked with, they",
      "offset": 3211.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "they went off the guys who were really",
      "offset": 3214.64,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "sharp went off and became quants and are",
      "offset": 3216.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "now running like, you know, the",
      "offset": 3218.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "investment arm of Dubai or something",
      "offset": 3219.68,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "like this. Um uh you know, or they they",
      "offset": 3221.839,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "went off and you know, they they went",
      "offset": 3225.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "off and started companies. But most of",
      "offset": 3226.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the people in the in the the physicists",
      "offset": 3228,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "that they sort of people sort of forgot",
      "offset": 3230.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "about how you could apply theoretical",
      "offset": 3232.319,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "physics to AI. So, I kind of snuck in,",
      "offset": 3234,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "you know, through the old like an it's",
      "offset": 3237.28,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "like an old back door in the lab that",
      "offset": 3238.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "people forgot was there and I was able",
      "offset": 3240.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to sneak in and do stuff. If people I",
      "offset": 3242.559,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "think if enough people remembered all",
      "offset": 3244.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "this theoretical physics stuff that the",
      "offset": 3245.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "the opportunity wouldn't be there",
      "offset": 3247.76,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "because they would have known about it.",
      "offset": 3249.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "But I I sort of, you know, I got kind of",
      "offset": 3250.16,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "lucked out that I'm old enough to",
      "offset": 3252.16,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "remember this stuff. You talked about",
      "offset": 3253.52,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "the analogy in uh quant for all of this",
      "offset": 3255.04,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "stuff. Like what's the physics analogy",
      "offset": 3259.119,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "for all of this stuff?",
      "offset": 3261.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "Well, you know, physics and quant are",
      "offset": 3264.079,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "very close, right? So, in physics, um",
      "offset": 3265.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "this whole idea of we talk about",
      "offset": 3269.52,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "self-organized criticality and the",
      "offset": 3270.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "emergent signatures of emergence. Um",
      "offset": 3272.559,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "there there's a technique in physics",
      "offset": 3275.28,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "called reormalization group. My my",
      "offset": 3276.72,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "undergraduate adviser Ken Wilson won the",
      "offset": 3278.319,
      "duration": 2.961
    },
    {
      "lang": "en",
      "text": "Nobel Prize for developing",
      "offset": 3280.16,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "reormalization group. And it's it's",
      "offset": 3281.28,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "actually a really fundamental thing in",
      "offset": 3282.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "theoretical physics to understand the",
      "offset": 3284.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "properties of the universe. Why do",
      "offset": 3286,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "electrons have mass? Why do quarks have",
      "offset": 3287.68,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "mass? Things like this. And how do you",
      "offset": 3289.68,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "describe it? And it turns out that",
      "offset": 3291.2,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "um it can be used to describe things",
      "offset": 3294.319,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "like when water boils. Okay. When you",
      "offset": 3297.64,
      "duration": 5.719
    },
    {
      "lang": "en",
      "text": "have water and you boil it and you see",
      "offset": 3301.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "all the phase changes. Yeah. It's a",
      "offset": 3303.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "phase change. Yes. It describes the",
      "offset": 3304.88,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "phase change. It's the phase boundary.",
      "offset": 3306.48,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So reormalization group is the",
      "offset": 3308.24,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "mathematical theory used to describe",
      "offset": 3309.68,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "phase boundaries between to to describe",
      "offset": 3311.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "phase changes. That's the theory you",
      "offset": 3313.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "apply for Weight Watcher. And so it",
      "offset": 3315.68,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "turns out that um you can think of this",
      "offset": 3317.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "it's kind of convenient that",
      "offset": 3320.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "everything's a matrix. Yes. Well, you",
      "offset": 3321.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "know, uh I I'm I grew up in the Cold",
      "offset": 3324,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "War. I learned all this math. It turned",
      "offset": 3326.72,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "out to be",
      "offset": 3328.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "useful. So it it turns out like if you",
      "offset": 3329.24,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "if you think about boiling water, when",
      "offset": 3331.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you watch water boil and you look at the",
      "offset": 3333.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "size of the bubbles, there are all sorts",
      "offset": 3335.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "of bubbles. There little bubbles, medium",
      "offset": 3338,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bubbles, big bubbles, you all sorts of",
      "offset": 3339.52,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "bubbles in the water, right? There's not",
      "offset": 3340.96,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "like one size of bubble.",
      "offset": 3342.64,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "That idea is those are the correlations",
      "offset": 3345.359,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "in the system. There's little tiny",
      "offset": 3347.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "correlations and there's mediumsiz",
      "offset": 3350.24,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "correlations and really big",
      "offset": 3352,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "fluctuations. That's analogous to the",
      "offset": 3354.04,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "information in the layer. So when a",
      "offset": 3357.52,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "layer is learning, it learns little bits",
      "offset": 3361.28,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "of correlations between the the training",
      "offset": 3363.359,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "data. It'll learn sort of medium-sized",
      "offset": 3365.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "correlations and it learns long way long",
      "offset": 3367.52,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "correlations between the whole data set",
      "offset": 3370.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "across the entire data set. Right?",
      "offset": 3372.559,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "there's correlations across all the data",
      "offset": 3374.319,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and they're they're sort of equally",
      "offset": 3376.799,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "distributed. That's that's the analogy",
      "offset": 3378.76,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "from physics. The the the fact that the",
      "offset": 3381.119,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "bubbles at when you boil water at the",
      "offset": 3382.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "phase transition between water and a gas",
      "offset": 3385.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "that all those bubbles are basically the",
      "offset": 3387.28,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "same or different sizes and shapes, it's",
      "offset": 3388.88,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the exact they're all circular. They're",
      "offset": 3391.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "all different sizes. It's the same idea",
      "offset": 3393.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "that when a layer is learning the",
      "offset": 3396,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "information in the training data, it has",
      "offset": 3398.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "to learn all the correlations of all the",
      "offset": 3400.559,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "different sizes. And if it doesn't learn",
      "offset": 3402.88,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "all the correlations, then it can't",
      "offset": 3405.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "generalize that well. And and if it",
      "offset": 3407.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "learns and and that that's the idea. And",
      "offset": 3409.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "if and if you cross the boundary, you",
      "offset": 3411.68,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "know, you you might be um you might",
      "offset": 3413.839,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "overdo it or so you like you're going",
      "offset": 3416.079,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "from water to ice. You freeze out. And",
      "offset": 3418.559,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "if you freeze out, you get stuck. And if",
      "offset": 3420.559,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "you freeze out, you're overfit, right?",
      "offset": 3422.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "So there's sort of this boundary between",
      "offset": 3425.2,
      "duration": 5.919
    },
    {
      "lang": "en",
      "text": "ice, water, and gas. And you know, water",
      "offset": 3426.96,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "is sort of it it can adapt to any",
      "offset": 3431.119,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "situation. You like you remember like",
      "offset": 3435.68,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Bruce Lee said, be like the water. If I",
      "offset": 3437.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "pour the water in the vase, it becomes",
      "offset": 3439.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the vase. I pour in the cup, it becomes",
      "offset": 3441.28,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the cup. I pour in the glass, becomes",
      "offset": 3443.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the glass. Be like the water. Being able",
      "offset": 3444.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "to generalize is like being like",
      "offset": 3447.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "water. If you don't learn enough",
      "offset": 3449.88,
      "duration": 4.679
    },
    {
      "lang": "en",
      "text": "information, it's like you've overboiled",
      "offset": 3452.64,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "and you're a gas and it there's just no",
      "offset": 3454.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "structure. There's nothing there. It's",
      "offset": 3456.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just random. And if you learn too much,",
      "offset": 3457.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you freeze and you're like ice and",
      "offset": 3459.839,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you've frozen and now you can't",
      "offset": 3462.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "generalize. That that is exactly the",
      "offset": 3463.559,
      "duration": 4.121
    },
    {
      "lang": "en",
      "text": "analogy.",
      "offset": 3466,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "Yeah. And because it comes from that's",
      "offset": 3467.68,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "the physics and it's ex it's the exact",
      "offset": 3469.599,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "same mathematics and physical theory you",
      "offset": 3471.839,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "use. Um, you know, there there's there's",
      "offset": 3474.48,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "a uh I I give you like there's a there's",
      "offset": 3477.359,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "a paper that came out today was you take",
      "offset": 3480.079,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "a reinforcement learning system and you",
      "offset": 3482.72,
      "duration": 4.839
    },
    {
      "lang": "en",
      "text": "train these models",
      "offset": 3485.2,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "on random reinforcements like 25% of the",
      "offset": 3487.559,
      "duration": 6.201
    },
    {
      "lang": "en",
      "text": "feedback is just random. They get they",
      "offset": 3491.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "still get better, right? How could that",
      "offset": 3493.76,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "be? How could you fine-tune a model on",
      "offset": 3496.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "random dropout? Yeah, it right. Right.",
      "offset": 3497.839,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "It's because it's like the system is",
      "offset": 3501.839,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "frozen and you heated it up and cooled",
      "offset": 3503.599,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "it down again. And that's so much of",
      "offset": 3505.599,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "like all these training recipes is like",
      "offset": 3507.44,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "how do we introduce just enough noise,",
      "offset": 3509.2,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "just enough random so that Yeah. Yeah.",
      "offset": 3511.119,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "That's right out of uh that's right out",
      "offset": 3513.599,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "of physics that the idea of what we call",
      "offset": 3515.599,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "spin glass theory or glass theory that",
      "offset": 3517.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "when systems become too brittle",
      "offset": 3519.359,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "basically they're brittle, right?",
      "offset": 3522.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "They're brittle and brittle systems are",
      "offset": 3523.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "easy to bake. You think about a metal.",
      "offset": 3525.119,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "If you want to make a metal that's not",
      "offset": 3526.96,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "brittle, you have to heat it up, cool it",
      "offset": 3528.24,
      "duration": 2.24
    },
    {
      "lang": "en",
      "text": "down, heat it up, cool it down, heat it",
      "offset": 3529.28,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "up, cool it down, becomes strong. If you",
      "offset": 3530.48,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "freeze it really quickly, it becomes",
      "offset": 3532.079,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "brittle and it will crack. It's the same",
      "offset": 3533.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "thing. And and the math and the physics,",
      "offset": 3535.839,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "all the same physics and math, like all",
      "offset": 3537.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the math used to describe that in the",
      "offset": 3540,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "physics is the same stuff I'm using to",
      "offset": 3541.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "describe neural networks. It it just",
      "offset": 3543.52,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "turns out that uh everybody forgot it.",
      "offset": 3545.92,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Yeah, that's all because, you know, we",
      "offset": 3549.2,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "all or they all died, you know, they're",
      "offset": 3551.359,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "all, you know, Right. They're all",
      "offset": 3552.88,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "retired, right? I shouldn't say that",
      "offset": 3554.559,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "should be they're all retired, right?",
      "offset": 3557.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "All the guys who retired are not doing",
      "offset": 3558.64,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "work anymore.",
      "offset": 3560,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "But you know there there's a saying in",
      "offset": 3561.48,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "science that science progresses when old",
      "offset": 3563.839,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "scientists pass",
      "offset": 3565.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "away, right? Because they they take they",
      "offset": 3567.559,
      "duration": 4.601
    },
    {
      "lang": "en",
      "text": "they they stop they the old guys stop",
      "offset": 3569.839,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "the new guys from doing anything new.",
      "offset": 3572.16,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "They don't want anything new. So when",
      "offset": 3573.76,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "they pass away now you can start",
      "offset": 3575.28,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "publishing. You know, they're no longer",
      "offset": 3576.64,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "there interfering in what you're trying",
      "offset": 3577.76,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to do, you know. But it it turned out",
      "offset": 3579.599,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "that um it turns out that this stuff",
      "offset": 3582.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "does work if you spend the physics",
      "offset": 3585.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "theories are useful for some things and",
      "offset": 3586.96,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and we're trying to basically and and",
      "offset": 3589.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the other thing is it's to me it's it's",
      "offset": 3591.28,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "really important that you have an open",
      "offset": 3592.72,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "source tool and the work be 100%",
      "offset": 3594.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "reproducible. I I need to be able to",
      "offset": 3597.24,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "give the tool to somebody else and they",
      "offset": 3599.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "need to be able to run it and try it.",
      "offset": 3600.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "Sometimes this stuff works. Sometimes we",
      "offset": 3602.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "don't get the right result. We don't",
      "offset": 3604.799,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "know why. I don't know. What do I know?",
      "offset": 3605.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "I don't it doesn't describe you know we",
      "offset": 3608.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "80% of the time we know what's going on",
      "offset": 3609.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "but 20% of the time we see things we",
      "offset": 3611.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "don't understand and we're still you",
      "offset": 3613.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "know picking at it to try to figure it",
      "offset": 3615.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "out but hopefully the tool is still is",
      "offset": 3616.88,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "still useful to people and and that's",
      "offset": 3619.119,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "the goal of this. So how does the",
      "offset": 3621.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "reormalization group uh stuff and the",
      "offset": 3623.839,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "physics uh you know basis of this lead",
      "offset": 3626.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 3629.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "HTSR and this whole power law stuff. So",
      "offset": 3631,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "in in reormalization group there's this",
      "offset": 3635.88,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "idea of a volume preserving a scale",
      "offset": 3638.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "invariant transformation and that's the",
      "offset": 3640.16,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "idea there's a scale invariant",
      "offset": 3642.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "transformation things operate you have",
      "offset": 3643.839,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "different scales the physics if you",
      "offset": 3646.079,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "change the scale of the system the",
      "offset": 3647.92,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "physics remains the same just some of",
      "offset": 3649.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the numbers change like the the mass of",
      "offset": 3651.599,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "the electron or the mass of a quark",
      "offset": 3653.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "might change depending on the scale just",
      "offset": 3654.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you know the scale okay now is this",
      "offset": 3656.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "related to like gauge and variance and",
      "offset": 3658.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that kind of stuff do you know that",
      "offset": 3661.359,
      "duration": 4.041
    },
    {
      "lang": "en",
      "text": "stuff kind of I do know kind uh to be",
      "offset": 3662.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "technical because I'm using a hard",
      "offset": 3665.4,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "cutoff technically I pro my system is",
      "offset": 3667.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "probably not gauge invariance so gauge",
      "offset": 3669.76,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "invariance is a different kind of",
      "offset": 3671.44,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "invariance this is a scale invariance",
      "offset": 3672.4,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "and if you do reormalization group like",
      "offset": 3673.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "I probably need like loop corrections to",
      "offset": 3675.599,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "do the gauge invariance but yeah but it",
      "offset": 3677.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "turns out that uh so when I was",
      "offset": 3680.319,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "formulating the theory I was trying to",
      "offset": 3682.4,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "understand I have this HTSR theory we",
      "offset": 3684.24,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have this alpha metric and it seems to",
      "offset": 3685.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be this universal metric like every",
      "offset": 3688,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "model seems to like to be at two right",
      "offset": 3690.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "two seems to be universal There's this",
      "offset": 3692.72,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "random matrix theory tells you at the",
      "offset": 3694.96,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "bottom of the universality class there's",
      "offset": 3697.359,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "alpha equals 2 there's a little tiny",
      "offset": 3698.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "universality class in between for some",
      "offset": 3700.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "reason two is special you know I got to",
      "offset": 3702.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "figure out a way to derive this from",
      "offset": 3704.72,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "first principles and and in physics that",
      "offset": 3706,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "would be called a critical exponent a",
      "offset": 3708.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "universal critical exponent whenever you",
      "offset": 3710.559,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "measure a system near a phase transition",
      "offset": 3713.2,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "simple system simple systems um they",
      "offset": 3716.079,
      "duration": 6.081
    },
    {
      "lang": "en",
      "text": "exhibit critical exponents they what the",
      "offset": 3719.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "however the the change and say the heat",
      "offset": 3722.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "capacity is governed by some power law",
      "offset": 3724.319,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "and this is a critical exponent and it's",
      "offset": 3727.28,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the same thing you see in the",
      "offset": 3729.119,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "neuroscience the self-organized",
      "offset": 3731.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "criticality that these neurons seem to",
      "offset": 3732.319,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "have some sort of critical exponent they",
      "offset": 3734.16,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "all approach the same all the data",
      "offset": 3736.079,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "approaches the same exponent maybe",
      "offset": 3738.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "there's some fluctuations because it's",
      "offset": 3740.319,
      "duration": 2.561
    },
    {
      "lang": "en",
      "text": "you know these are small systems",
      "offset": 3741.76,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "compared to like you know boiling a pot",
      "offset": 3742.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "of water you know with you know 10 to",
      "offset": 3744.319,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "the 23rd atoms in it so this alpha is a",
      "offset": 3746.4,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "critical exponent so I knew that I need",
      "offset": 3749.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to be able figure out a way to derive",
      "offset": 3752.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this. And as I'm going through the",
      "offset": 3754.68,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "derivation sort of in the back of my",
      "offset": 3756.88,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "mind, you know, there's got to be some,",
      "offset": 3758.24,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "you know, critical exponents are",
      "offset": 3759.76,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "typically associated with reormalization",
      "offset": 3761.119,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "group. So when the reormalization group",
      "offset": 3763.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "theory applies, you typically expect to",
      "offset": 3765.44,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "see a critical exponent. So we would",
      "offset": 3767.119,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "expect that ALF equal 2 is that. So in",
      "offset": 3768.88,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "in the course of trying to derive the",
      "offset": 3771.04,
      "duration": 5.519
    },
    {
      "lang": "en",
      "text": "HTSR theory, um I realized I have to do",
      "offset": 3772.799,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "this. Yeah, in order to make the theory",
      "offset": 3776.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "the math easier, I have to make this",
      "offset": 3778.319,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "assumption about a scale invariant",
      "offset": 3780.319,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "transformation.",
      "offset": 3781.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "And then I realized oh that's re",
      "offset": 3783.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "normalization group transformation and",
      "offset": 3784.96,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "then I tested it empirically and it",
      "offset": 3786.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "turns out that when you measure alpha",
      "offset": 3789.119,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "equals 2 that that funny paral law it",
      "offset": 3792,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "turns out you can also measure the scale",
      "offset": 3794.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "invariance you can test whether the",
      "offset": 3796.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "system is scale invariant by looking at",
      "offset": 3798.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "the trace log of the IGEN values um or",
      "offset": 3800.4,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "what's or what's called the log",
      "offset": 3802.96,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "determinant. So it's a log determinant",
      "offset": 3804.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "relation that arises or the trace log.",
      "offset": 3805.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So it's simple. You just compute the",
      "offset": 3808,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "IGEN values using SVD, you know, double",
      "offset": 3809.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "or you know, whatever I solver you want.",
      "offset": 3811.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "You you you simply sum up. You take the",
      "offset": 3814.079,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "logarithm of them and you sum them up",
      "offset": 3816.72,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "and you start at the tail and you work",
      "offset": 3818.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "backwards. And when as soon as you get",
      "offset": 3820.319,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "close to zero, boom, that's that's where",
      "offset": 3822.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the information concentrates. That's the",
      "offset": 3824.799,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "scale and variance. And it turns out and",
      "offset": 3826.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and that's the connection. Now, I",
      "offset": 3828.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "haven't been able to prove that the",
      "offset": 3830,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "alpha equals 2 is in fact the critical",
      "offset": 3832.559,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "exponent for this transformation. I",
      "offset": 3834.48,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "think I know how to do it, but you know,",
      "offset": 3836,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "I I do stuff like that. I'm going to be",
      "offset": 3837.28,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "living out of my car. You know, I need",
      "offset": 3839.119,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "to get a way to fund this operation,",
      "offset": 3840.559,
      "duration": 3.081
    },
    {
      "lang": "en",
      "text": "right?",
      "offset": 3842,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "So, but I I'm fairly certain that the",
      "offset": 3843.64,
      "duration": 5.159
    },
    {
      "lang": "en",
      "text": "alpha equals 2 is related to this. Um,",
      "offset": 3846.559,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you can measure them both and if you and",
      "offset": 3848.799,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "it turns out you can measure the scale,",
      "offset": 3850.319,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "you can use it's called the dead x",
      "offset": 3851.839,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "condition because the determinant of the",
      "offset": 3853.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "of the exponent should be one. So in the",
      "offset": 3855.52,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "weight watcher you can measure it and",
      "offset": 3857.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you can see that if you violate the",
      "offset": 3859.119,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "deadex condition the scale invariant",
      "offset": 3862,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "renormalization group condition if it",
      "offset": 3863.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "violates it you seem to be overfitting",
      "offset": 3865.76,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "and it turns out like in we didn't",
      "offset": 3868,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "publish this in the groing paper because",
      "offset": 3869.28,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "it was too much because it' be like 50",
      "offset": 3871.2,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you know but it turns out it also works",
      "offset": 3873.039,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "in the groing paper we published like",
      "offset": 3874.4,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "you could see the deadex condition",
      "offset": 3875.839,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "change so it turns out that these two",
      "offset": 3877.039,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "things are very related and there's",
      "offset": 3878.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "something you can actually measure the",
      "offset": 3881.2,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "idea I'll tell you where the idea came",
      "offset": 3883.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "from if you're curious is that we had",
      "offset": 3884.72,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "this sort of side project at Black",
      "offset": 3887.2,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "Rockck that was sort of like a side",
      "offset": 3888.72,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "project. We were trying to figure out",
      "offset": 3890.079,
      "duration": 3.641
    },
    {
      "lang": "en",
      "text": "can we detect when the market's going to",
      "offset": 3891.599,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "crash and there's this and you know",
      "offset": 3893.72,
      "duration": 3.639
    },
    {
      "lang": "en",
      "text": "because you because the market crashed",
      "offset": 3895.839,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you know and we're going what if we can",
      "offset": 3897.359,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "detect this there's a theory um by a guy",
      "offset": 3899.039,
      "duration": 5.441
    },
    {
      "lang": "en",
      "text": "named Der Snee uh who has this theory",
      "offset": 3902,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "about how why markets crash he has he",
      "offset": 3904.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "actually has a book called why stock",
      "offset": 3906.48,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "markets crash they published like 20 25",
      "offset": 3907.92,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "years ago and the theory says you can",
      "offset": 3910.079,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "measure the signatures of reormalization",
      "offset": 3912.48,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "group in the stock market and we were I",
      "offset": 3914.64,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "was spending our spare time let's see if",
      "offset": 3917.68,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "we could see a measure it's It's a",
      "offset": 3918.799,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "different technique than what I'm using",
      "offset": 3920.319,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "for Weight Watcher, but it starts by",
      "offset": 3921.359,
      "duration": 3.041
    },
    {
      "lang": "en",
      "text": "looking at looking for power law",
      "offset": 3922.96,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "signatures and looking for something",
      "offset": 3924.4,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "called log periodic fluctuations around",
      "offset": 3925.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the power law signature. I have a blog",
      "offset": 3927.839,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "post on it for Bitcoin. Can you detect",
      "offset": 3929.839,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "when Bitcoin's going to crash? You know,",
      "offset": 3931.359,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "that kind of stuff. Um, so it was like",
      "offset": 3932.799,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "this hobby project. Well, you know, it's",
      "offset": 3935.2,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I'm not going to trade it. You I'll let",
      "offset": 3937.92,
      "duration": 2.879
    },
    {
      "lang": "en",
      "text": "you trade it. You know, you can protect",
      "offset": 3939.44,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "it, but can you trade it?",
      "offset": 3940.799,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "And it was like a hobby project we were",
      "offset": 3943.039,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "doing uh because I work with you know",
      "offset": 3945.359,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "with one of deer's classmates at at",
      "offset": 3946.96,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "Black Rockck and we were doing this I",
      "offset": 3948.88,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "knew about the work and we were sort of",
      "offset": 3950.079,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "doing this in my spare time because I",
      "offset": 3951.2,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "knew about this reormalization. It was",
      "offset": 3952.88,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "like one of these like crazy",
      "offset": 3954,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "applications of reormalization group and",
      "offset": 3955.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "it turned out but it was this idea that",
      "offset": 3958,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "I could measure the signatures of scale",
      "offset": 3960.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and variance in a physical system and so",
      "offset": 3962.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I had this idea there's a way to measure",
      "offset": 3965.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "scale and variance in a physical system",
      "offset": 3967.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and they use it to do things like can",
      "offset": 3969.76,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "you predict when and when an avalanche",
      "offset": 3971.44,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "like when you predict if you have a",
      "offset": 3974.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "crack in a material can you predict",
      "offset": 3975.359,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "where the material is going to like is a",
      "offset": 3977.52,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "bridge going to collapse and so you can",
      "offset": 3979.039,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "look for cracks the the best basically",
      "offset": 3980.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "the distribution of the cracks and if",
      "offset": 3982.799,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "the distribution of the cracks start",
      "offset": 3984.48,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "following a power law now you got a",
      "offset": 3986,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "problem the bridge is probably going to",
      "offset": 3987.839,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "collapse and then that's sort of like",
      "offset": 3988.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "the practical aspect of it I said gee I",
      "offset": 3990.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "wonder if I could apply this to neural",
      "offset": 3992.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "networks and it turns out you can it",
      "offset": 3994.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "turns out it works um and we can predict",
      "offset": 3996.64,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "when the crash in this case being the",
      "offset": 4000.16,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "overfitting you know you the",
      "offset": 4002.119,
      "duration": 4.281
    },
    {
      "lang": "en",
      "text": "generalization collapse that's the crash",
      "offset": 4003.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and so it turns out it works and so that",
      "offset": 4006.4,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "was sort of where the idea came from",
      "offset": 4008.72,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just sort of you know doing sort of",
      "offset": 4010.559,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "these you know when I was in When I was",
      "offset": 4012.079,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "at Black Rockck, my job was to come up",
      "offset": 4013.28,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "with crazy ideas to predict the stock",
      "offset": 4014.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "market, you know, just nutty things, you",
      "offset": 4016.28,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "know, and and the point being that um",
      "offset": 4018.799,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "only the nutty things are going to work",
      "offset": 4021.52,
      "duration": 2.559
    },
    {
      "lang": "en",
      "text": "because everyone has tried everything",
      "offset": 4022.96,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "else. So, you got to try something no",
      "offset": 4024.079,
      "duration": 3.441
    },
    {
      "lang": "en",
      "text": "one else ever tried otherwise you can't",
      "offset": 4025.599,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "predict anything. So, I would just come",
      "offset": 4027.52,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "up with this sort of this nutty stuff",
      "offset": 4029.039,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "all the time. This is one of the nutty",
      "offset": 4030.319,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "ideas that we had. And I apply and it",
      "offset": 4031.839,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "turns out you apply it to neural",
      "offset": 4034,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "networks, it it actually does work. So,",
      "offset": 4035.2,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these old theoretical physicists, you",
      "offset": 4037.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know, they and they were doing, you",
      "offset": 4038.96,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "know, they're smart guys, right? you",
      "offset": 4041.28,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "know, they made the bomb, so you know,",
      "offset": 4042.559,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "they know what they're doing. It does",
      "offset": 4044,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "work. It it's just somewhat remarkable",
      "offset": 4045.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to me that uh and so that that's what's",
      "offset": 4047.52,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "going",
      "offset": 4049.92,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "on. And I'm happy to go through all the",
      "offset": 4050.839,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "math and be, you know, nerd out on as",
      "offset": 4053.119,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "much as you want, but that's sort of",
      "offset": 4054.799,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "the, you know, if you derive all the I",
      "offset": 4055.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "have this long paper. It's about 120",
      "offset": 4057.44,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "pages long. It's in draft form. Um,",
      "offset": 4059.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "every week Mike and I, Mahoney, we I ask",
      "offset": 4061.839,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "them try to find some typos in it",
      "offset": 4063.76,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "because, you know, it's got like 500. I",
      "offset": 4065.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think it's like three 400 equations. Is",
      "offset": 4067.2,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "it up on archive or something? Is it",
      "offset": 4069.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "something? It's not on the archive yet",
      "offset": 4071.039,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "because I don't want to put on the",
      "offset": 4072.48,
      "duration": 2.639
    },
    {
      "lang": "en",
      "text": "archive till we find all the typos, but",
      "offset": 4073.76,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "I I have it on a GitHub repo and it just",
      "offset": 4075.119,
      "duration": 5.041
    },
    {
      "lang": "en",
      "text": "says draft and I'm happy to share it.",
      "offset": 4077.359,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Um, if you find like last week we found",
      "offset": 4080.16,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "I was missing a trace operator on one of",
      "offset": 4082.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "the, you know, an appendix A3. There was",
      "offset": 4084.319,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "no trace. So, until we get all the typos",
      "offset": 4086.16,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "out, it's it's it's just we don't have",
      "offset": 4088.16,
      "duration": 3.919
    },
    {
      "lang": "en",
      "text": "time to read this thing, you know. It's",
      "offset": 4090.559,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "it took me a year probably to write it.",
      "offset": 4092.079,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "Um, but it it's something we'll",
      "offset": 4094.16,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "eventually get on the archive, maybe",
      "offset": 4095.68,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "another year. like the HTSR theory",
      "offset": 4096.96,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "paper, it took us three years to get it",
      "offset": 4099.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "published. So that work was done like it",
      "offset": 4101.44,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "was done in",
      "offset": 4103.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "2018 and we didn't get it published",
      "offset": 4104.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "until 2021. It just took that long. So",
      "offset": 4107.04,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "maybe when I retire I'll get the CDL",
      "offset": 4110.64,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "paper",
      "offset": 4112.64,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "published. But, you know, I'm happy to",
      "offset": 4113.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "have anyone read it. And, you know,",
      "offset": 4115.6,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "it's, you know, if you wanna, if you",
      "offset": 4116.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "like theoretical physics and you want to",
      "offset": 4118.56,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "nerd out and spend, you know, a month,",
      "offset": 4119.92,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you know, on vacation doing this, uh,",
      "offset": 4121.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "probably you'll get divorced if you do",
      "offset": 4124,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "that. But, you know, maybe you are",
      "offset": 4125.44,
      "duration": 5.799
    },
    {
      "lang": "en",
      "text": "divorced to give you something to do.",
      "offset": 4127.12,
      "duration": 4.119
    },
    {
      "lang": "en",
      "text": "uh in the H not the HDSR paper, the",
      "offset": 4132.239,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "groing paper, one of the noted",
      "offset": 4135.04,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "limitations is that you validated all",
      "offset": 4139.239,
      "duration": 8.281
    },
    {
      "lang": "en",
      "text": "this on a three layer MLP and MNEST is",
      "offset": 4143.359,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "the data set that uh in some ways is",
      "offset": 4147.52,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "pretty far from how you'd like to use",
      "offset": 4150.239,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "these ideas. Well, you know, it it's",
      "offset": 4152.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it's a trade-off, right? If you think",
      "offset": 4155.04,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "about doing development of the",
      "offset": 4156.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "Schroinger equation, okay, I used to do",
      "offset": 4158.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "quantum chemistry. So we run quantum",
      "offset": 4160.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "chemistry on big systems, right? But you",
      "offset": 4162.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "got to start with a hydrogen atom,",
      "offset": 4164,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "right? You got to start. You have to",
      "offset": 4166,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "start if you make a theory, you at least",
      "offset": 4167.279,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "be able to do small things. I see it",
      "offset": 4169.279,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "more like the bore atom, like what I'm",
      "offset": 4171.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "doing, you know? It's Mike always says",
      "offset": 4173.12,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "that's so arrogant of you. I go, well,",
      "offset": 4174.88,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "the bore atom's wrong. The bore model is",
      "offset": 4176.319,
      "duration": 4.161
    },
    {
      "lang": "en",
      "text": "wrong. What do you mean? You know, but",
      "offset": 4178,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's more like the bore model of the",
      "offset": 4180.48,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "atom. You know, we're trying to come up",
      "offset": 4181.92,
      "duration": 2.799
    },
    {
      "lang": "en",
      "text": "with the Schroinger equation. Once we",
      "offset": 4182.96,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "come up with like when I was in grad",
      "offset": 4184.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "school, we would study, you know, like",
      "offset": 4185.92,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "hydrogen dimer, you know, or nitrogen",
      "offset": 4187.44,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "dimer and you'd see these interesting",
      "offset": 4190.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "properties and then you have to go and",
      "offset": 4192,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "apply it to big systems. So I just don't",
      "offset": 4193.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have the capital and I I'm not",
      "offset": 4195.84,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "anthropic. I don't have any funding for",
      "offset": 4197.44,
      "duration": 2.719
    },
    {
      "lang": "en",
      "text": "this at all. This has all been a hobby",
      "offset": 4198.8,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "project, right? This is all my spare",
      "offset": 4200.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "time. Um I'm trying not to get divorced,",
      "offset": 4201.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "you know, if I keep working on it. But",
      "offset": 4204,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you know, we only have so much compute",
      "offset": 4206.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "resources and we need to understand",
      "offset": 4208.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these very fundamental things. So my",
      "offset": 4210.32,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "approach to this is I'll I'll we try to",
      "offset": 4212.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "study small problems and understand them",
      "offset": 4214.719,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "analytically and you know do analytic",
      "offset": 4216.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "theory we have like the I mean the way",
      "offset": 4218.32,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "watch this idea of the reormization",
      "offset": 4220,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "group I this really is 300 pages to",
      "offset": 4221.44,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "derive the equations and in the end you",
      "offset": 4223.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "get one metric which is one little",
      "offset": 4225.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "10line subretine you can put inside the",
      "offset": 4227.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "code uh and you test it and so my my",
      "offset": 4229.92,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "idea is look I make an open source tool",
      "offset": 4232.719,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "I'll give it to people you can test it",
      "offset": 4234.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "on bigger systems and see if it's useful",
      "offset": 4236.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "and and that's the idea you know as We",
      "offset": 4239.92,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "tell like one like we don't understand",
      "offset": 4241.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "things like like the graen we'd like to",
      "offset": 4243.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "study bigger problems but you know if",
      "offset": 4245.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "you look at an attention model there",
      "offset": 4248.159,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "there are a couple things going on there",
      "offset": 4250.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "are you like they have the attention",
      "offset": 4251.84,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "block and yes well does it apply to LLMs",
      "offset": 4253.32,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "okay well it turns out it seems to work",
      "offset": 4256.32,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "really really well for the internal",
      "offset": 4259.36,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "parts of the attention block the K and Q",
      "offset": 4262.32,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "matrices it seems like like even in",
      "offset": 4264.28,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "something like llama where you know half",
      "offset": 4266.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "the model seems to be overfit not the K",
      "offset": 4268.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "and Q's They can't get us to line up",
      "offset": 4270.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "really really nicely with the theory for",
      "offset": 4272.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "some reason. The V matrix it it'll go",
      "offset": 4274.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "like this and it just blows up and comes",
      "offset": 4276.64,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "back. Like what happened? They're like",
      "offset": 4278.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "so we know that like we know it works",
      "offset": 4279.92,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "inside the attention block. Does it work",
      "offset": 4282.32,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "for all the layers of the attention",
      "offset": 4284.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "block? And if not why not? So those are",
      "offset": 4286.159,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "things we're trying to understand better",
      "offset": 4288.56,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "and it's just really hard you know it's",
      "offset": 4289.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "and those models are hard to train",
      "offset": 4291.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "right. Um, so we're trying to look at",
      "offset": 4293.12,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "like what model maybe BERT is that's a",
      "offset": 4295.44,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "that people are fine-tuning BERT in",
      "offset": 4298.48,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "production for example that that's still",
      "offset": 4300.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "a common thing people do right for",
      "offset": 4302.32,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "building classifiers and and so we that",
      "offset": 4303.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "would be like the the level of model we",
      "offset": 4305.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "might look at next all the theory has",
      "offset": 4307.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "been developed on like a this is",
      "offset": 4309.52,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "hydrogen model stuff right it you know",
      "offset": 4311.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it's the bore model right MLP unminced",
      "offset": 4313.199,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and I said and we give the code away try",
      "offset": 4316.4,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "it on fashion mints try you know we did",
      "offset": 4318.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "some experiments on CR10 CR 100 you know",
      "offset": 4320.719,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "there is point where you know it's you",
      "offset": 4323.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know I I I would love to do it on you",
      "offset": 4325.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "know big systems right but you know it's",
      "offset": 4327.84,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "compute right computes computes it's",
      "offset": 4331.44,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "expensive so being able to do",
      "offset": 4333.52,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "observational studies you know we you",
      "offset": 4335.679,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "know instead of having to run big",
      "offset": 4337.92,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "experiments let's instead of doing obs",
      "offset": 4339.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "like running experiments on small models",
      "offset": 4341.76,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which computer is expensive my approach",
      "offset": 4344.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "is sort of like doing meta experiments",
      "offset": 4346.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "by doing observational studies on",
      "offset": 4349.44,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "hundreds of models Right. So we have a",
      "offset": 4351.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "paper in nature where we looked at the",
      "offset": 4353.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "time we looked at like 500 at that time",
      "offset": 4355.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "there there weren't that you even then",
      "offset": 4357.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you know 20 you know 2020 I think we did",
      "offset": 4358.96,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it 2019 2020 look at a 100 open source",
      "offset": 4361.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "models I think it was 500 we looked at",
      "offset": 4364.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "500 open source models and compared how",
      "offset": 4365.679,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "the average metric compares to those 500",
      "offset": 4368,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "models today I have a website on the web",
      "offset": 4370.44,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "wait what's your website I just have you",
      "offset": 4373.36,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "know different models llama Quinn",
      "offset": 4374.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "deepsee you know",
      "offset": 4377.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "falcon you know we just try to look at",
      "offset": 4380.12,
      "duration": 4.039
    },
    {
      "lang": "en",
      "text": "the big models and and write up reports",
      "offset": 4382,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and show that you and and that's",
      "offset": 4384.159,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "basically the best we could. So trying",
      "offset": 4386,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "to do observational studies, you know,",
      "offset": 4387.52,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "I'd like to study like you know, you",
      "offset": 4389.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "know, 100,000 models on on uh on Hugging",
      "offset": 4392.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Face, but you know, that would probably",
      "offset": 4395.12,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "bankrupt me and my VC and I', you know,",
      "offset": 4396.239,
      "duration": 3.161
    },
    {
      "lang": "en",
      "text": "I'd probably be",
      "offset": 4398.08,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "sued. But that that's what we're trying",
      "offset": 4399.4,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "to",
      "offset": 4401.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "do. That's awesome. Awesome. Maybe an",
      "offset": 4402.28,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "interesting question since you are kind",
      "offset": 4405.6,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "of out in the field working with",
      "offset": 4408.08,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "customers that are trying to put Genai",
      "offset": 4409.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to use any uh beyond the stuff that",
      "offset": 4412.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we've discussed thus far, you know, hot",
      "offset": 4416,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "takes, hard fought lessons in terms of,",
      "offset": 4418.4,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you know, making this stuff work? Look,",
      "offset": 4421.679,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I I think that the magic has been um the",
      "offset": 4424.08,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "prompt engineering and and that that has",
      "offset": 4426.719,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "opened the door for being able to do",
      "offset": 4428.96,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "things when you don't have data. because",
      "offset": 4432.64,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "data is so hard to get and training",
      "offset": 4434.719,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "models is hard. Um, and it's hard from a",
      "offset": 4436.719,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "personnel perspective. Like you have to",
      "offset": 4440.48,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "pay a lot of money to hire people know",
      "offset": 4442.239,
      "duration": 2.401
    },
    {
      "lang": "en",
      "text": "what they're doing. And if you don't",
      "offset": 4443.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "know what you're doing, you can really",
      "offset": 4444.64,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "spin your wheels and and not make any",
      "offset": 4445.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "progress. And the Gen AI stuff has been",
      "offset": 4447.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "phenomenal. I don't think anyone really",
      "offset": 4450.96,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "expected in context learning to work the",
      "offset": 4452.92,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "way it does.",
      "offset": 4455.199,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "Now I I see a lot of people trying to do",
      "offset": 4457.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "I worked a lot inert so I see a lot of",
      "offset": 4459.28,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "people trying to do rag and they don't",
      "offset": 4461.84,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "realize rag is old rag is from the 90s",
      "offset": 4464.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that's called latent semantic analysis",
      "offset": 4467.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "that was invented Chicago I've been I've",
      "offset": 4468.88,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "been trying to do rag for years it never",
      "offset": 4470.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "really works and and I I that has been",
      "offset": 4471.92,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "um you know people think oh I'll just",
      "offset": 4474.92,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "stuff stick stuff in a vector database",
      "offset": 4477.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and it'll be fine and you know you just",
      "offset": 4480,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "get all sorts of nonsense right I mean",
      "offset": 4482.88,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it's not going to there's no guarantee",
      "offset": 4484.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "So that I think is something people are",
      "offset": 4486.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "are and I've done a lot of work in",
      "offset": 4488.56,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "search most of my a lot of industry work",
      "offset": 4490.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "in search and you sort of see there's a",
      "offset": 4492.32,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "bit of a naivity about it",
      "offset": 4494.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "that there's a lot of emphasis on",
      "offset": 4496.76,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "getting the search engine to work the",
      "offset": 4500.32,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the plumbing so right getting the vector",
      "offset": 4502.48,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "space getting the getting the the",
      "offset": 4504.96,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "documents translated into an embedding",
      "offset": 4508.679,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "and how fast can you do that because",
      "offset": 4511.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "that's slow and it takes compute and so",
      "offset": 4513.12,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you have to pay for and you know what",
      "offset": 4515.6,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "size embedding should you use and",
      "offset": 4517.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "getting the vector database up and",
      "offset": 4519.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "maintaining the database and should you",
      "offset": 4520.8,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "via should you buy a vector database or",
      "offset": 4523.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "use an offtheshelf one I mean this the",
      "offset": 4525.84,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "original one was developed by Spotify",
      "offset": 4527.92,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "something called annoy there was an",
      "offset": 4530.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "annoy package years ago and then there",
      "offset": 4531.44,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "was the one from Facebook and then one",
      "offset": 4533.52,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "from Microsoft and now there are all",
      "offset": 4535.199,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "these commercial versions and and what",
      "offset": 4536.719,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "I've seen working with search people is",
      "offset": 4538.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that 90% of the resources or more go",
      "offset": 4540,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "into these getting the thing operational",
      "offset": 4542.48,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "and then you get to for the relevance",
      "offset": 4545.12,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "and you ask, &quot;Oh, I can just do rag and",
      "offset": 4546.719,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "I'll get relevance.&quot; It's terrible. You",
      "offset": 4548.4,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "know, it just doesn't really work. And",
      "offset": 4550.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and the reason is because it doesn't",
      "offset": 4552.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "learn from the clickstream. You want to",
      "offset": 4554.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "learn if you're doing a system where",
      "offset": 4557.28,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "you're trying you're either you're",
      "offset": 4559.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "trying to learn from the clickstream,",
      "offset": 4560.239,
      "duration": 2.881
    },
    {
      "lang": "en",
      "text": "you got to learn from the click stream,",
      "offset": 4562.08,
      "duration": 2.159
    },
    {
      "lang": "en",
      "text": "which means you have to train a model on",
      "offset": 4563.12,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the clickstream. And and you know, that",
      "offset": 4564.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "means you either have to put some model",
      "offset": 4566.48,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "on top of rag. To put that in other",
      "offset": 4568.32,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "words, you talk to enough folks that are",
      "offset": 4570.56,
      "duration": 7.679
    },
    {
      "lang": "en",
      "text": "doing search and retrieval, like the",
      "offset": 4574.44,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "thing that they're focused on is",
      "offset": 4578.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "relevant as opposed to plumbing. Uh, and",
      "offset": 4579.92,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "in and rag, the thing that they're",
      "offset": 4583.12,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "focused on is, you know, that retrieval",
      "offset": 4584.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "step as opposed to the generation. And",
      "offset": 4587.199,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "uh you know what you're highlighting is",
      "offset": 4591.04,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "this idea that",
      "offset": 4593.36,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "um and also you know in traditional",
      "offset": 4596.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "search like that was what they were like",
      "offset": 4599.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "that was their job and their job was to",
      "offset": 4601.76,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "make sure that when the user searched",
      "offset": 4603.679,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "they get the results that they're",
      "offset": 4605.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "looking for and it's not like a",
      "offset": 4607.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "oneandone. It's like you know I I've",
      "offset": 4608.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "worked with I I've worked with guys from",
      "offset": 4611.679,
      "duration": 5.361
    },
    {
      "lang": "en",
      "text": "Google, eBay, Walmart, you know the big",
      "offset": 4613.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the big the big engines, right? Yeah.",
      "offset": 4617.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Yeah. Yeah. And I tell you 90% of the,",
      "offset": 4618.4,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "you know, it's just not relevance is",
      "offset": 4620.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "hard. And this is the point that I'm",
      "offset": 4623.719,
      "duration": 4.761
    },
    {
      "lang": "en",
      "text": "getting at. And people who work on it",
      "offset": 4626.159,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "for years, they work on it for years and",
      "offset": 4628.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and people who try to get into it are",
      "offset": 4630.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "very naive and you know, they I mean,",
      "offset": 4632.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I'm not sure I've ever seen an AB test",
      "offset": 4635.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "that I believe, right? I mean, I've like",
      "offset": 4637.52,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "even like they I've seen clients who",
      "offset": 4640.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "don't understand like they'll put all",
      "offset": 4642,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "this effort in the engineering don't",
      "offset": 4643.76,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "understand you have to run an AA test",
      "offset": 4644.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "just to measure the variance. they don't",
      "offset": 4646.32,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "do it like okay I mean I've worked on",
      "offset": 4648.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "internal search um I've worked on",
      "offset": 4651.199,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "internal search semantic search I mean",
      "offset": 4654.239,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "all the different variants of search I",
      "offset": 4656.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "invented technology for search we worked",
      "offset": 4657.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "on e-how first billion dollar since",
      "offset": 4659.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "Google and you know it's you know artvar",
      "offset": 4661.36,
      "duration": 4.879
    },
    {
      "lang": "en",
      "text": "acquired by Google search relevance is",
      "offset": 4663.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "widely widely",
      "offset": 4666.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "um underestimate it's it's it's",
      "offset": 4668.64,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "overestimated how hard it is I excuse me",
      "offset": 4670.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "underestimated how hard it's",
      "offset": 4673.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "misappreciated for how difficult and",
      "offset": 4675.199,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "there's very little good academic",
      "offset": 4677.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "information. It in some sense search",
      "offset": 4679.199,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "relevance is like trading on the stock",
      "offset": 4681.36,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "market. The people who really know how",
      "offset": 4682.8,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "to do it aren't going to tell you what",
      "offset": 4684.64,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to do because that's where the gold is.",
      "offset": 4685.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "And and I think that thinking that you",
      "offset": 4688.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "can just do rag and expect that to just",
      "offset": 4690.56,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "give you good results is you know are",
      "offset": 4693.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you actually even do you even know if",
      "offset": 4696.239,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "you have good results? Do you even know",
      "offset": 4697.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "what your bounce rates are? I mean you",
      "offset": 4699.04,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "know I've seen so I mean I've seen cases",
      "offset": 4700.32,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "where we you have search and so part of",
      "offset": 4702.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the machine learning of Weight Watchers",
      "offset": 4704.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "is like yeah I'd worked in search so",
      "offset": 4705.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "much I was thinking about you know I",
      "offset": 4707.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "want to redeploy the search engine every",
      "offset": 4708.88,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "day I want to retrain the model and I",
      "offset": 4710.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "want to make sure it doesn't go bananas",
      "offset": 4712.159,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "and sort of the motivation was how do I",
      "offset": 4714.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "model the thing I I can't do AB",
      "offset": 4716,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "experiments all the time they're",
      "offset": 4718.239,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "expensive expenses are expensive they're",
      "offset": 4719.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "hard they're difficult to interpret um",
      "offset": 4722.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and if a lot of data and a lot of data",
      "offset": 4724.239,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "you have and people do all sorts of",
      "offset": 4727.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they'll do things in production for",
      "offset": 4729.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "their AB tests that you really shouldn't",
      "offset": 4731.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be doing. They're running multiple tests",
      "offset": 4734,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "at the same time and there's leakage",
      "offset": 4735.6,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "between the experiments. Stuff like",
      "offset": 4737.04,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "this. I'm like, it's useless. Like the",
      "offset": 4738.719,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "the noise is I'm a I'm a I'm a I was",
      "offset": 4741.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "trained as a physical scientist, man. I",
      "offset": 4743.92,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mean, I did quantum physics. I mean, I",
      "offset": 4745.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "understand how interpretive experiments,",
      "offset": 4747.28,
      "duration": 4.959
    },
    {
      "lang": "en",
      "text": "you know, you can't have you can't have",
      "offset": 4748.96,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "error bars this big, you know, if you're",
      "offset": 4752.239,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "only looking at that much, you know, and",
      "offset": 4754.64,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "it's sort of like, what are you doing?",
      "offset": 4756.239,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "And you know it it's just well and and",
      "offset": 4758.92,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "that part of the problem with the rag",
      "offset": 4761.44,
      "duration": 3.279
    },
    {
      "lang": "en",
      "text": "stuff is that it doesn't take into",
      "offset": 4763.199,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "account the clickstream. And so you have",
      "offset": 4764.719,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to somehow do that and meaning unless",
      "offset": 4766.56,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "you've got either an implicit or",
      "offset": 4769.679,
      "duration": 5.081
    },
    {
      "lang": "en",
      "text": "explicit feedback mechanism from the",
      "offset": 4771.679,
      "duration": 6.241
    },
    {
      "lang": "en",
      "text": "user then you're flying blind. And you",
      "offset": 4774.76,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "see now with AI this is part of the",
      "offset": 4777.92,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "problem with fine-tuning is really what",
      "offset": 4779.6,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you'd like to do is and you know",
      "offset": 4781.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "fine-tune some sort of adapter on top of",
      "offset": 4782.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the rag to to adapt it so it will work.",
      "offset": 4785.12,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "Mo that thing has to work. Think about a",
      "offset": 4787.92,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "fine. You can't have you can't you're",
      "offset": 4790.159,
      "duration": 2.641
    },
    {
      "lang": "en",
      "text": "running inference if you're in",
      "offset": 4791.679,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "e-commerce. You've got a 200 millisecond",
      "offset": 4792.8,
      "duration": 4.359
    },
    {
      "lang": "en",
      "text": "250 millisecond",
      "offset": 4795.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "SLA. It can't you can't how you going to",
      "offset": 4797.159,
      "duration": 4.441
    },
    {
      "lang": "en",
      "text": "run inference on that. I you know I run",
      "offset": 4799.44,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I run SVM. The SVM has a 10 it runs in",
      "offset": 4801.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "30 milliseconds. I could you know you",
      "offset": 4804.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "know it just all the all the fact all",
      "offset": 4807.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "the in fact I can run you can run XG",
      "offset": 4810.159,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "boost in 30 milliseconds. You can run",
      "offset": 4811.92,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the SVM in under 10 milliseconds. all",
      "offset": 4813.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "the overhead is based but you know you",
      "offset": 4815.6,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "it's this latency from the network but",
      "offset": 4817.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "you're trying to run you're trying to do",
      "offset": 4819.679,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "a model and you're trying to do",
      "offset": 4821.92,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "production search and you're trying to",
      "offset": 4824.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "run inference on this huge thing you",
      "offset": 4825.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "know you've got to boil it down even",
      "offset": 4827.36,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Facebook only uses like simple embedding",
      "offset": 4829.6,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "models here's one for you know this you",
      "offset": 4831.44,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "know Facebook doesn't use PyTorch in",
      "offset": 4833.199,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "production they use Cafe 2 and they have",
      "offset": 4834.56,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "they have simple you know simple models",
      "offset": 4837.84,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "so I I think a lot of what I see sort of",
      "offset": 4840.159,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "is what I'm seeing is that there's just",
      "offset": 4842.239,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "you know search is still very very hard",
      "offset": 4843.679,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "and you know and rag is not a magic box.",
      "offset": 4846.48,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Um and you're seeing now what's really",
      "offset": 4850.719,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "interesting are integr you know people",
      "offset": 4853.52,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "trying to do integrated LLMs that learn",
      "offset": 4855.84,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "how to do search on the fly right the",
      "offset": 4857.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "search is integrated into the LLM",
      "offset": 4860.159,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "somehow. Um and and I think it's just",
      "offset": 4861.8,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "it's really it's just that's probably",
      "offset": 4864.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "the in terms of being in the field like",
      "offset": 4866.239,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "trying to get this stuff to work. Um,",
      "offset": 4868.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "and then the rag stuff itself, you know,",
      "offset": 4870.96,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you have prompt engineering issues. You",
      "offset": 4872.56,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "have to ask the right thing. You have to",
      "offset": 4874.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "get the right documents. So, I think a",
      "offset": 4875.76,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "lot of that is, you know, there's sort",
      "offset": 4877.52,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of an um a bit of a naivity about it and",
      "offset": 4878.88,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "and so if you're trying to to train and",
      "offset": 4882.239,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that still is probably one of the",
      "offset": 4884.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "biggest challenges I see. Um, and again,",
      "offset": 4886.159,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and also because you have to you have to",
      "offset": 4888.719,
      "duration": 2.721
    },
    {
      "lang": "en",
      "text": "track the click stream all the way",
      "offset": 4890.159,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "through, right? You have to track from",
      "offset": 4891.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "the user, know what the user is doing.",
      "offset": 4893.36,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "So I think there a lot of people wanting",
      "offset": 4894.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to do this kind of stuff and it there",
      "offset": 4896.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you know there's always a lot of just a",
      "offset": 4899.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "lot of money left on the table right to",
      "offset": 4902.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to get it right and and whether this",
      "offset": 4904.8,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "technology can help you you know part of",
      "offset": 4906.96,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the idea is you know can you fine-tune a",
      "offset": 4909.04,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "model that you can put like a little",
      "offset": 4910.64,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "tiny model an adapter model you can put",
      "offset": 4912,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "on top of the rag system you know if you",
      "offset": 4913.679,
      "duration": 3.321
    },
    {
      "lang": "en",
      "text": "get sort of",
      "offset": 4916,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "the if you get the prefetching right",
      "offset": 4917,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "right you get the broad spectrum of you",
      "offset": 4919.6,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "get the document set you can do",
      "offset": 4921.199,
      "duration": 2.801
    },
    {
      "lang": "en",
      "text": "relevance in real time with this stuff",
      "offset": 4922.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and that That's sort of, you know, some",
      "offset": 4924,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "of the motivation to doing this. Um, and",
      "offset": 4926.48,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "I think from the real world, you see,",
      "offset": 4928.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "and of course, now we're seeing that",
      "offset": 4930.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Google for the first time, um, since",
      "offset": 4931.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "e-how really, uh, is starting to lose",
      "offset": 4934,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "traffic, right? They're losing traffic.",
      "offset": 4936.76,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "And I I haven't I don't use Google for",
      "offset": 4938.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "anything anymore. I mean, I use Gmail,",
      "offset": 4940.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but you know, maybe Google Docs, but",
      "offset": 4942.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "what are you using instead? I use",
      "offset": 4945.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "OpenAI. I3. If I'm paying for it, I'm",
      "offset": 4947.52,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "going to use it. Mhm. I use it for",
      "offset": 4950.4,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "everything. If I don't use 03, I'll use",
      "offset": 4952.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Grock. Uh maybe I'll go to Gemini, but",
      "offset": 4954,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "you know, Grock is, you know, 03 is",
      "offset": 4956.639,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "problem with 03 is just slow. It's just",
      "offset": 4958,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "slow. Yeah. Yeah. So, I I but I'll go to",
      "offset": 4959.84,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Grock if I need something quick. You",
      "offset": 4961.76,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "know, Grock is quick. And then I'll go",
      "offset": 4963.44,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to 03 maybe for you know, and I I",
      "offset": 4964.719,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "started using codeex. Codeex is pretty",
      "offset": 4967.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "cool. Um you know, I I'm not a cursor",
      "offset": 4968.719,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "guy yet. I I you know, I try to I don't",
      "offset": 4971.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "really like this thing. It's smoking my",
      "offset": 4973.52,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "code. You know, it'll you know, you tell",
      "offset": 4975.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it to to fix a problem and it just",
      "offset": 4977.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "erased Codeex did this this morning. It",
      "offset": 4979.04,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "erased one of my unit tests. I fixed it.",
      "offset": 4980.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "It runs now. Where'd it go? It's",
      "offset": 4982.56,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "completely gone. You",
      "offset": 4984.4,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "know, I like codecs because you can",
      "offset": 4986.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "check the pull request like you you make",
      "offset": 4988.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "sure it doesn't delete everything you I",
      "offset": 4991.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I think that's, you know, um but search",
      "offset": 4993.04,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "to me is still like that's one of the",
      "offset": 4995.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "big ones and one of the the main uses",
      "offset": 4996.639,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "for this technology because it really",
      "offset": 4999.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "gives you a natural language interface",
      "offset": 5001.04,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "to search. Um I I was almost suggested",
      "offset": 5002.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the other day I wanted to remake Arvar.",
      "offset": 5005.679,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "Arvar was a product that we that was",
      "offset": 5007.28,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "sold to Google. I was a a scientist at",
      "offset": 5008.96,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "Arvar. Um where it would you would ask",
      "offset": 5011.08,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "it a question, it would go out and find",
      "offset": 5013.679,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "someone on the internet to answer the",
      "offset": 5015.28,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "question for you and I wanted to",
      "offset": 5016.719,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "integrate this into into the LLM. So",
      "offset": 5019.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "when it's when it lies to you or screws",
      "offset": 5021.28,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "something up and you don't know how to",
      "offset": 5023.12,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "fix it because you're way out of your",
      "offset": 5024.96,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "le, you know, you're punching way out of",
      "offset": 5027.04,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "your weight class. You'll go find",
      "offset": 5028.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "someone to fix the problem that the LLM",
      "offset": 5030.159,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "created. Like find a real person like",
      "offset": 5032.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "find an expert to fix whatever the LLM",
      "offset": 5034.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "broke. I thought that would be a good",
      "offset": 5036.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "product,",
      "offset": 5038.88,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "right? But, you know, the the technology",
      "offset": 5040.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "is amazing. I'm very bullish on it, but",
      "offset": 5043.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "you know, it's not a magic, you not a",
      "offset": 5045.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "silver bullet, right? You still have to",
      "offset": 5047.36,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "uh pay attention to what you're doing,",
      "offset": 5049.44,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "but it is an amazing technology, and",
      "offset": 5050.719,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "that's why I'm sort of all in on it,",
      "offset": 5052.4,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "right? Um s sort of like, you know,",
      "offset": 5053.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "Sergey Bren said, he came out of",
      "offset": 5056.239,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "retirement just to do this. So, I'm sort",
      "offset": 5057.92,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "of like uh um that that's what it is.",
      "offset": 5060.32,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "It's just all day long, it's all we do",
      "offset": 5063.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "now. you know, having worked in AI and",
      "offset": 5064.639,
      "duration": 3.841
    },
    {
      "lang": "en",
      "text": "machine learning since the 90s, it's",
      "offset": 5066.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "unbelievably exciting. Uh, but it it's",
      "offset": 5068.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "also a little overwhelming sometimes,",
      "offset": 5070.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "too, which is why you have these great",
      "offset": 5072.48,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "podcasts. You can try to figure out what",
      "offset": 5074.8,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "just what's actually going on. Yeah,",
      "offset": 5076.159,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "absolutely. Absolutely. Well, Charles,",
      "offset": 5078.159,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "uh, it has been great, uh, catching up",
      "offset": 5080.88,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "and digging into what you've been",
      "offset": 5083.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "working on. Thanks so much. Hey, thanks",
      "offset": 5085.199,
      "duration": 3.201
    },
    {
      "lang": "en",
      "text": "for the time. I really appreciate it,",
      "offset": 5087.12,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "Sam. All right. Thank you. All right.",
      "offset": 5088.4,
      "duration": 4.319
    }
  ],
  "cleanText": "So, think of it like baking a cake. When you bake a cake, you have the temperature, you watch the cake. Imagine a cake has a lot of layers. Well, if the oven's too hot, some layers are going to burn and the inside's not going to get cooked because you don't get good conduction through the heat. So, what you want, even when you bake a cake, you still have to be careful to adjust the temperature, right? To make sure that the layers all cook at the same way. Uh, and it's the same idea in a model.\n\n[Music]\n\nAll right, everyone. Welcome to another episode of the TWIML AI Podcast. I am your host, Sam Cherington. Today, I'm joined by Charles Martin. Charles is the founder of Calculation Consulting. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Charles, I am super excited to have you on the podcast. We've been working on this one for quite a while now.\n\nGreat. Thanks a lot, Sam. I'm really happy to be here. This is a great show. Appreciate it. I appreciate it. You know, to get things started, I'd love to have you share a little bit about your background and what you've been up to recently with our audience.\n\nSure thing. So, you know, I have a I'm an AI researcher. Um, I did my PhD at University of Chicago. You may know my more famous classmate John Jumper who just won the Nobel Prize for AlphaFold. I then was an NSF postoc. Uh I I my other um you may know another classmate of mine Jurgen Schmidhuber who basically claims to have invented everything else. So I did my so I've been doing this for a very long time. I really got I mean I I've been doing working in industry you know I left grad school and postto went into industry. I've been doing consulting work doing building AI and machine learning solutions for companies forever. I did Arvar was acquired by Google. I did e-How first billion dollar IPO since Google and probably the biggest crash. Um I was a quant on Wall Street. I was also been scientific advisor to Larry Page's family. So and during the course of that I about 10 years ago I decided to get back into uh AI research working with my friend Michael Mahoney at UC Berkeley very informally and we've been working on this project called uh what I call the WeightWatcher project which is a large which is an open source tool. We have almost 200,000 downloads which is designed to help people who are monitoring training and fine-tuning their own AI models. And all of this has been just really a passion project to get back into research using techniques from theoretical physics and chemistry to understand how these models work and how to help my clients.\n\nAnd you've been at this for a while. Uh I think we first got to know one another in the deep learning era. Uh now we're all talking about these large language models. How does this stuff apply to LLMs?\n\nWell, it it actually works really well. Um I I designed I did theory. uh but as a theorist I you know I work with engineers so I know that theory you know they say you know in theory practice and theory are the same and in practice they're different right so I started I actually got into this I had a client in all places of Slovenia and we were they were making like fake text for things like weight loss articles Amazon reviews things like that so we were generating this text I realized I'm generating like 50,000 pieces of text a day. I have no way to know if what I'm generating is any good or not. And I can't hire 500 people to evaluate it, you know, so it would blow my cost. Like I've got to get a theory. I've got to invent some kind of theory that would let me know whether these AI models are working correctly or not. And I started working on my spare time and working on, you know, kind of cracking the books and, you know, acting like a scientist again. And and that's where it's come up. So it turns out it applies really well to LLMs because they're really big and they have lots and lots of layers and this is basically theory which analyzes the weights in the layers. It's WeightWatcher so it looks at the layer weight matrices and so the more layers you have the bigger the model the better the theory works.\n\nIt's kind of interesting to me that even in that practical context, so working with a client that has a job to be done as opposed to in academia, the approach you took was a theoretical approach as opposed to a more, you know, let's call it a statistical course like the application of evals as a practice as opposed to um you know a theoretical take on it.\n\nI I have I did all this stuff in the 90s and I have this background in theoretical physics and chemistry. So and and of course I was at Quantum Wall Street. So some of the techniques I'm using actually are quant techniques that we used on Wall Street to predict the markets. So I knew how to apply these techniques to real systems because we use that's how we that's what we do. We do portfolio theory.\n\nWhat's an example of uh a technique that and how is it used on Wall Street and how are you using it in your project?\n\nSo in portfol when you're like at a big place I was at BlackRock, you know that's the 800 pound gorilla on Wall Street and we we have you know the the group I was in you couldn't even trade in the group unless you had $200 million, right? That was the scale so it was a big portfolio and you have these big portfolios and you have to figure out signal versus noise where's the signal in the portfolio versus noise you're trying to reallocate how much Google, how much Apple, how much GM, how much, you know, that kind of stuff in the portfolio and so you have to decide signal versus noise and so you use something called random matrix theory to do that and you can detect the signal and the noise and it's important that you not peak at the stock market data you not peak at things because you'll you'll overfit yourself to the history so there's two things going I'm using random matrix theory is what we do and I'm making sure not to look at the data because if you look at the data you'll fool yourself, right? You'll think you know so that idea is actually essentially you can think of WeightWatcher as and trying to find the signal from noise in a model by looking at the individual weight matrices which are like little portfolios. They're like the portfolios you would form when you're doing portfolio theory. And I knew about some of the theory because we you we do we do theory when you're at quant. And so it turned out there were some very interesting and and they're interesting scientific properties. You know, as a scientist, I started studying this stuff and I go, you know, what I did was I just took the models that people had trained. They these open source models, right? Today we have hugging face. There are a million models on hugging face. When I started there were less than a hundred open source models. I started looking at them, right? What do they look like? Which what are the what do the good ones look like versus the bad ones? Just like when you're a quant, what do good companies look like versus bad companies? And you analyze their properties. And it turned out there was some interesting science behind this. Um, and one of the guys I worked with, the guy who's the head of futures and equities at BlackRock was a theoretical physicist. And you know, he showed me some of the stuff they were doing. You know, we can apply this to AI. And it just turned out to work, you know, and I just kept digging and digging and digging. It just worked better and better and better and then I discovered there's some some deep science in this. So that's where we ended up.\n\nAnd so traditionally in the ML AI world, the way to overcome that, you know, overfitting on the past data is to split it into test and train and to only look at part of it. Are you uh you're not looking at any of it?\n\nNo. No. And in fact, you don't do that in physics. Yeah. Like when you do analysis, you you you I have a very I have like a 100 I have this 120 long 20 page paper theoretical physics explains how we do things but it's it's actually different, right? We actually what you're doing is you're looking at it turns out the weight matrices when you train a model really, really well the weight matrices have these universal property I call them signatures of emergence and they actually this this idea actually comes from I actually because I did AI in the '90s I know something about neuroscience. It turns out these signatures are very similar to the signatures you see when studying spiking neurons and it turns out spiking neurons exhibit something called parallel structure and they have these sort of universal properties and we knew about them and sort of the original tool is I took these tools from computational neuroscience and I just applied them to the layers of weight matrix and it turns out they have the same signals. So it turns out that the spatial temporal correlations in the weight matrices in the layers in the neurons are very similar to what you see in actual neurons like that you would cult you take a lab you culture them you grow them in the lab and you look and there it's the same thing and and this work was pioneered actually by um some physicist in the 90s in particular a friend of you know some guys a guy named Jack Cow University of Chicago has done a lot of work on this he's sort of invented a lot of this stuff so it turns out this stuff works um and it it's actually very simple.\n\nIf you train a model, you're training a layer, right? But most people think of training the model as I minimize the training accuracy. But you minimize the error, right? Or you minimize the error, right? Accuracy of the error. Either Yeah. Excuse me. You minimize the error, not the accuracy, right? Maximize the accuracy, right? Right. Right. Right.\n\nSo, think of it like baking a cake. When you bake a cake, you have the temperature, you watch the cake. Imagine a cake has a lot of layers. Well, if the oven's too hot, some layers are going to burn and the inside's not going to get cooked because you don't get good conduction through the heat. So, what you want, even when you bake a cake, you still have to be careful to adjust the temperature right to make sure that the layers all cook at the same way. Uh, and it's the same idea in a model is that if you know, if if you're training like for example, if you turn the learning rate up too high, it turns out some of the layers overfit. So they become they they they become too there's too much information in them and they just overfit and all of a sudden they stop generalizing and and we have a very interesting paper that I've done with a fellow who came out of uh just getting his master's degree now super smart guy and he he was looking at this and he said well what do I take this whole grokking problem and it turns out if you take a simple model and you train it for a very, very long period of time you even if you don't turn the learning rate crazy up high you'll see that it starts to overfit and we can detect this. Um, and the signature of overfitting. Um, you know, really the reason I started thinking is because, you know, if I were if I were trading in the stock market and I were making an AI model, what's the one thing you don't want to do? You don't want to overfit to the history.\n\nOver fit on the history. Yeah. Because, you know, and I, believe it or not, when I was at BlackRock, we saw guys doing this. I'm like, what are you doing? You know, like you can't you can't even do this stuff. Well, that's the classic thing. You learn a little bit of machine learning. You say, let me download all of the historical data and train a model on it.\n\nYeah. Hey, you got like here's a guy a PhD from electrical engineering from Stanford. I'm like what are you guys doing? You know, but but this is a classic thing and so it's very critical to understand when the overfitting occurs and so that you know having been a quant on Wall Street are really, really sensitive to this kind of stuff. And it turns out there's some ideas from physics that are like phase transitions and where you and the stuff that was we knew this stuff happened in the 90s like we understood the theory. Um the thing is we're physicists. The physicists and the computer scientists didn't really talk that much.\n\nSo you know that so we have these theories from physics that we know we can detect this stuff but how do you apply it and that's what we're trying to figure out how to do and it turns out that yeah you could detect when the model overfits and when it's under fit and you can see it in the layers it's just like you could see this layer is burnt it it it it absorbed too much information and absorbed so much information from the training data it it it got stuck and it doesn't learn anything just it's overfit, right? Um and then there are cases where the layers are underfit like they're too the bottle's too big or something's going on. You know, the layers just didn't really learn anything. So, we we can detect that very easily. So, we'll dig into this paper.\n\nUh before we do that, um a contextual context setting question and then a possible rabbit hole. So the context setting question is uh one application of all this is uh fine-tuning models and um when we were chatting before you were talking about how fine-tuning is really difficult and a lot of folks get it wrong and find it really frustrating. Other folks I hear from say, \"Oh, fine-tuning's so easy right now.\" Uh, compared to, you know, a few years ago, like a like uh kind of square that circle for me. And talk a little bit about your specific experience with regards to fine-tuning.\n\nSo, if you you look at what people are doing, there's a report by Kenzie that said maybe 9% of companies doing AI are fine-tuning. So, the rest are probably doing prompt engineering. And and the question becomes you know look some people most people do I'd say out of those probably of those only 9% are really doing full fine-tuning on very large data sets. Yeah, you could do something like Laura which called low low rank adaptation and you can kind of tweak the model to you know if you want to get a JSON output well or you're trying to get markdown output you just tweak the model a little bit to change its outputs you can kind of steer it a little bit, right? But if you\n\n\nI want to add a huge amount of data to your model, and you want the model to learn from that data while still, you know, keeping its own knowledge. It turns out that it's actually quite hard. It's hard for a number of reasons. It's fun because I've worked in enterprises. It's hard to get good data in the enterprise, and that's part of it. I just talked to a client the other a couple weeks ago. We said, \"Yeah, we had our model running for a year in production. We had to dump it and start over because we didn't realize it was broken because something happened in the data pipeline. The data pipeline stopped, you know, somebody changed a column table, or they changed something, and it screwed up the model.\" You know, doing, I've worked with projects. I've worked with GoDaddy and eBay and Walmart, and these things happen all the time. You know, you're in a production environment, the data changes, you don't know it changed, where they told you, you know, three months later the model's cra... the model's not working. What happened? So, you know, this is the problem with fine-tuning, or you know, because you know you're, or even training your own model from scratch, that the data goes crazy and you don't know it. Um, and that's probably... and then the thing is, you have, how do you prepare good data sets? Data sets are, they're duplicates, and there's noise, and there's problems. So, in a real environment, a production environment, a big, in a legacy company, this is very hard. Um, so yeah, if you're just training on a very small data set, and you can look at the data yourself and curate it, you could probably get the data right. But when you're in a, you know, production environment with, I've worked like, you know, Walmart, millions of examples on the clickstream, you know, from the search engine, you can't curate it yourself. You need, you need tools which let you look at the model and say, \"Did something go...\"\n\nUm, it's very hard because you don't, you just don't know, you don't have visibility. The other thing that makes fine-tuning hard is just that...\n\n[Music]\n\nUm, you know, you never really know is, is, is whether you're evaluating the model correctly. There are, there are a hundred different evals, and you know, there's all this controversy, and I remember Llama 4. Oh, they, they let it put Llama 4 out, books. They cooked the books, right? They snuck it in. It turned out now the... and then it turns out that the the LMC guys, you know, they, they've been secretly h, you know, handing over the answers, like 25% of the answers to Google, and that's the coher paper that came out a couple weeks ago. Yes. Yeah. The coher... I gave a talk at coheri a few weeks ago. Yeah. So, so they've been cooking the book. So, you don't really know what the model can do, right? So, don't say that though, because they just raised a ton of money in there, you know, uh, you know, it's well, you know, they got, they're trying to figure out, right? They're trying to figure out what's going on. So, look, I work with real clients, and my clients, when my models don't work, they don't pay me. Like, you know, so I, you know, I, you know, it, it reminds me of the difference. Do you ever see the old movie Ghostbusters? Uhhuh. I remember they, they're getting ready to leave the university, and and uh, I think it was um, I can't remember which one of them said, he goes, \"Look, man, you, you've spent your whole life in academics, you've never been in the real world. They expect results.\" So I'm coming from that perspective, like, so, you know, it's yes, of course, the tool... the problem that's happened is that in the industry is that the tooling has gotten easier, and with the tooling getting easier, there's just, there's a lower bar to entry. Everybody's trying to do things, and so there's a much wider variance in what's going on. And so we see that. I don't see a lot of people, if they are fine-tuning, you know, there's a lot of problems, things break. And the goal of this product was to figure out how to make fine-tuning work really well and to detect problems in production. So if you're retraining a model every day, every, you know, I worked in search engine, retrain every hour, but you might retrain your model once a month, maybe once a week, once a month, you want to know, you want to make sure things didn't drift and go crazy. And this technology is designed to help you find those kinds of problems, that you to find problems you can't find using any other technique.\n\nBut one of the things you said, uh, that was pretty interesting was, uh, kind of the suggestion that there are different types or degrees of fine-tuning. Like there's a, you know, surface level fine-tuning and a deeper fine-tuning is, you know, I'm going to force fit your cake analogy. Like you've got a vanilla cake. Do you want just like chocolate icing, or do you want chocolate at the bottom layer, or you want to stick a layer in between? That's exactly right. Yeah. If you're just putting sprinkles on the cake, it's not a big deal. Okay. But if you're trying to, you know, stick something in the middle, you know, and I've not really seen like any like concrete, you know, taxonomy or elucidation of like degrees of fine-tuning and or what characterizes an easy finetune, what characterizes a hard fine tune. You have models that are instruction fine-tuned. Okay. So instruction fine-tuning for us, the our technology is really you're doing instruction fine-tuning on a on a 100 thousand examples or a million examples. We were working, for example, with a group in Poland, and they're trying to do instruction fine-tuning on a model to try to adapt it to the Polish language, and we found there were some funny things going on, like they were, they're trying to do, there's a model called Solar, which is quite good, and they were trying to adapt the Solar technology to their model called bio, and the model's not bad, right? But something happened inside the fine-tuning and the model training that somehow, whatever they did, the weights and the weight matrices and the layers doesn't... and by the way, this is all published in city publication, it, it doesn't look like Solar. Like something went wrong, and we couldn't figure out what it was. Like, what, what did you guys do, you know, cuz you're trying to replicate the engine. You want to keep what you like about the base model, but add some behavior or add some knowledge or add something uh at the same time. And it sounds like they broke something fundamental. They broke something. And the thing is, you're trying to follow an instruction that somebody else has given you, but you don't really know what's like, you're trying to do something a little different, and it's difficult to make the recipe exact, right? You changed something a little bit, and now you know, you used a different kind of flour, a different kind of oil, a different kind of butter. Something went wrong, and you don't know why. And and this stuff is so ephemeral, you know, it's so opaque. You know, whatever you do on one data set might not work on another data set for whatever reason. We don't know why. And that's what we see happening is that they tried to replicate the training process exactly, but when you look at the models, they're quite different. What, what did... why did what happened? And so quite different in terms of the the weight behavior and performance or the...\n\nOkay. Some from the perspective of your...\n\nYeah. So, one thing that this tool does is it makes clear some set of differences between these two, you know, what they started with and what they ended up with. And the problem is that you don't know what the problems are going to be until you go into production. People start doing things. You know, there are a million things you could do. You're trying to, like I said, you're generating fake texts. You're answering questions. You're doing things. You don't know how to respond. So, we're already a client of yours, and like it was natural to use WeightWatcher, like were they use weight using Weight Watchers from the beginning and so it was clear what...\n\nNo, no. They came in later. What happens? They're doing it, they train the model. What did they see then? Yeah. What did they see that said, \"We need help with this?\" The layer WeightWatcher gives you a layer quality metric. So if I have a model with a thousand layers, and you have some of these models now have a thousand layers, right? But you have a thousand layers, maybe a 100 layers in your model, every layer gets a quality metric, what's the score on the layer, and that score should be somewhere between two and four, two and six in that range, two, say two to five. Now they... now what you find when you fine-tune is you can look at the fine-tuning update. So I train the model, here's the update. I want to look at the update. If you've train... if the update should show good quality scores between two and four, two and five. If you look at the instruction fine-tuning updates of Llama, Quinn, Falcon, Mistral, they all show reasonably good layer qualities. You look at these, even Solar, their instruction fine-tuning for some reason had a large number of layers that seem to be underfit. The layer quality was too high. I should say quality. The quality was low in the sense that the the metric was above six, maybe nine, maybe 10, maybe 15, way too high, which indicates the layer is almost random. So what happened that those layers for some reason did not learn any information, or if they learned it, they learned it very weakly. There's only a very small amount of information those layers learned. And and that's, you know, you're wasting a lot of compute. You know, you're spending all... I mean, they're running on a supercomputer in Poland. So, you know, they're, they're spending all this compute time, energy to figure out what's going on. And that's what the tool is telling you. You know, there, there's a step before that, what was the thing that they were observing in the training process that that caused them to call you? Was it just...\n\nWell, performance or they're...\n\nYeah. Yeah. They're just trying to get the performance up. They're trying different things. They're again, they don't really know what to... Yeah. You read a paper, there's instructions, there's code, but when you run on your data set, it does something different. So, we read this paper. Based on everything we read, we should be able to take our data, apply, you know, a fine-tuning approach to it and see performance like this. But we're seeing performance down here. Can you help us figure out what's wrong, right? And so, one of the things they, they're trying... one of the things people do now is on these big models, they're doing all sorts of... it used to be you take a model, you train it, right? Now people are trying to like, they tried to replicate part of the model. Here's part of the model, and we're going to take these layers and replicate them and stick them up here. Right? And then we're going to take two models. We're going to merge them together. Right? So they do all these funny things, right? And oh, and then people say, \"Oh, you can replicate layers. You can merge models together.\" And it's kind of like that's a little strange, you know? And it turns out it gives goofy results. And that's the problem, right? And you don't know, did you do it correctly or not. They just don't know. You just don't know because you don't really know, like the recipe doesn't have enough detail, and it depends on the ingredients, which are, you know, which change, they're not stable. So that's what makes this stuff so hard, and and similar to fine-tuning, like if you don't have the exact data, and maybe the hyperparameters were wrong, like maybe our... maybe the hyperparameters for this data set are different from the hyperparameters for our data set. How do we select the hyperparameters? How do we select the learning rates for each layer? Should we have dropout? Should we not have dropout? Should we have weight decay? Should we not have weight decay? These are the open questions, and it changes from data set to data set. How would you know? And and this is the kind of thing that we found that it was very difficult to try to, and especially because you're running on a supercomput. You could run it once, right, on the computer. It's, it runs for a few weeks. You come back, you know, you're not Google. You can't run it a 100 times on their million node cluster.\n\nI mentioned that I had a possible degression rabbit hole, and that is uh, when you were describing the spiking nature of the neural networks, uh, it made me think of like Na... the Jeff Hawkins stuff, like do you have you come across that? Like I've met them at the conferences. I like what they're doing, you know, it's uh...\n\nLook, the idea of modeling spiking neurons was developed by a guy named Jack... there were sort of two or three people at the time in the 1960s. Uh, and one of them is Jack Cowan. If you follow Schmid Huber on Twitter, you know, he was complaining that Hotfield got the Nobel Prize, and he said, \"You deserve the Nobel Prize.\" You know, these guys, you know, what are you going on Twitter? He's also, you know, what are you going on Twitter saying stuff like this, you know? I mean, come on, man. You know, I mean, that's something you stay for a bar when you're drunk, not when you're on Twitter. Um, but during the the late '60s, people started modeling neurons. You'd go in the lab, and you would model the spike electrical activity and try to come up with models for the electrical response or neurochemical response of a neuron. I was a theoretical chemist, that's what we do is what theoretical chemistry does. So it turns out that um, you know, people have like models of spiking neurons, and there are people... I mean, but I mean actual neurons, like, like in a lab, you would take the neurons, you grow them in a lab, and you put electrodes in, you watch how they spike, right? You know, some Elon Musk kind of thing. He's gonna try to, you know, how they know Neurolink. Yeah. Yeah. How it works. That started by growing neurons in a lab, right? That's where it comes from. That that was like the early work on this. So, you know, there actually is a deep connection between that stuff and AI people. Yeah. And the models we have today were developed to try to explain these spiking neurons, and they eventually became sort of computer sciency.\n\n\nModels and we run them on, you know, GPUs. Uh, but there's a lot of theory behind that, and we and there's a lot of experimental observations, and you can observe what I call signatures of emergence. These signatures, there there's a book by a guy named, um, a late physicist, Per Bak, and he invented a theory called self-organized criticality, and it's this idea that systems self-organize to a critical point, a critical point between order and chaos, and you can see the signature of this critical point between order and chaos inside many physical systems. You, you know, in particular, things like avalanches is, you know, the snow is falling on the mountain. All of a sudden, it collapses, and there's this point right before, there's this this point right before the critical point when it collapses where it's in this sort of semi-stable state of self-organized criticality. That's what's going on inside the brain. That's a theory. It's it's called the um, the critical brain hypothesis. And so we can see these signatures of criticality, I call them signatures of emergence of of AI inside actual neurons. And it turns out our experimental work shows that the closer the quality, I have this layer quality metric, there's a sweet spot at two, there's a value of two. And when all the layers reach two, we think that the model is perfectly optimized. And the goal of the theory is to try to get your model so all the layers reach two. And that's what we're trying to do, try to develop technology to do that. But you know, here you can, and that's sort of we and we see it in, you know, just really basic experiments on things like grokking and double descent, where really you can really flush it all out. But you know, you have to, you got to, you know, we try to study small models and then apply the results to larger models, and then we go back and try to figure out what's going on, and that's what we've been doing, and and it's been fairly successful. There's a lot of open questions, but you know, the idea of is I give people, so that that's what the spiking neurons are talking about is just that you can measure these signatures, and you can see them in the actual models you're training, and you can use them, you can use them to make better models.\n\nSo you've mentioned grokking a couple of times. Uh, what's grokking? Grokking is a phenomenon where if you take a model and you train it for, you know, some small data set, it it it tends to reproduce the training, it has perfect training accuracy, zero error, and people think that it's somehow memorizing the training set, and then and it has almost no test, and the test error is like as big as it could be. It's horrible, right? And then all of a sudden, it it very quickly learns how to generalize. And so the training accuracy stays high, but then the test accuracy gets high. So my test accuracy might get like 85%, you know, 90%. Not super high, but it gets high, very high for a small data set.\n\nThat's something that people in the theoretically is very odd because it's odd that you would get a model that can describe the training data perfectly. You think, well, it must be overfit, it must have memorized the data, and then all of a sudden it's able to generalize. So the kind of nonlinearity in the learning is what's interesting about, yeah, a phase transition, really. It just goes from not being able to generalize at all to being able to generalize extremely well, and it happens very, and it happens just sort of suddenly, and that's called, you know, the grokking. Yeah. And then generalization collapse. Generalization collapse, something we detected. What we define is if you continue training, all of a sudden it stops generalizing. It starts going down again. So it still has memorized the training data in some sense that the training accuracy is perfect. But it it it starts to learn, it generalizes, and all of a sudden, and it stops generalizing, and it starts going down, and it might go down to like, you know, instead of like, you know, an 10% accuracy would be random, it might go down to like 50% accuracy. So it doesn't get ter, it doesn't get, it does a little bit of learning, but it's confused. It's like in this state of confusion, deep confusion.\n\nGot it. And these terms come from the title of the paper that we've referred to here: Grokking and generalization collapse insights from HTSR theory and HTSR theory. Heavy-tailed self-regularization. You got to have an acronym in in science. Every theory needs an acronym, right? So HTSR, that's the theory. Uh, heavy-tailed self-work I published with Charles Martin back in 2021. Uh, so wow, it's been 5 years. Wow, it's hard to believe it's that long. Uh, so this is the theory behind WeightWatcher. This is why WeightWatcher works. Part part of the theory. The um, so what we discovered is that when people think about a model memorizing, they think, oh, the training accuracy is perfect, it must be memorizing. No, no, we actually, and it turns out that, you know, we discovered this other phase of memorizing, which is more like confusion, and so there's a state of memorization, and there's a state of confusion, and they're very different, and you can detect them using WeightWatcher. Both states appear to reproduce the training accuracy perfectly. Why would confusion reproduce the training uh, data perfectly? Okay. So what's happening is what's what's happening in these models. It it turns out that in the state before grokking, we call it the pre-grokking phase, training accuracy is perfect. Why can't it generalize? It turns out some of the layers have good quality, but some of the layers have very bad quality. So what's happened is you've learned the training data, but the layer that learned the training data, the layer that that needs to learn how to, the layer that's most important for generalizing hasn't converted. So the idea is that it's not that you are memorizing before, and then you're you've switched from like this thing called memorizing to this thing called uh, generalization. Uh, it's more that part of your network is sufficiently memorized or not sufficiently memorized, but sufficiently learned, right? It's converged, uh, and other critical parts have not yet, and uh, that's why you don't, and that those, that's required for the generalization. Yes.\n\nNow there's sort of a, somebody has suggested, I saw a paper recently that said the reason it it it doesn't learn, there's like a numerical instability in the softmax. And so you have, and so it just it just gets numerically unstable, and it just kind of it's trying to learn, but it just can't find its way, and you have to train it for a long period of time until it eventually jumps out of this, jumps over the hill and learns. Confusion, or what we call, you know, we call overfitting and WeightWatcher, like a confusion is that now the layers learn too much information. So they're overcon converged, they're over, they're they're they're overbaked, right? They're burned, you know, they're cooked, they're overcooked. So they've learned too much information, and now they're they're stuck, and all they know is what, and and they can't, they get confused as to what's going on because they they've learned so much that now they can't generalize. So it's a different thing. So in one case, we see the layers, some of the layers converge and going down, and some of them going to stay up here, and in the other case, they're all way down here, like they're all the way down, they're all way too low, and what you want is you want them out here in the safe range, and and that's what we're seeing, and that's, and the thing that that could happen in a real model in the real world if you're fine-tuning, you know, you're training, that you'll see some of the layers are say they're down here, they're good, but some of the layers are up here, they haven't learned anything yet, they're stuck, and if If you go too long, they all fall down. You you burn the system. You burn it. So I to me, I like the analogy of baking a cake because it's like if you turn the oven up or you leave it in the oven too long, it the whole thing will burn, right? It just it'll cook. It'll overcake. And so something has gone wrong. There's some numerical instability in the solver. Maybe the softmax is off. Maybe there's some goofiness in the training data. Something went wrong that's preventing the model from converging. And we can see that. And that's a different phenomenon than overfitting. And the goal is to try how how do you fix it? Yeah. Yeah.\n\nSo, a couple of things jump out at me as interesting here. One is this idea of like, I don't know, really just kind of thinking about this as layers, I think, and really kind of locking in on that, I think, is interesting. And then that each of these layers can be independently uh, underfit, overfit, or, you know, in the the target zone. Um, that's interesting. Uh, but then that kind of leads me to, um, and I'm this is kind of a lead into like, to what degree have you looked at these things, or is this like, you know, next uh, steps here? But like, you know, when I think about what um, I think about like data-centric AI or like the focus on data curation as a way to get models to perform better more efficiently. You can also think of it in this context as like, can I construct a incremental training data set that targets the deficiencies of a particular layer or set of layers? Yeah. Yeah. I I think so. I mean, that that's the kind, we haven't looked deeply at changing the data set as much as changing the learning rates on the layers or making a new kind of regularizer. But I think absolutely that there's something, and you because there's numerical instabilities data, right? My my take on data-centric AI is that these guys went into industry and tried to apply their their stuff, and then they realize that it's just a mess, right? Like I've been doing this for 25 years. I tell you right now, you know, you're I can't get an SVM working in production. You think you're going to get an AI model working? You have no idea what's going on in these companies, right? So, that to me is like, you know, that's just they didn't understand like their business model, what they were selling. Um, and you know, it's it's that kind of thing where um, trying to figure out, you know, how to get, you know, how to target the data correctly. That that's exactly one of the things we'd like to know. For example, is, you know, you know, this is stuff we'd like to get into more if we could, you know, is um, can we figure out which parts of the data are being learned and which parts are not, right? And now you should be able to see you pass the data through and you see which neurons light up and which ones don't. Those are the kind of things we'd like to do. You know, doing an ablation study on a model to figure out where in your data set is it insufficient, where what part of the data do you need to fill in? Maybe like part of your data set is not fully filled in, and if you added more data there, maybe even fake data, you know, autogenerate data, you might be able to improve training, stuff like that, those are things we'd like to understand better, um, and you know, we're trying to, you know, and of course, I'm trying to raise funding actually right now to do that, to develop some of this technology, um, and and I think it's just, you know, these are things that require um, just a lot of experimentation to figure out, but we we definitely see it now, like there's a paper that came out just just this week from Stanford by Chris Manning about how if you look at some of these large models like Llama or Quinn, that a bunch of the layers are not learn, they're looking at how the residuals flow through the layers, and they can see that some layers are not really converging, some layers are not learning, and like, yeah, we told you that like three, four years ago, like it's on the website. Yeah, I wish they'd used the tool and stuff, but you know, it's a verifi, it's a validation of what we've been saying for some time is that our technology can detect this without seeing the data. The the other, and was there is there a one-to-one relationship between the characteristic that they were talking about, and I'm not sure your characteristic, we're starting to just dig into it now, it it is, we we've seen cases like we have a paper that came out where I guess the broader point is like, this sounds like a really interesting way to to characterize individual layers of models, but there probably a ton of different ways to, well, yeah, there's a lot, the the main thing that we can do that no one else can do is, we we know the cutoffs. We know the bottom, we know the lower bound is two, and the upper bound is like six. So there any, you can use any model. You can measure the entropy, the distance. You do whatever you want, that all you can see, you know, if you take two models, are those practical bounds or theory? Yeah, those are practical bounds. Yeah, they they work in production. Yeah, it really does work.\n\nWere are they empirical, or are they based on some theoretical analysis, like mathematical analysis? Combination of both. They're based on a combination. There's some new theory coming out. Uh, it uses techniques from theoretical physics called renormalization group. And so it's a combination of theory and experiment going together. You know, we have theories that show there's a boundary at two. We actually have two different metrics for the lower bound. You can, and they have to line up. And when they line up, then we know. So there's there's some deep theory, like I got a 100-page theoretical physics paper on this thing to really justify it. But you see it empirically as well. And this Grokking paper in particular is an important paper for us. You know, we wrote this nice 10-page paper to really show, look, it works perfectly. And and for me, the other thing is that I had somebody else do it with the tool. So, it's independently verified, right? You know, I you know, so these are real things you can use in production. I mean, they're they're they're real. Um, the the challenge, as you say, is you have to analyze the data, you know, and that's it's always tough. I mean, from a consulting perspective, look, I I did a large project with Walmart. I tried to help them, you know, fix their search engine, you know, and you find out, oh, you know, you need to deploy your product on our Jupyter notebook serving system. Okay. But you're a consultant. You're not allowed to have access to our Jupyter Nervix servings because we put our all our customer data there, including all their credit card numbers, and you can't have access to that. Okay? You know,\n\n\nWhat do you want me to do here?\nYou know, I mean, that's very common.\nThe same thing.\nI did a project years ago, almost 20 years ago for France Telecom, and at the end of the and we had to only use fake data because I'm not an EU citizen.\nSince I'm not an EU citizen, I can't access personal data.\nAnd so what happens in these big companies is that, you know, they'll stuff all the data on Hadoop into one system.\nAnd then it turns out they have compliance issues, GDPR, CCPA, you know, you you you can't access things.\nSo it's a huge problem.\nAnd so part of WeightWatcher is that I wanted to build a tool that I didn't need access to any data because nobody ever lets me look at it anyway.\nSo, so it's a problem and these are real production problems, you know, uh, and so part of, you know, trying now to convince people, you know, it's it's an easier sell to give someone a tool and analyze the model because, you know, I mean, in a sense, I don't have the tokenizer, so I don't really I mean, I could kind of reverse engineer the model.\nIt's kind of hard, but getting customer data and trying to look at customer data is much harder.\nYou know, it's a different ask.\nUm, it's a different ask in the org.\nUm, there are compliance issues around it.\nSo, these are things that, you know, we'd like to try to do as a product level, but that takes some time to do.\nYeah, this is this is a herculean effort to try to get around a very pragmatic organizational problem.\nIt's very hard.\nYou know, I I I always tell people, you know, if you want to do something, you've got to I have a friend who's who's figured out a way to like automate all of the marketing in his company.\nAnd he's just he's totally obsessed with prompt engineering.\nHe's got this and I and he says because the last and they won't let him do a poll he can't do a poll request because he you have to get you have to get someone to sign off on the poll request.\nAnd so we built the whole thing in Google Sheets, you know, and Google Drive.\nAnd it's just this, you know, something an accountant would build, you know, and um but I said, \"The last thing you want to do is be able to do a poll request because you'll get into the engineering or once you're in the engineering or you have to follow their best practices and and those best practices may not work for what you're trying to do.\nThey're they're best practices.\nThey're just not best for you.\"\nUm and then a lot of that we see a lot of this here is as these kinds of tools, you know, to run in in an org.\nUm, part of like I say like why fine-tuning is hard.\nWell, I I've been I've had people come, we want you to build a model for us, but and we want the model and we want you to design this model for us, but you're not allowed to ever look at the data for any reason because it's a compliant.\nI'm not going to do how can I possibly get it to work?\nI don't know how it would work, but you know, so there's those kinds of things are real and they exist in big orgs.\nAnd so, um, you know, looking at data and peeking, we certainly would like to look at and peek at what's going on, uh, with the tool.\nAnd this one of the things we're trying to to figure out how to how to do that in a compliant way.\nWith the in the paper you benchmark the results against a few different approaches.\nActivation sparity, absolute weight entropy, absolute local local circuit complexity like talk a little bit about the prior work.\nSo in this paper um you know there are people at Google, DeepMind, and people at Anthropic which are trying to find out metrics to figure out what's going on in grokking and and you know we've picked sort of the four top ones that people have in this in the theory and what we find is that none of them can detect this third phase of anti-grokking.\nNone of these metrics they have I mean they can kind of detect grokking like they can see the phase transition but because there's a threshold they don't know what the thresholds are they don't know that anti-grokking or this sort of confusion phase exists, the generalization collapse, and their technique even if they could detect it, it's not clear what you know when it's happening and when it isn't.\nSo that's sort of the part of the paper is not only can we detect this new phase but none of the existing proposed metrics can detect it, only our tool can detect it.\nSo, so we can just you know we pick you know the top ones DeepMind and Anthropic, right?\nThose are the big ones, right?\nAnd so that and that's sort of the point is that whatever you know even like they have this thing called circuit complexity where you're trying you know this is a big thing that's come out of Anthropic recently, the circuits, how do you know the circuit is overfit?\nCan't tell, can't tell, does you know and and it may see that it has higher complexity or like people do compression another say oh if you compress the quality of a model is correlated to how much each layer can be compressed.\nOkay, that's true.\nNo question about it.\nQuestion is, what about if you overcompress?\nIf you overcompress, you overfit.\nThat's the third phase.\nThey can't detect that.\nSo, that's what we're able to do with the theory is that we can detect this overfitting phase.\nAnd you know, this is something that um I'll give you an example of where it might and it's not always bad, but let me give you an example where you might actually want to do this.\nWe've looked at models that are like segment anything models, the SAM models from Facebook that allow you to do zero-shot vision learning, right?\nIn other words, it can it can it turns out if you look at those a lot a lot of the early layers in the SAM models are overfit according to our theory.\nSo here's my early as in base, yes, closer to well the they're closer to the data, so you think of that early mean that you know the closer the data, the earlier the model is closer to the label, the later the layer is.\nSo the what What I think is happening is that there are these primitive features in all of vision, you know, lines, line segments and little circles and things like that that our visual system picks up.\nI think what's happening in the segment anything models is that they look at a large number of natural images and they memorize these abstract features, primitive features in the data and that's why they're able to perform good zero-shot learning because most natural images, you know, in the natural environment things are pretty much all the same.\nYou know, a tree is a tree is a tree.\nUm, and you know, it could detect what a tree is because it can see, oh, that looks like a tree because it has the same primitive features.\nYou know, a tree in China looks like a tree in the US.\nAlthough a pine tree is not the same as, you know, whatever.\nMaybe a cherry blossom in Japan is not the same as a pine tree, but its primitive features are close enough that it can segment them and detect it as a tree.\nAnd that's what I think is going on.\nSo, it's not necessarily that overfitting is always bad.\nIt might be something good.\nAnd so, if you're trying to build a zero-shot learning model, you might want to optimize for this kind of overfitting in the early layers.\nNow, I give you an example where it might be bad.\nWe've looked at models like Llama Guard, you know, these guard model guardrail models.\nGuard rails show the opposite behavior.\nThe layers near the labels is seem seem to be overfit.\nAnd so what I think is happening and again these are this is all conjectural.\nI haven't gone and done ablation studies.\nThis is all conjectural.\nBut I think what's happening in in the guard models is that it's overfitting to you know whatever the examples you're giving to try to build the guard rail.\nAnd which is why they can be you know you can get around them.\nYou can get deeper into the model and find the more abstract thing to tell the model to think about.\nIt's able to go around the guardrail.\nSo you may be that if you're trying to build guardrails for models, you may need to have and these are these are the instruction fine-tuned components on top of Llama.\nSo if you're trying to instruction fine-tune a model to give it a guardrail, you may want to have the overfitting go very very deep into the back layers near the data to prevent that kind of backdooring.\nSo, so we have cases where overfitting is not necessarily bad, but you but knowing what it is and how to detect it um is what we're trying to do with with the tool.\nAnd so the idea is you we so these are examples we've worked out and sort of the goal is look, we have an open source tool that's 200,000 downloads.\nI give it to you, try it out, then talk to me about what you're doing.\nWe'll see if we can figure out, you know, what's going on, how to make it work work for you.\nThat that's essentially the WeightWatcher project.\nAnd when you compare against these other methods, are you strictly comparing against the ability to predict this thing that you made up or are there more like objective metrics that uh that the other uh labs have published?\nI'll give you one that was very surprising.\nOkay.\nA couple years ago, I took a look at all the existing base models, you know, Llama and and you know, um, you know, whatever was back then was hot back then, you know, this stuff.\nAnd I compared the average alpha quality metric to the hallucination metric.\nOkay.\nAnd it turns out according to our theory, the closer you are to optimality, the more the model hallucinates.\nInteresting.\nYeah, that's kind of like what you would think that a thing that hallucinates more is not optimal in some way.\nNo, we're creative.\nRight.\nRight.\nRight.\nPeople are like, \"Oh, we don't want them on, but other people No, hallucination is a feature.\nIt's a In fact, it was so amazing.\nI gave a TED talk on it.\nI mean, it was incredible that like like you think about talking to a child, like my um you know, my three-year-old niece, right?\nShe'll just make stuff up.\nIt sounds good.\nI'll make up a little story.\nIt sounds good.\nI'll tell you, right?\nThat's what they do.\nThat's what children do.\nThey're creative, right?\nThey're exploring still.\nSo these models, they seem to have this this hallucination ability seems to be related to the optimal convergence properties.\nAnd so that's an example of where we looked at that was like, wow, that's really cool.\nUm, and you know, maybe it means that certain layers are contributing more to the hallucinations than others, right?\nAnd and there's a and there's a trade-off between, you know, thinking inside the box, thinking outside the box, right?\nAnd I always say, you know, people want models and not hallucinate.\nIn other words, you don't want them to be so creative, right?\nYou don't want them going off into the tangent.\nMaybe you want them really to memorize the data and not be so good at generalizing.\nAnd that tells you something about what the layer should look like.\nAnd that's the kind of thing we've been able to figure out um just using the theory and and and comparing to, you know, some things other people are doing that hopefully I I don't think people are um I don't think they're gaming the maybe they're gaming the hallucination metric, but it would be the other way.\nTrying to convince people that it's not hallucinating.\nBut we we definitely that's the kind of stuff we're seeing.\nAnd by the way, I think ours is the that was the only metric of all the metrics that was the only one that actually correlated.\nLike the other stuff didn't really it was just sort of random.\nLike just random stuff.\nAll these other Yeah.\nThe other eval are not really correlated.\nLike these other eval people come up with they they don't seem to you know whatever they're doing is is not as correlated with um the quality metrics you have as the hallucination.\nI I I think hallucination is actually testing something fundamental about the model and how it's trained and the other eval seem to be just maybe overfit to the data in some way you know very specific to the data you're using so you test it on one data set you see one thing you get different data set you get something.\nMhm.\nDo you think of this as uh meant or mechanistic interpretability or is that a tool that you're using or I don't I don't is that an academic thing and you're trying to solve problems?\nYeah.\nYeah.\nIt's an academic.\nI don't actually read any I there's some people ask me stuff like that.\nI don't know what are they doing, you know?\nI don't I don't look deeply at what they're doing because uh you know I I know what I want to I know I know theoretical physics.\nI know what I want to do.\nUm like that that's what we had that's was nice to have someone come and collaborate with me you know my uh you know because we wrote this grokking paper um and you know someone else I wouldn't you know go out and find all these other metrics and see what they can do the mechanistic interpretability stuff I I think I you know my impression is that it hasn't gone anywhere, right?\nLike there's like you know half a dozen things they've done and what practical impact has it had on what anybody's doing I don't know.\nI maybe internally in Anthropic it's useful, right?\nBecause they're doing things specifically for their model, but a lot of it seemed very specific to specific data sets and specific models and not something you can really apply.\nYou know, if I'm fine-tuning a model for, you know, Home Depot or someone like that, that's that's an Atlanta company, Home Depot, how do I how do I use it?\nI don't know.\nSo, it hasn't it hasn't been something that we've looked at in any depth.\nI'm happy to collaborate with anyone doing it.\nI'm happy to kind of compare what we're doing to them, but we have so many things on our plates that we just haven't looked deeply at it.\nIt's so it's the same thing.\nSometimes people ask me, well, how is your theory related to like reproducing Colonel Hilbert's basis?\nSome ac I colonel is something from physics.\nWhat do you guys I just don't know.\nI mean, it's you know, if you're doing it, you let you tell me.\nI I'll explain.\n\n\nTo you what I'm doing.\nI'll explain to you what I'm doing, and you tell me how it's similar to what you're doing.\nI don't know what you're doing.\nThe problem is there's just so much going on.\nYeah.\nEvery day you wake up and you know there are a thousand new papers that have been, you know, I got 100 ML papers being published every day.\nSo how do you keep up with everything?\nRight.\nHow do you keep up with everything?\nI rely on other people to come to me with interesting stuff.\nOkay.\nYou know, I go on, I, if you know, that's it.\nYou know, I go on Twitter.\nI spent a lot of time on Twitter trying to see what what are people talking about on Twitter and try to follow good people and listen to podcasts like this and try to keep up.\nBut, you know, it's everything's moving a thousand.\nYou know, you're moving at, you know, a couple hundred miles an hour.\nSo, you're just doing the best you can.\nI sort of lucked in that, you know, I did this stuff in the 90s and all the guys I worked with, they they went off, the guys who were really sharp went off and became quants and are now running like, you know, the investment arm of Dubai or something like this.\nUm, uh, you know, or they they went off and, you know, they they went off and started companies.\nBut most of the people in the in the the physicists that they sort of people sort of forgot about how you could apply theoretical physics to AI.\nSo, I kind of snuck in, you know, through the old like an it's like an old back door in the lab that people forgot was there and I was able to sneak in and do stuff.\nIf people I think if enough people remembered all this theoretical physics stuff that the the opportunity wouldn't be there because they would have known about it.\nBut I I sort of, you know, I got kind of lucked out that I'm old enough to remember this stuff.\nYou talked about the analogy in uh quant for all of this stuff.\nLike what's the physics analogy for all of this stuff?\nWell, you know, physics and quant are very close, right?\nSo, in physics, um, this whole idea of we talk about self-organized criticality and the emergent signatures of emergence.\nUm, there there's a technique in physics called reormalization group.\nMy undergraduate adviser Ken Wilson won the Nobel Prize for developing reormalization group.\nAnd it's it's actually a really fundamental thing in theoretical physics to understand the properties of the universe.\nWhy do electrons have mass?\nWhy do quarks have mass?\nThings like this.\nAnd how do you describe it?\nAnd it turns out that um it can be used to describe things like when water boils.\nOkay.\nWhen you have water and you boil it and you see all the phase changes.\nYeah.\nIt's a phase change.\nYes.\nIt describes the phase change.\nIt's the phase boundary.\nSo reormalization group is the mathematical theory used to describe phase boundaries between to to describe phase changes.\nThat's the theory you apply for WeightWatcher.\nAnd so it turns out that um you can think of this it's kind of convenient that everything's a matrix.\nYes.\nWell, you know, uh I I'm I grew up in the Cold War.\nI learned all this math.\nIt turned out to be useful.\nSo it it turns out like if you if you think about boiling water, when you watch water boil and you look at the size of the bubbles, there are all sorts of bubbles.\nThere little bubbles, medium bubbles, big bubbles, you all sorts of bubbles in the water, right?\nThere's not like one size of bubble.\nThat idea is those are the correlations in the system.\nThere's little tiny correlations and there's medium-sized correlations and really big fluctuations.\nThat's analogous to the information in the layer.\nSo when a layer is learning, it learns little bits of correlations between the the training data.\nIt'll learn sort of medium-sized correlations and it learns long way long correlations between the whole data set across the entire data set.\nRight?\nThere's correlations across all the data and they're they're sort of equally distributed.\nThat's that's the analogy from physics.\nThe the the fact that the bubbles at when you boil water at the phase transition between water and a gas that all those bubbles are basically the same or different sizes and shapes, it's the exact they're all circular.\nThey're all different sizes.\nIt's the same idea that when a layer is learning the information in the training data, it has to learn all the correlations of all the different sizes.\nAnd if it doesn't learn all the correlations, then it can't generalize that well.\nAnd and if it learns and and that that's the idea.\nAnd if and if you cross the boundary, you know, you you might be um you might overdo it or so you like you're going from water to ice.\nYou freeze out.\nAnd if you freeze out, you get stuck.\nAnd if you freeze out, you're overfit, right?\nSo there's sort of this boundary between ice, water, and gas.\nAnd you know, water is sort of it it can adapt to any situation.\nYou like you remember like Bruce Lee said, be like the water.\nIf I pour the water in the vase, it becomes the vase.\nI pour in the cup, it becomes the cup.\nI pour in the glass, becomes the glass.\nBe like the water.\nBeing able to generalize is like being like water.\nIf you don't learn enough information, it's like you've overboiled and you're a gas and it there's just no structure.\nThere's nothing there.\nIt's just random.\nAnd if you learn too much, you freeze and you're like ice and you've frozen and now you can't generalize.\nThat that is exactly the analogy.\nYeah.\nAnd because it comes from that's the physics and it's ex it's the exact same mathematics and physical theory you use.\nUm, you know, there there's there's a uh I I give you like there's a there's a paper that came out today was you take a reinforcement learning system and you train these models on random reinforcements like 25% of the feedback is just random.\nThey get they still get better, right?\nHow could that be?\nHow could you fine-tune a model on random dropout?\nYeah, it right.\nRight.\nIt's because it's like the system is frozen and you heated it up and cooled it down again.\nAnd that's so much of like all these training recipes is like how do we introduce just enough noise, just enough random so that Yeah.\nYeah.\nThat's right out of uh that's right out of physics that the idea of what we call spin glass theory or glass theory that when systems become too brittle basically they're brittle, right?\nThey're brittle and brittle systems are easy to bake.\nYou think about a metal.\nIf you want to make a metal that's not brittle, you have to heat it up, cool it down, heat it up, cool it down, heat it up, cool it down, becomes strong.\nIf you freeze it really quickly, it becomes brittle and it will crack.\nIt's the same thing.\nAnd and the math and the physics, all the same physics and math, like all the math used to describe that in the physics is the same stuff I'm using to describe deep neural networks.\nIt it just turns out that uh everybody forgot it.\nYeah, that's all because, you know, we all or they all died, you know, they're all, you know, Right.\nThey're all retired, right?\nI shouldn't say that should be they're all retired, right?\nAll the guys who retired are not doing work anymore.\nBut you know there there's a saying in science that science progresses when old scientists pass away, right?\nBecause they they take they they they stop they the old guys stop the new guys from doing anything new.\nThey don't want anything new.\nSo when they pass away now you can start publishing.\nYou know, they're no longer there interfering in what you're trying to do, you know.\nBut it it turned out that um it turns out that this stuff does work if you spend the physics theories are useful for some things and and we're trying to basically and and the other thing is it's to me it's it's really important that you have an open source tool and the work be 100% reproducible.\nI need to be able to give the tool to somebody else and they need to be able to run it and try it.\nSometimes this stuff works.\nSometimes we don't get the right result.\nWe don't know why.\nI don't know.\nWhat do I know?\nI don't it doesn't describe you know we 80% of the time we know what's going on but 20% of the time we see things we don't understand and we're still you know picking at it to try to figure it out but hopefully the tool is still is still useful to people and and that's the goal of this.\nSo how does the reormalization group uh stuff and the physics uh you know basis of this lead to HTSR and this whole power law stuff.\nSo in in reormalization group there's this idea of a volume preserving a scale invariant transformation and that's the idea there's a scale invariant transformation things operate you have different scales the physics if you change the scale of the system the physics remains the same just some of the numbers change like the the mass of the electron or the mass of a quark might change depending on the scale just you know the scale okay now is this related to like gauge and variance and that kind of stuff do you know that stuff kind of I do know kind uh to be technical because I'm using a hard cutoff technically I pro my system is probably not gauge invariance so gauge invariance is a different kind of invariance this is a scale invariance and if you do reormalization group like I probably need like loop corrections to do the gauge invariance but yeah but it turns out that uh so when I was formulating the theory I was trying to understand I have this HTSR theory we have this alpha metric and it seems to be this universal metric like every model seems to like to be at two right two seems to be universal There's this random matrix theory tells you at the bottom of the universality class there's alpha equals 2 there's a little tiny universality class in between for some reason two is special you know I got to figure out a way to derive this from first principles and and in physics that would be called a critical exponent a universal critical exponent whenever you measure a system near a phase transition simple system simple systems um they exhibit critical exponents they what the however the the change and say the heat capacity is governed by some power law and this is a critical exponent and it's the same thing you see in the neuroscience the self-organized criticality that these neurons seem to have some sort of critical exponent they all approach the same all the data approaches the same exponent maybe there's some fluctuations because it's you know these are small systems compared to like you know boiling a pot of water you know with you know 10 to the 23rd atoms in it so this alpha is a critical exponent so I knew that I need to be able figure out a way to derive this.\nAnd as I'm going through the derivation sort of in the back of my mind, you know, there's got to be some, you know, critical exponents are typically associated with reormalization group.\nSo when the reormalization group theory applies, you typically expect to see a critical exponent.\nSo we would expect that ALF equal 2 is that.\nSo in in the course of trying to derive the HTSR theory, um I realized I have to do this.\nYeah, in order to make the theory the math easier, I have to make this assumption about a scale invariant transformation.\nAnd then I realized oh that's re normalization group transformation and then I tested it empirically and it turns out that when you measure alpha equals 2 that that funny paral law it turns out you can also measure the scale invariance you can test whether the system is scale invariant by looking at the trace log of the IGEN values um or what's or what's called the log determinant.\nSo it's a log determinant relation that arises or the trace log.\nSo it's simple.\nYou just compute the IGEN values using SVD, you know, double or you know, whatever I solver you want.\nYou you you simply sum up.\nYou take the logarithm of them and you sum them up and you start at the tail and you work backwards.\nAnd when as soon as you get close to zero, boom, that's that's where the information concentrates.\nThat's the scale and variance.\nAnd it turns out and and that's the connection.\nNow, I haven't been able to prove that the alpha equals 2 is in fact the critical exponent for this transformation.\nI think I know how to do it, but you know, I I do stuff like that.\nI'm going to be living out of my car.\nYou know, I need to get a way to fund this operation, right?\nSo, but I I'm fairly certain that the alpha equals 2 is related to this.\nUm, you can measure them both and if you and it turns out you can measure the scale, you can use it's called the dead x condition because the determinant of the of the exponent should be one.\nSo in the WeightWatcher you can measure it and you can see that if you violate the deadex condition the scale invariant renormalization group condition if it violates it you seem to be overfitting and it turns out like in we didn't publish this in the grokking paper because it was too much because it'd be like 50 you know but it turns out it also works in the grokking paper we published like you could see the deadex condition change so it turns out that these two things are very related and there's something you can actually measure the idea I'll tell you where the idea came from if you're curious is that we had this sort of side project at Black Rockck that was sort of like a side project.\nWe were trying to figure out can we detect when the market's going to crash and there's this and you know because you because the market crashed you know and we're going what if we can detect this there's a theory um by a guy named Der Snee uh who has this theory about how why markets crash he has he actually has a book called why stock markets crash they published like 20 25 years ago and the theory says you can measure the signatures of reormalization group in the stock market and we were I was spending our spare time let's see if we could see a measure it's It's a different technique than what I'm using for WeightWatcher, but it starts by looking at looking for power law signatures and looking for something called log periodic fluctuations around the power law signature.\nI have a blog post on it for Bitcoin.\nCan you detect when Bitcoin's going to crash?\nYou know, that kind of stuff.\nUm, so it was like this hobby project.\nWell, you know, it's I'm not going to trade it.\nYou I'll let you trade it.\nYou know, you can protect it, but can you trade it?\nAnd it was like a hobby project\n\n\nWe were doing, uh, because I work with, you know, with one of Deer's classmates at Black Rock and we were doing this. I knew about the work, and we were sort of doing this in my spare time because I knew about this renormalization. It was like one of these like crazy applications of renormalization group, and it turned out, but it was this idea that I could measure the signatures of scale and variance in a physical system. So I had this idea: there's a way to measure scale and variance in a physical system, and they use it to do things like, can you predict when an avalanche, like when you predict if you have a crack in a material, can you predict where the material is going to, like, is a bridge going to collapse? And so you can look for cracks, the best, basically, the distribution of the cracks, and if the distribution of the cracks start following a power law, now you got a problem; the bridge is probably going to collapse. And then that's sort of like the practical aspect of it. I said, \"Gee, I wonder if I could apply this to neural networks.\" And it turns out you can; it turns out it works. Um, and we can predict when the crash, in this case, being the overfitting, you know, you the generalization collapse, that's the crash. And so it turns out it works, and so that was sort of where the idea came from, just sort of, you know, doing sort of these, you know, when I was in When I was at Black Rock, my job was to come up with crazy ideas to predict the stock market, you know, just nutty things, you know, and and the point being that, um, only the nutty things are going to work because everyone has tried everything else. So, you got to try something no one else ever tried, otherwise you can't predict anything. So, I would just come up with this sort of this nutty stuff all the time. This is one of the nutty ideas that we had. And I apply, and it turns out you apply it to neural networks, it it actually does work. So, these old theoretical physicists, you know, they and they were doing, you know, they're smart guys, right? You know, they made the bomb, so you know, they know what they're doing. It does work. It it's just somewhat remarkable to me that, uh, and so that that's what's going on. And I'm happy to go through all the math and be, you know, nerd out on as much as you want, but that's sort of the, you know, if you derive all the I have this long paper. It's about 120 pages long. It's in draft form. Um, every week Mike and I, Mahoney, we I ask them try to find some typos in it because, you know, it's got like 500. I think it's like three, 400 equations. Is it up on archive or something? Is it something? It's not on the archive yet because I don't want to put on the archive till we find all the typos, but I I have it on a GitHub repo and it just says draft, and I'm happy to share it. Um, if you find like last week we found I was missing a trace operator on one of the, you know, an appendix A3. There was no trace. So, until we get all the typos out, it's it's it's just we don't have time to read this thing, you know. It's it took me a year probably to write it. Um, but it it's something we'll eventually get on the archive, maybe another year. Like the HTSR theory paper, it took us three years to get it published. So that work was done like it was done in 2018, and we didn't get it published until 2021. It just took that long. So maybe when I retire I'll get the CDL paper published. But, you know, I'm happy to have anyone read it. And, you know, it's, you know, if you wanna, if you like theoretical physics and you want to nerd out and spend, you know, a month, you know, on vacation doing this, uh, probably you'll get divorced if you do that. But, you know, maybe you are divorced to give you something to do.\n\nUh, in the H, not the HDSR paper, the grokking paper, one of the noted limitations is that you validated all this on a three-layer MLP and MNIST is the data set that, uh, in some ways is pretty far from how you'd like to use these ideas. Well, you know, it it's it's a trade-off, right? If you think about doing development of the Schrodinger equation, okay, I used to do quantum chemistry. So we run quantum chemistry on big systems, right? But you got to start with a hydrogen atom, right? You got to start. You have to start if you make a theory, you at least be able to do small things. I see it more like the bore atom, like what I'm doing, you know? It's Mike always says that's so arrogant of you. I go, well, the bore atom's wrong. The bore model is wrong. What do you mean? You know, but it's more like the bore model of the atom. You know, we're trying to come up with the Schrodinger equation. Once we come up with like when I was in grad school, we would study, you know, like hydrogen dimer, you know, or nitrogen dimer, and you'd see these interesting properties, and then you have to go and apply it to big systems. So I just don't have the capital, and I I'm not anthropic. I don't have any funding for this at all. This has all been a hobby project, right? This is all my spare time. Um, I'm trying not to get divorced, you know, if I keep working on it. But you know, we only have so much compute resources, and we need to understand these very fundamental things. So my approach to this is I'll I'll we try to study small problems and understand them analytically, and you know, do analytic theory. We have like the I mean the WeightWatcher idea of the renormalization group, I this really is 300 pages to derive the equations, and in the end you get one metric, which is one little 10-line subroutine you can put inside the code, uh, and you test it. And so my my idea is, look, I make an open-source tool, I'll give it to people, you can test it on bigger systems and see if it's useful, and and that's the idea, you know, as We tell like one like we don't understand things like like the graen, we'd like to study bigger problems, but you know, if you look at an attention model, there there are a couple things going on. There are you like they have the attention block, and yes, well, does it apply to LLMs? Okay, well, it turns out it seems to work really, really well for the internal parts of the attention block, the K and Q matrices. It seems like like even in something like Llama, where you know half the model seems to be overfit, not the K and Q's. They can't get us to line up really, really nicely with the theory for some reason. The V matrix, it it'll go like this and it just blows up and comes back. Like what happened? They're like, so we know that like we know it works inside the attention block. Does it work for all the layers of the attention block? And if not, why not? So those are things we're trying to understand better, and it's just really hard, you know, it's and those models are hard to train, right? Um, so we're trying to look at like what model, maybe BERT is that's a that people are fine-tuning BERT in production, for example, that that's still a common thing people do, right, for building classifiers. And and so we that would be like the the level of model we might look at next. All the theory has been developed on like a this is hydrogen model stuff, right? It you know, it's the bore model, right? MLP unminced. And I said, and we give the code away, try it on fashion mints, try you know, we did some experiments on CR10, CR 100, you know, there is point where you know it's you know, I I I would love to do it on you know, big systems, right? But you know, it's compute, right? Compute's compute, it's expensive. So being able to do observational studies, you know, we you know, instead of having to run big experiments, let's instead of doing obs like running experiments on small models, which computer is expensive, my approach is sort of like doing meta experiments by doing observational studies on hundreds of models. Right. So we have a paper in nature where we looked at the time we looked at like 500 at that time, there there weren't that you even then, you know, 20, you know, 2020, I think we did it 2019, 2020, look at a 100 open-source models, I think it was 500. We looked at 500 open-source models and compared how the average metric compares to those 500 models. Today I have a website on the web.\n\nWait, what's your website?\n\nI just have, you know, different models, Llama, Quinn, DeepSee, you know, Falcon, you know, we just try to look at the big models and and write up reports and show that you and and that's basically the best we could. So trying to do observational studies, you know, I'd like to study like, you know, you know, 100,000 models on on uh on Hugging Face, but you know, that would probably bankrupt me and my VC and I', you know, I'd probably be sued. But that that's what we're trying to do.\n\nThat's awesome. Awesome. Maybe an interesting question since you are kind of out in the field working with customers that are trying to put GenAI to use, any uh beyond the stuff that we've discussed thus far, you know, hot takes, hard-fought lessons in terms of, you know, making this stuff work?\n\nLook, I I think that the magic has been um the prompt engineering, and and that that has opened the door for being able to do things when you don't have data because data is so hard to get and training models is hard. Um, and it's hard from a personnel perspective. Like you have to pay a lot of money to hire people know what they're doing. And if you don't know what you're doing, you can really spin your wheels and and not make any progress. And the Gen AI stuff has been phenomenal. I don't think anyone really expected in context learning to work the way it does.\n\nNow I I see a lot of people trying to do I worked a lot in search, so I see a lot of people trying to do RAG, and they don't realize RAG is old. RAG is from the 90s. That's called latent semantic analysis. That was invented Chicago. I've been I've been trying to do RAG for years; it never really works. And and I I that has been um you know, people think, \"Oh, I'll just stuff stick stuff in a vector database and it'll be fine,\" and you know, you just get all sorts of nonsense, right? I mean, it's not going to there's no guarantee. So that I think is something people are are, and I've done a lot of work in search, most of my a lot of industry work in search, and you sort of see there's a bit of a naivety about it.\n\nThat there's a lot of emphasis on getting the search engine to work, the the plumbing, so right, getting the vector space, getting the getting the the documents translated into an embedding, and how fast can you do that because that's slow and it takes compute, and so you have to pay for, and you know, what size embedding should you use, and getting the vector database up and maintaining the database, and should you via should you buy a vector database or use an off-the-shelf one? I mean, this the original one was developed by Spotify, something called annoy. There was an annoy package years ago, and then there was the one from Facebook, and then one from Microsoft, and now there are all these commercial versions. And and what I've seen working with search people is that 90% of the resources or more go into these getting the thing operational, and then you get to for the relevance, and you ask, \"Oh, I can just do RAG and I'll get relevance.\" It's terrible. You know, it just doesn't really work. And and the reason is because it doesn't learn from the clickstream. You want to learn if you're doing a system where you're trying you're either you're trying to learn from the clickstream, you got to learn from the click stream, which means you have to train a model on the clickstream. And and you know, that means you either have to put some model on top of RAG. To put that in other words, you talk to enough folks that are doing search and retrieval, like the thing that they're focused on is relevant as opposed to plumbing. Uh, and in and RAG, the thing that they're focused on is, you know, that retrieval step as opposed to the generation. And uh you know what you're highlighting is this idea that, um, and also you know in traditional search, like that was what they were like, that was their job, and their job was to make sure that when the user searched, they get the results that they're looking for, and it's not like a one-and-one. It's like, you know, I I've worked with I I've worked with guys from Google, eBay, Walmart, you know, the big the big the big engines, right? Yeah. Yeah. Yeah. And I tell you 90% of the, you know, it's just not relevance is hard. And this is the point that I'm getting at. And people who work on it for years, they work on it for years, and and people who try to get into it are very naive, and you know, they I mean, I'm not sure I've ever seen an AB test that I believe, right? I mean, I've like even like they I've seen clients who don't understand like they'll put all this effort in the engineering, don't understand you have to run an AA test just to measure the variance. They don't do it like okay, I mean I've worked on internal search, um, I've worked on internal search, semantic search, I mean all the different variants of search. I invented technology for search. We worked on e-how, first billion-dollar since Google, and you know, it's you know, artvar acquired by Google. Search relevance is widely widely um underestimated. It's it's it's overestimated how hard it is. I excuse me, underestimated how hard it's misappreciated for how difficult, and there's very little good academic information. It in some sense, search relevance is like trading on the stock market. The people who really know how to do it aren't going to tell you what to do because that's where the gold is. And and I think that thinking that you can just do RAG and expect that to just give you good results is, you know, are you actually even do you even know if you have good results? Do you even know what your bounce rates are? I mean, you know, I've seen so I mean, I've seen cases where we you have search, and so part of the machine learning of Weight Watcher is like, yeah, I'd worked in search so much, I was thinking about, you know, I want to redeploy the search engine every day, I want to retrain the model, and I want to make sure it doesn't go bananas, and sort of the motivation was how do I model the thing? I I can't do AB experiments all the time, they're expensive, expenses are expensive, they're hard, they're difficult to interpret, um, and if a lot of data and a lot of\n\n\nData.\nYou have and people do all sorts of things.\nThey'll do things in production for their AB tests that you really shouldn't be doing.\nThey're running multiple tests at the same time, and there's leakage between the experiments.\nStuff like this.\nI'm like, it's useless.\nLike the the noise is I'm a I'm a I'm a I was trained as a physical scientist, man.\nI mean, I did quantum physics.\nI mean, I understand how interpretive experiments, you know, you can't have you can't have error bars this big, you know, if you're only looking at that much, you know, and it's sort of like, what are you doing?\nAnd you know it it's just well and and that part of the problem with the RAG stuff is that it doesn't take into account the clickstream.\nAnd so you have to somehow do that, and meaning unless you've got either an implicit or explicit feedback mechanism from the user, then you're flying blind.\nAnd you see now with AI, this is part of the problem with fine-tuning is really what you'd like to do is and you know fine-tune some sort of adapter on top of the RAG to to adapt it so it will work.\nThat thing has to work.\nThink about a fine.\nYou can't have you can't you're running inference if you're in e-commerce.\nYou've got a 200 millisecond, 250 millisecond SLA.\nIt can't you can't how you going to run inference on that.\nI you know I run I run SVM.\nThe SVM has a 10 it runs in 30 milliseconds.\nI could you know you know it just all the all the fact all the in fact I can run you can run XG boost in 30 milliseconds.\nYou can run the SVM in under 10 milliseconds.\nAll the overhead is based, but you know you it's this latency from the network, but you're trying to run you're trying to do a model and you're trying to do production search and you're trying to run inference on this huge thing, you know, you've got to boil it down.\nEven Facebook only uses like simple embedding models.\nHere's one for you know this you know Facebook doesn't use PyTorch in production, they use Cafe 2, and they have they have simple you know simple models.\nSo I I think a lot of what I see sort of is what I'm seeing is that there's just you know search is still very, very hard, and you know and RAG is not a magic box.\nUm, and you're seeing now what's really interesting are integr you know people trying to do integrated LLMs that learn how to do search on the fly, right?\nThe search is integrated into the LLM somehow.\nUm, and and I think it's just it's really it's just that's probably the in terms of being in the field like trying to get this stuff to work.\nUm, and then the RAG stuff itself, you know, you have prompt engineering issues.\nYou have to ask the right thing.\nYou have to get the right documents.\nSo, I think a lot of that is, you know, there's sort of an um a bit of a naivety about it, and and so if you're trying to to train and that still is probably one of the biggest challenges I see.\nUm, and again, and also because you have to you have to track the click stream all the way through, right?\nYou have to track from the user, know what the user is doing.\nSo I think there a lot of people wanting to do this kind of stuff, and it there you know there's always a lot of just a lot of money left on the table, right, to to get it right.\nAnd and whether this technology can help you, you know, part of the idea is, you know, can you fine-tune a model that you can put like a little tiny model, an adapter model, you can put on top of the RAG system, you know, if you get sort of the if you get the prefetching right, right, you get the broad spectrum of you get the document set, you can do relevance in real time with this stuff, and that That's sort of, you know, some of the motivation to doing this.\nUm, and I think from the real world, you see, and of course, now we're seeing that Google for the first time, um, since e-how really, uh, is starting to lose traffic, right?\nThey're losing traffic.\nAnd I I haven't I don't use Google for anything anymore.\nI mean, I use Gmail, but you know, maybe Google Docs, but what are you using instead?\nI use OpenAI.\nI3.\nIf I'm paying for it, I'm going to use it.\nMhm.\nI use it for everything.\nIf I don't use 03, I'll use Grok.\nUh, maybe I'll go to Gemini, but you know, Grok is, you know, 03 is problem with 03 is just slow.\nIt's just slow.\nYeah.\nYeah.\nSo, I I but I'll go to Grok if I need something quick.\nYou know, Grok is quick.\nAnd then I'll go to 03 maybe for you know, and I I started using Codeex.\nCodeex is pretty cool.\nUm, you know, I I'm not a cursor guy yet.\nI I you know, I try to I don't really like this thing.\nIt's smoking my code.\nYou know, it'll you know, you tell it to to fix a problem and it just erased Codeex did this this morning.\nIt erased one of my unit tests.\nI fixed it.\nIt runs now.\nWhere'd it go?\nIt's completely gone.\nYou know, I like codecs because you can check the pull request like you you make sure it doesn't delete everything you I I think that's, you know, um, but search to me is still like that's one of the big ones and one of the the main uses for this technology because it really gives you a natural language interface to search.\nUm, I I was almost suggested the other day I wanted to remake Arvar.\nArvar was a product that we that was sold to Google.\nI was a a scientist at Arvar.\nUm, where it would you would ask it a question, it would go out and find someone on the internet to answer the question for you, and I wanted to integrate this into into the LLM.\nSo when it's when it lies to you or screws something up and you don't know how to fix it because you're way out of your le, you know, you're punching way out of your weight class.\nYou'll go find someone to fix the problem that the LLM created.\nLike find a real person, like find an expert to fix whatever the LLM broke.\nI thought that would be a good product, right?\nBut, you know, the the technology is amazing.\nI'm very bullish on it, but you know, it's not a magic, you not a silver bullet, right?\nYou still have to uh pay attention to what you're doing, but it is an amazing technology, and that's why I'm sort of all in on it, right?\nUm, s sort of like, you know, Sergey Bren said, he came out of retirement just to do this.\nSo, I'm sort of like uh um that that's what it is.\nIt's just all day long, it's all we do now.\nYou know, having worked in AI and machine learning since the 90s, it's unbelievably exciting.\nUh, but it it's also a little overwhelming sometimes, too, which is why you have these great podcasts.\nYou can try to figure out what just what's actually going on.\nYeah, absolutely.\nAbsolutely.\nWell, Charles, uh, it has been great, uh, catching up and digging into what you've been working on.\nThanks so much.\nHey, thanks for the time.\nI really appreciate it, Sam.\nAll right.\nThank you.\nAll right.\n",
  "dumpedAt": "2025-07-21T18:43:25.469Z"
}