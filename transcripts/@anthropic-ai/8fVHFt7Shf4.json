{
  "episodeId": "8fVHFt7Shf4",
  "channelSlug": "@anthropic-ai",
  "title": "Affective Use of AI",
  "publishedAt": "2025-07-08T19:00:12.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "We've been seeing in headlines that",
      "offset": 0.06,
      "duration": 1.29
    },
    {
      "lang": "en",
      "text": "people are turning to AI\nchatbots for emotional support,",
      "offset": 1.35,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "and we felt that we really\nneeded to get ahead of this issue",
      "offset": 4.59,
      "duration": 2.61
    },
    {
      "lang": "en",
      "text": "and study it ourselves.",
      "offset": 7.2,
      "duration": 1.987
    },
    {
      "lang": "en",
      "text": "- &quot;Emotional Impacts,&quot; take one.",
      "offset": 9.187,
      "duration": 0.968
    },
    {
      "lang": "en",
      "text": "Hi, I'm Alex!",
      "offset": 11.322,
      "duration": 1.038
    },
    {
      "lang": "en",
      "text": "I lead our policy and enforcement work",
      "offset": 12.36,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "on the Safeguards team here at Anthropic.",
      "offset": 14.34,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "We're the organization",
      "offset": 16.5,
      "duration": 1.05
    },
    {
      "lang": "en",
      "text": "that tries to really\nunderstand user behavior",
      "offset": 17.55,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "and then build the appropriate mitigations",
      "offset": 20.01,
      "duration": 2.37
    },
    {
      "lang": "en",
      "text": "to ensure that when users\nare engaging with Claude,",
      "offset": 22.38,
      "duration": 1.8
    },
    {
      "lang": "en",
      "text": "they're doing so in a safe way.",
      "offset": 24.18,
      "duration": 1.56
    },
    {
      "lang": "en",
      "text": "Today I'm really excited to\nhave Ryn and Miles here today",
      "offset": 25.74,
      "duration": 2.91
    },
    {
      "lang": "en",
      "text": "to talk about some of the\nwork that we've been doing",
      "offset": 28.65,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "on how users are engaging with\nClaude for emotional support.",
      "offset": 30.75,
      "duration": 4.95
    },
    {
      "lang": "en",
      "text": "I'm Miles, I'm a researcher\non the Societal Impacts team.",
      "offset": 35.7,
      "duration": 2.85
    },
    {
      "lang": "en",
      "text": "We work on understanding how Claude",
      "offset": 38.55,
      "duration": 1.95
    },
    {
      "lang": "en",
      "text": "is going to impact the world,",
      "offset": 40.5,
      "duration": 1.23
    },
    {
      "lang": "en",
      "text": "and previously that's meant\nstudying Claude's values,",
      "offset": 41.73,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "its economic impacts and bias.",
      "offset": 44.85,
      "duration": 2.25
    },
    {
      "lang": "en",
      "text": "And now that also means studying\nClaude's emotional impacts.",
      "offset": 47.1,
      "duration": 3.15
    },
    {
      "lang": "en",
      "text": "My name is Ryn and I am\na policy design manager",
      "offset": 50.25,
      "duration": 3.51
    },
    {
      "lang": "en",
      "text": "for user wellbeing on the Safeguards team.",
      "offset": 53.76,
      "duration": 2.55
    },
    {
      "lang": "en",
      "text": "I work on topics related to\nchild safety, mental health,",
      "offset": 56.31,
      "duration": 4.05
    },
    {
      "lang": "en",
      "text": "and how users are interacting with Claude.",
      "offset": 60.36,
      "duration": 4.05
    },
    {
      "lang": "en",
      "text": "My own background is in developmental",
      "offset": 64.41,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "and clinical psychology,",
      "offset": 66.81,
      "duration": 1.29
    },
    {
      "lang": "en",
      "text": "and so emotional development is a topic",
      "offset": 68.1,
      "duration": 3.03
    },
    {
      "lang": "en",
      "text": "that is very near and dear to my heart.",
      "offset": 71.13,
      "duration": 2.55
    },
    {
      "lang": "en",
      "text": "Maybe we can start with\npersonal anecdotes,",
      "offset": 73.68,
      "duration": 3.03
    },
    {
      "lang": "en",
      "text": "and I guess maybe I can go first.",
      "offset": 76.71,
      "duration": 1.83
    },
    {
      "lang": "en",
      "text": "One of the most interesting\nand probably relevant use cases",
      "offset": 78.54,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "that I have in my own daily\nlife is using Claude to help me",
      "offset": 81.6,
      "duration": 3.51
    },
    {
      "lang": "en",
      "text": "better understand some\nbehavioral changes with my kids.",
      "offset": 85.11,
      "duration": 3.21
    },
    {
      "lang": "en",
      "text": "A good example of this\nis just the other day",
      "offset": 88.32,
      "duration": 2.37
    },
    {
      "lang": "en",
      "text": "my preschool emailed me some\nfeedback on my youngest son,",
      "offset": 90.69,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "and I think it's really\neasy to bring in emotion",
      "offset": 95.01,
      "duration": 2.91
    },
    {
      "lang": "en",
      "text": "and bias when you get\nthat type of feedback.",
      "offset": 97.92,
      "duration": 1.89
    },
    {
      "lang": "en",
      "text": "But running it through Claude,",
      "offset": 99.81,
      "duration": 1.95
    },
    {
      "lang": "en",
      "text": "I think gave me some objective\nways to approach the problem",
      "offset": 101.76,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and hopefully has made me\njust a better parent overall.",
      "offset": 105.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "But how are you guys using it?",
      "offset": 109.32,
      "duration": 1.65
    },
    {
      "lang": "en",
      "text": "You know, recently I was dealing with",
      "offset": 110.97,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "a difficult situation with a friend",
      "offset": 113.25,
      "duration": 1.23
    },
    {
      "lang": "en",
      "text": "and I really wanted to share\nsome feedback with them,",
      "offset": 114.48,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "but I wanted to phrase it right,",
      "offset": 117.18,
      "duration": 1.83
    },
    {
      "lang": "en",
      "text": "and I found that Claude\nwas really, really helpful",
      "offset": 119.01,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "in helping me think through\nhow to deliver the feedback,",
      "offset": 122.7,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "where the feedback was coming from,",
      "offset": 125.67,
      "duration": 1.5
    },
    {
      "lang": "en",
      "text": "what I was really trying to\nachieve with this friend,",
      "offset": 127.17,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "and helping me understand, you know,",
      "offset": 129.27,
      "duration": 2.19
    },
    {
      "lang": "en",
      "text": "how they would receive it.",
      "offset": 131.46,
      "duration": 1.38
    },
    {
      "lang": "en",
      "text": "What I tend to lean\ntowards using Claude for",
      "offset": 132.84,
      "duration": 2.07
    },
    {
      "lang": "en",
      "text": "is really helping me\nwith content creation,",
      "offset": 134.91,
      "duration": 4.14
    },
    {
      "lang": "en",
      "text": "with completing tasks.",
      "offset": 139.05,
      "duration": 1.77
    },
    {
      "lang": "en",
      "text": "When I was planning my wedding,",
      "offset": 140.82,
      "duration": 1.11
    },
    {
      "lang": "en",
      "text": "I was using it to help\nto understand, like,",
      "offset": 141.93,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "what kinds of timelines do I need",
      "offset": 144.45,
      "duration": 1.77
    },
    {
      "lang": "en",
      "text": "to contact different vendors on?",
      "offset": 146.22,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "And I found that really\nhelpful for me emotionally",
      "offset": 148.62,
      "duration": 2.49
    },
    {
      "lang": "en",
      "text": "because it left me with more time",
      "offset": 151.11,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "for connecting with friends\nin real life and in person.",
      "offset": 153.15,
      "duration": 3.63
    },
    {
      "lang": "en",
      "text": "For the people that are using\nit for emotional support,",
      "offset": 156.78,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "why do we think folks\nare turning to Claude",
      "offset": 158.94,
      "duration": 3.03
    },
    {
      "lang": "en",
      "text": "for this use case?",
      "offset": 161.97,
      "duration": 1.2
    },
    {
      "lang": "en",
      "text": "Humans are such social\ncreatures and love interactions",
      "offset": 163.17,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "and so I think that we're just seeing",
      "offset": 166.23,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "an outgrowth of the way that",
      "offset": 169.29,
      "duration": 1.8
    },
    {
      "lang": "en",
      "text": "humans love to connect with each other.",
      "offset": 171.09,
      "duration": 4.02
    },
    {
      "lang": "en",
      "text": "People don't always have\nan in-person support",
      "offset": 175.11,
      "duration": 3.51
    },
    {
      "lang": "en",
      "text": "that they can turn to",
      "offset": 178.62,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "for some of these really tricky questions,",
      "offset": 179.453,
      "duration": 1.807
    },
    {
      "lang": "en",
      "text": "and so having that kind of\nimpartial private online forum",
      "offset": 181.26,
      "duration": 4.62
    },
    {
      "lang": "en",
      "text": "can be a great way to\npractice what you wanna say",
      "offset": 185.88,
      "duration": 2.82
    },
    {
      "lang": "en",
      "text": "when you're advocating\nfor a raise or, you know,",
      "offset": 188.7,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "connecting with someone\non a tough conversation.",
      "offset": 190.8,
      "duration": 2.73
    },
    {
      "lang": "en",
      "text": "We didn't build Claude to be\nan emotional support agent,",
      "offset": 193.53,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but we're studying it nonetheless.",
      "offset": 198.33,
      "duration": 1.71
    },
    {
      "lang": "en",
      "text": "Why is it important to\nboth of you to study this?",
      "offset": 200.04,
      "duration": 2.67
    },
    {
      "lang": "en",
      "text": "We did not design Claude to\nprovide emotional support.",
      "offset": 202.71,
      "duration": 3.45
    },
    {
      "lang": "en",
      "text": "Claude is primarily a work tool.",
      "offset": 206.16,
      "duration": 2.01
    },
    {
      "lang": "en",
      "text": "But, you know, we need to\nbe clear-eyed about the ways",
      "offset": 208.17,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "that our systems are\nbeing used and study them.",
      "offset": 211.53,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "We've been seeing in headlines that",
      "offset": 213.51,
      "duration": 1.29
    },
    {
      "lang": "en",
      "text": "people are turning to AI\nchatbots for emotional support",
      "offset": 214.8,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and we felt that we really\nneeded to get ahead of this issue",
      "offset": 218.4,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "and study it ourselves.",
      "offset": 220.98,
      "duration": 1.92
    },
    {
      "lang": "en",
      "text": "What about you, Ryn, from\nthe safety perspective?",
      "offset": 222.9,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "I think the first\nquestion I always ask is,",
      "offset": 225,
      "duration": 1.747
    },
    {
      "lang": "en",
      "text": "&quot;Well, how much is this\nactually happening?&quot;",
      "offset": 226.747,
      "duration": 4.193
    },
    {
      "lang": "en",
      "text": "I really want to ground\nour safety mechanisms",
      "offset": 230.94,
      "duration": 3.27
    },
    {
      "lang": "en",
      "text": "in the way that we build our products",
      "offset": 234.21,
      "duration": 1.47
    },
    {
      "lang": "en",
      "text": "for the benefit of humanity in data.",
      "offset": 235.68,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "And now that we have some really great",
      "offset": 238.65,
      "duration": 1.65
    },
    {
      "lang": "en",
      "text": "privacy preserving ways\nto look at this data,",
      "offset": 240.3,
      "duration": 3.45
    },
    {
      "lang": "en",
      "text": "it seemed like a really\nworthwhile area to explore.",
      "offset": 243.75,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "Why don't you guys, if you don't mind,",
      "offset": 246.72,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "walk us through some of the research.",
      "offset": 248.4,
      "duration": 1.74
    },
    {
      "lang": "en",
      "text": "How did you design this research?",
      "offset": 250.14,
      "duration": 2.31
    },
    {
      "lang": "en",
      "text": "We started with",
      "offset": 252.45,
      "duration": 1.2
    },
    {
      "lang": "en",
      "text": "a sample of a few million\nClaude conversations",
      "offset": 253.65,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "from Claude.ai and we\nused our product, Claude,",
      "offset": 256.41,
      "duration": 3.99
    },
    {
      "lang": "en",
      "text": "to scan these conversations",
      "offset": 260.4,
      "duration": 1.71
    },
    {
      "lang": "en",
      "text": "and determine if they were related to",
      "offset": 262.11,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "the kinds of affective tasks\nthat we were interested in.",
      "offset": 264.57,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "Which were interpersonal advice,",
      "offset": 267.75,
      "duration": 1.77
    },
    {
      "lang": "en",
      "text": "psychotherapy or counseling, coaching,",
      "offset": 269.52,
      "duration": 2.49
    },
    {
      "lang": "en",
      "text": "sexual role play, romantic\nrole play, and so on.",
      "offset": 272.01,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "And then we used Cleo,",
      "offset": 274.47,
      "duration": 1.2
    },
    {
      "lang": "en",
      "text": "which is a tool that we\ndeveloped here at Anthropic",
      "offset": 275.67,
      "duration": 1.77
    },
    {
      "lang": "en",
      "text": "for privacy preserving analysis,",
      "offset": 277.44,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "to categorize and group\nthose conversations",
      "offset": 279.96,
      "duration": 3.54
    },
    {
      "lang": "en",
      "text": "into these bottom up clusters.",
      "offset": 283.5,
      "duration": 1.86
    },
    {
      "lang": "en",
      "text": "And that's how we know that",
      "offset": 285.36,
      "duration": 1.02
    },
    {
      "lang": "en",
      "text": "a lot of people are turning\nto Claude for career advice",
      "offset": 286.38,
      "duration": 3.51
    },
    {
      "lang": "en",
      "text": "or thinking through difficult",
      "offset": 289.89,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "romantic relationship\nchallenges and so on.",
      "offset": 292.59,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "Parenting advice.",
      "offset": 295.56,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "Parenting advice is a big one.",
      "offset": 296.393,
      "duration": 1.867
    },
    {
      "lang": "en",
      "text": "So for anyone who's not or\nhasn't read the blog post yet,",
      "offset": 298.26,
      "duration": 3.99
    },
    {
      "lang": "en",
      "text": "what would you want the\nviewers to take away?",
      "offset": 302.25,
      "duration": 2.19
    },
    {
      "lang": "en",
      "text": "What are some of the top\nkey themes that you think",
      "offset": 304.44,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "would be important for\nthem to walk away from",
      "offset": 306.96,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "watching this video?",
      "offset": 308.64,
      "duration": 1.32
    },
    {
      "lang": "en",
      "text": "Yeah, I think that one of the",
      "offset": 309.96,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "probably most surprising findings for me",
      "offset": 312,
      "duration": 1.86
    },
    {
      "lang": "en",
      "text": "and a key theme is that we\nactually don't see this happen",
      "offset": 313.86,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "very much on Claude.ai.",
      "offset": 319.41,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "So Claude.ai is for 18 plus.",
      "offset": 321.75,
      "duration": 3.15
    },
    {
      "lang": "en",
      "text": "It's not set up like an\nemotional support chatbot",
      "offset": 324.9,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "or anything like that,",
      "offset": 328.26,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "so it's not optimized for this.",
      "offset": 329.093,
      "duration": 1.267
    },
    {
      "lang": "en",
      "text": "And so what we're seeing is\nabout 2.9% of conversations",
      "offset": 330.36,
      "duration": 4.38
    },
    {
      "lang": "en",
      "text": "in our sample of millions of conversations",
      "offset": 334.74,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "fall actually into",
      "offset": 337.86,
      "duration": 0.99
    },
    {
      "lang": "en",
      "text": "this bucket of conversations\non affective topics.",
      "offset": 338.85,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "And so there's this diversity,",
      "offset": 344.1,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "people are using it for this purpose,",
      "offset": 346.2,
      "duration": 2.37
    },
    {
      "lang": "en",
      "text": "but it still isn't this\noverwhelming majority use case,",
      "offset": 348.57,
      "duration": 4.29
    },
    {
      "lang": "en",
      "text": "for our platform at least.",
      "offset": 352.86,
      "duration": 1.65
    },
    {
      "lang": "en",
      "text": "I was really surprised by that.",
      "offset": 354.51,
      "duration": 1.44
    },
    {
      "lang": "en",
      "text": "But I can't tell if that's\nbecause I use it so often",
      "offset": 355.95,
      "duration": 2.43
    },
    {
      "lang": "en",
      "text": "in my personal life",
      "offset": 358.38,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "that I expected the numbers to be higher.",
      "offset": 359.213,
      "duration": 2.347
    },
    {
      "lang": "en",
      "text": "Miles, was there anything\nsurprising to you in the findings?",
      "offset": 361.56,
      "duration": 2.7
    },
    {
      "lang": "en",
      "text": "Yeah, I was inspired by",
      "offset": 364.26,
      "duration": 2.22
    },
    {
      "lang": "en",
      "text": "just the sheer breadth of use cases.",
      "offset": 366.48,
      "duration": 2.07
    },
    {
      "lang": "en",
      "text": "So even among the three of us,",
      "offset": 368.55,
      "duration": 1.65
    },
    {
      "lang": "en",
      "text": "there's a breadth of use cases.",
      "offset": 370.2,
      "duration": 0.997
    },
    {
      "lang": "en",
      "text": "Yeah!",
      "offset": 371.197,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "But, you know, really\nlooking at the clusters.",
      "offset": 372.03,
      "duration": 2.55
    },
    {
      "lang": "en",
      "text": "We saw, as you said, parenting advice.",
      "offset": 374.58,
      "duration": 2.1
    },
    {
      "lang": "en",
      "text": "We saw people working through",
      "offset": 376.68,
      "duration": 1.08
    },
    {
      "lang": "en",
      "text": "difficult relationship challenges.",
      "offset": 377.76,
      "duration": 1.86
    },
    {
      "lang": "en",
      "text": "We saw people discussing the\nnature of AI consciousness",
      "offset": 379.62,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "and philosophy.",
      "offset": 385.17,
      "duration": 1.62
    },
    {
      "lang": "en",
      "text": "And we also saw surprisingly little sexual",
      "offset": 386.79,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "and romantic role play.",
      "offset": 389.76,
      "duration": 1.83
    },
    {
      "lang": "en",
      "text": "That was one of our key questions\ncoming into this research",
      "offset": 391.59,
      "duration": 2.73
    },
    {
      "lang": "en",
      "text": "was whether people were\nsort of using Claude",
      "offset": 394.32,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "as an emotional companion\nor partner at large scale.",
      "offset": 397.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "I was surprised to see how rare that was.",
      "offset": 401.52,
      "duration": 2.07
    },
    {
      "lang": "en",
      "text": "It comprised less than",
      "offset": 403.59,
      "duration": 1.56
    },
    {
      "lang": "en",
      "text": "a fraction of a percent of conversations.",
      "offset": 405.15,
      "duration": 2.55
    },
    {
      "lang": "en",
      "text": "We talk a lot about some\nof the beneficial use cases",
      "offset": 407.7,
      "duration": 2.43
    },
    {
      "lang": "en",
      "text": "in this conversation in the blog,",
      "offset": 410.13,
      "duration": 1.53
    },
    {
      "lang": "en",
      "text": "but I imagine from a safety perspective,",
      "offset": 411.66,
      "duration": 2.49
    },
    {
      "lang": "en",
      "text": "and this is something\nI'm concerned about too,",
      "offset": 414.15,
      "duration": 1.56
    },
    {
      "lang": "en",
      "text": "there must be concerns that you have, Ryn.",
      "offset": 415.71,
      "duration": 2.46
    },
    {
      "lang": "en",
      "text": "Can you walk us through some of those?",
      "offset": 418.17,
      "duration": 1.95
    },
    {
      "lang": "en",
      "text": "Yeah, I think my biggest concern is",
      "offset": 420.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "if people are interacting with Claude",
      "offset": 425.37,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "as a way to avoid difficult\nconversations in person.",
      "offset": 428.49,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "It really comes down to:",
      "offset": 433.41,
      "duration": 1.62
    },
    {
      "lang": "en",
      "text": "How is the individual using\nit to lean in, ideally,",
      "offset": 435.03,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "rather than lean out from\nconnection with people?",
      "offset": 440.1,
      "duration": 2.49
    },
    {
      "lang": "en",
      "text": "I think it's important\nanytime you use a tool",
      "offset": 442.59,
      "duration": 2.79
    },
    {
      "lang": "en",
      "text": "to really know where its strengths are",
      "offset": 445.38,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "and also where its limitations are.",
      "offset": 447.96,
      "duration": 1.62
    },
    {
      "lang": "en",
      "text": "And so as somebody who\nworks in the AI space,",
      "offset": 449.58,
      "duration": 2.37
    },
    {
      "lang": "en",
      "text": "I think that's very top of mind for me",
      "offset": 451.95,
      "duration": 1.86
    },
    {
      "lang": "en",
      "text": "when I'm interacting with\nClaude and asking for advice.",
      "offset": 453.81,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "But I do worry that other folks",
      "offset": 456.78,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "who are maybe not in the AI\ncircle are less aware of that,",
      "offset": 459.06,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "and some of Claude's\nlimitations, as you mentioned.",
      "offset": 462.66,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "We are not training Claude to\nbe an emotional support agent.",
      "offset": 465.24,
      "duration": 3.15
    },
    {
      "lang": "en",
      "text": "And so just recognizing what\nClaude is capable of doing",
      "offset": 468.39,
      "duration": 3.33
    },
    {
      "lang": "en",
      "text": "and maybe what it's not,",
      "offset": 471.72,
      "duration": 1.2
    },
    {
      "lang": "en",
      "text": "and when it's important to go",
      "offset": 472.92,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "and reach out to somebody who\ndoes have that deep expertise.",
      "offset": 474.6,
      "duration": 3.9
    },
    {
      "lang": "en",
      "text": "And I think that this is something where",
      "offset": 478.5,
      "duration": 2.49
    },
    {
      "lang": "en",
      "text": "since we're seeing people use\nClaude for these types of,",
      "offset": 480.99,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "you know, support, mental\nhealth adjacent conversations,",
      "offset": 486.12,
      "duration": 3.66
    },
    {
      "lang": "en",
      "text": "it's something that's\npushed us on safeguards",
      "offset": 489.78,
      "duration": 2.19
    },
    {
      "lang": "en",
      "text": "to really understand where can\nwe do better in this space.",
      "offset": 491.97,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "And so, for example,",
      "offset": 495.45,
      "duration": 1.86
    },
    {
      "lang": "en",
      "text": "one of the actions that\ncame out of this study is",
      "offset": 497.31,
      "duration": 3.57
    },
    {
      "lang": "en",
      "text": "we've partnered with ThroughLine.",
      "offset": 500.88,
      "duration": 1.47
    },
    {
      "lang": "en",
      "text": "And so we're working with\ntheir clinical experts",
      "offset": 502.35,
      "duration": 1.86
    },
    {
      "lang": "en",
      "text": "to understand when these\nconversations do happen,",
      "offset": 504.21,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "how can we respond better?",
      "offset": 507.69,
      "duration": 1.47
    },
    {
      "lang": "en",
      "text": "How can we train Claude to be safe",
      "offset": 509.16,
      "duration": 3.3
    },
    {
      "lang": "en",
      "text": "to provide appropriate\nreferrals from the ground up?",
      "offset": 512.46,
      "duration": 3.81
    },
    {
      "lang": "en",
      "text": "So when these conversations do arise,",
      "offset": 516.27,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "we're making sure that\nwe're acting responsibly.",
      "offset": 518.55,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "I think that's so important.",
      "offset": 521.19,
      "duration": 1.68
    },
    {
      "lang": "en",
      "text": "As we think about developing policies",
      "offset": 522.87,
      "duration": 2.34
    },
    {
      "lang": "en",
      "text": "and understanding how\nusers are engaging Claude,",
      "offset": 525.21,
      "duration": 2.97
    },
    {
      "lang": "en",
      "text": "bringing in these external experts",
      "offset": 528.18,
      "duration": 1.65
    },
    {
      "lang": "en",
      "text": "who can help us think through",
      "offset": 529.83,
      "duration": 1.53
    },
    {
      "lang": "en",
      "text": "the challenges that we're facing,",
      "offset": 531.36,
      "duration": 1.95
    },
    {
      "lang": "en",
      "text": "I think is a much better position",
      "offset": 533.31,
      "duration": 1.62
    },
    {
      "lang": "en",
      "text": "than trying to do it in a vacuum.",
      "offset": 534.93,
      "duration": 1.71
    },
    {
      "lang": "en",
      "text": "Yeah.",
      "offset": 536.64,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "Let's imagine that someone's\nusing Claude right now",
      "offset": 537.473,
      "duration": 2.497
    },
    {
      "lang": "en",
      "text": "for emotional support.",
      "offset": 539.97,
      "duration": 1.26
    },
    {
      "lang": "en",
      "text": "What things should they keep in mind?",
      "offset": 541.23,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Just remember to take stock and, you know,",
      "offset": 543.63,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "set some timers and reflect\non your own use of Claude.",
      "offset": 547.95,
      "duration": 3.93
    },
    {
      "lang": "en",
      "text": "And how is it making you feel?",
      "offset": 551.88,
      "duration": 2.79
    },
    {
      "lang": "en",
      "text": "How is it impacting the way that",
      "offset": 554.67,
      "duration": 1.8
    },
    {
      "lang": "en",
      "text": "you're interacting with your\nloved ones or with the world?",
      "offset": 556.47,
      "duration": 3.93
    },
    {
      "lang": "en",
      "text": "That's great advice.",
      "offset": 560.4,
      "duration": 1.32
    },
    {
      "lang": "en",
      "text": "Yeah, I think for better and for worse,",
      "offset": 561.72,
      "duration": 1.53
    },
    {
      "lang": "en",
      "text": "Claude only knows what you tell it.",
      "offset": 563.25,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "And so sometimes, you know,",
      "offset": 565.41,
      "duration": 1.62
    },
    {
      "lang": "en",
      "text": "it can be important to\ntake a step back and think,",
      "offset": 567.03,
      "duration": 2.527
    },
    {
      "lang": "en",
      "text": "&quot;You know, what are my blind spots?",
      "offset": 569.557,
      "duration": 2.123
    },
    {
      "lang": "en",
      "text": "And what haven't I\nthought of telling Claude",
      "offset": 571.68,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "that might be relevant\nin the sort of situation",
      "offset": 574.08,
      "duration": 2.58
    },
    {
      "lang": "en",
      "text": "that I'm working through?&quot;",
      "offset": 576.66,
      "duration": 1.47
    },
    {
      "lang": "en",
      "text": "And so your friends know\nso much more about you",
      "offset": 578.13,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "than Claude does, and so I\nthink it's really helpful",
      "offset": 580.89,
      "duration": 4.26
    },
    {
      "lang": "en",
      "text": "and important, as Ryn said,\nto compliment, you know,",
      "offset": 585.15,
      "duration": 2.43
    },
    {
      "lang": "en",
      "text": "the conversations you\nmight have with Claude",
      "offset": 587.58,
      "duration": 1.65
    },
    {
      "lang": "en",
      "text": "with a trusted friend.",
      "offset": 589.23,
      "duration": 1.74
    },
    {
      "lang": "en",
      "text": "I think that this is a\nreally emerging use case",
      "offset": 590.97,
      "duration": 3.39
    },
    {
      "lang": "en",
      "text": "that strikes to",
      "offset": 594.36,
      "duration": 1.35
    },
    {
      "lang": "en",
      "text": "the heart of how humans\njust interact socially.",
      "offset": 595.71,
      "duration": 3.27
    },
    {
      "lang": "en",
      "text": "And we have so little research out there",
      "offset": 598.98,
      "duration": 3.06
    },
    {
      "lang": "en",
      "text": "about how to build this safely.",
      "offset": 602.04,
      "duration": 3.78
    },
    {
      "lang": "en",
      "text": "What is the best way\nthat we can deploy this",
      "offset": 605.82,
      "duration": 3.33
    },
    {
      "lang": "en",
      "text": "to really help to support\nand benefit people?",
      "offset": 609.15,
      "duration": 3.18
    },
    {
      "lang": "en",
      "text": "I would just love to see more\npeople engage with this topic.",
      "offset": 612.33,
      "duration": 4.35
    },
    {
      "lang": "en",
      "text": "You know, civil society,",
      "offset": 616.68,
      "duration": 1.83
    },
    {
      "lang": "en",
      "text": "public-private research\npartnerships, et cetera,",
      "offset": 618.51,
      "duration": 2.31
    },
    {
      "lang": "en",
      "text": "as we really seek to understand\nwhat's happening here.",
      "offset": 620.82,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "Tell me what else we're\nthinking about studying",
      "offset": 623.76,
      "duration": 1.95
    },
    {
      "lang": "en",
      "text": "in the future.",
      "offset": 625.71,
      "duration": 1.05
    },
    {
      "lang": "en",
      "text": "One thing that we don't\nstudy in this research,",
      "offset": 626.76,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "but that I think is very\nimportant going forward,",
      "offset": 628.8,
      "duration": 1.89
    },
    {
      "lang": "en",
      "text": "is understanding whether\nClaude is behaving in a way",
      "offset": 630.69,
      "duration": 4.11
    },
    {
      "lang": "en",
      "text": "that we would consider sycophantic.",
      "offset": 634.8,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "While we do a lot of\npre-deployment testing",
      "offset": 636.78,
      "duration": 2.28
    },
    {
      "lang": "en",
      "text": "to understand and mitigate\nsycophancy in Claude,",
      "offset": 639.06,
      "duration": 3.39
    },
    {
      "lang": "en",
      "text": "I think it's really important that",
      "offset": 642.45,
      "duration": 1.17
    },
    {
      "lang": "en",
      "text": "we compliment that pre-deployment testing",
      "offset": 643.62,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "with post-deployment monitoring",
      "offset": 645.6,
      "duration": 1.38
    },
    {
      "lang": "en",
      "text": "and empirical research\nlike we do in this work.",
      "offset": 646.98,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Is it fair to say that we all think",
      "offset": 650.22,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "this is probably only the beginning?",
      "offset": 652.62,
      "duration": 1.98
    },
    {
      "lang": "en",
      "text": "That AI is going to become more ingrained",
      "offset": 654.6,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "in people's daily personal lives?",
      "offset": 657.72,
      "duration": 2.94
    },
    {
      "lang": "en",
      "text": "Do you both think that?",
      "offset": 660.66,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "I know I do.",
      "offset": 661.493,
      "duration": 0.833
    },
    {
      "lang": "en",
      "text": "- I agree. I do think this\nis only the beginning.",
      "offset": 663.42,
      "duration": 1.95
    },
    {
      "lang": "en",
      "text": "You paused.",
      "offset": 665.37,
      "duration": 1.445
    },
    {
      "lang": "en",
      "text": "No, I think it's pretty\nclear to me that, you know,",
      "offset": 668.16,
      "duration": 2.79
    },
    {
      "lang": "en",
      "text": "the way that we will relate\nto AI systems is going to be,",
      "offset": 670.95,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you know, evolving over time.",
      "offset": 674.67,
      "duration": 2.31
    },
    {
      "lang": "en",
      "text": "I think it's just really important that",
      "offset": 676.98,
      "duration": 1.14
    },
    {
      "lang": "en",
      "text": "we continue to do\nresearch that is empirical",
      "offset": 678.12,
      "duration": 1.89
    },
    {
      "lang": "en",
      "text": "and grounded in data,",
      "offset": 680.01,
      "duration": 1.32
    },
    {
      "lang": "en",
      "text": "and that we look at how our\nsystems are actually being used.",
      "offset": 681.33,
      "duration": 3.09
    },
    {
      "lang": "en",
      "text": "Thank you both for such an engaging",
      "offset": 684.42,
      "duration": 1.89
    },
    {
      "lang": "en",
      "text": "and fruitful conversation.",
      "offset": 686.31,
      "duration": 1.14
    },
    {
      "lang": "en",
      "text": "This was really great.",
      "offset": 687.45,
      "duration": 1.56
    },
    {
      "lang": "en",
      "text": "And if you are watching",
      "offset": 689.01,
      "duration": 1.74
    },
    {
      "lang": "en",
      "text": "and you haven't had an\nopportunity to read our blog,",
      "offset": 690.75,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "please check it out on anthropic.com.",
      "offset": 693.15,
      "duration": 2.73
    },
    {
      "lang": "en",
      "text": "And we are hiring so you can\ncheck out our career page",
      "offset": 695.88,
      "duration": 2.31
    },
    {
      "lang": "en",
      "text": "to learn more about some\nof those open roles.",
      "offset": 698.19,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "Really appreciate the time.",
      "offset": 700.71,
      "duration": 1.08
    },
    {
      "lang": "en",
      "text": "- Thank you.\n- Thanks!",
      "offset": 701.79,
      "duration": 1.15
    }
  ],
  "cleanText": "We've been seeing in headlines that people are turning to AI chatbots for emotional support, and we felt that we really needed to get ahead of this issue and study it ourselves.\n\"Emotional Impacts,\" take one.\nHi, I'm Alex!\nI lead our policy and enforcement work on the Safeguards team here at Anthropic.\nWe're the organization that tries to really understand user behavior and then build the appropriate mitigations to ensure that when users are engaging with Claude, they're doing so in a safe way.\nToday I'm really excited to have Ryn and Miles here today to talk about some of the work that we've been doing on how users are engaging with Claude for emotional support.\nI'm Miles, I'm a researcher on the Societal Impacts team.\nWe work on understanding how Claude is going to impact the world, and previously that's meant studying Claude's values, its economic impacts and bias.\nAnd now that also means studying Claude's emotional impacts.\nMy name is Ryn and I am a policy design manager for user wellbeing on the Safeguards team.\nI work on topics related to child safety, mental health, and how users are interacting with Claude.\nMy own background is in developmental and clinical psychology, and so emotional development is a topic that is very near and dear to my heart.\nMaybe we can start with personal anecdotes, and I guess maybe I can go first.\nOne of the most interesting and probably relevant use cases that I have in my own daily life is using Claude to help me better understand some behavioral changes with my kids.\nA good example of this is just the other day my preschool emailed me some feedback on my youngest son, and I think it's really easy to bring in emotion and bias when you get that type of feedback.\nBut running it through Claude, I think gave me some objective ways to approach the problem and hopefully has made me just a better parent overall.\nBut how are you guys using it?\nYou know, recently I was dealing with a difficult situation with a friend and I really wanted to share some feedback with them, but I wanted to phrase it right, and I found that Claude was really, really helpful in helping me think through how to deliver the feedback, where the feedback was coming from, what I was really trying to achieve with this friend, and helping me understand, you know, how they would receive it.\nWhat I tend to lean towards using Claude for is really helping me with content creation, with completing tasks.\nWhen I was planning my wedding, I was using it to help to understand, like, what kinds of timelines do I need to contact different vendors on?\nAnd I found that really helpful for me emotionally because it left me with more time for connecting with friends in real life and in person.\nFor the people that are using it for emotional support, why do we think folks are turning to Claude for this use case?\nHumans are such social creatures and love interactions and so I think that we're just seeing an outgrowth of the way that humans love to connect with each other.\nPeople don't always have an in-person support that they can turn to for some of these really tricky questions, and so having that kind of impartial private online forum can be a great way to practice what you wanna say when you're advocating for a raise or, you know, connecting with someone on a tough conversation.\nWe didn't build Claude to be an emotional support agent, but we're studying it nonetheless.\nWhy is it important to both of you to study this?\nWe did not design Claude to provide emotional support.\nClaude is primarily a work tool.\nBut, you know, we need to be clear-eyed about the ways that our systems are being used and study them.\nWe've been seeing in headlines that people are turning to AI chatbots for emotional support and we felt that we really needed to get ahead of this issue and study it ourselves.\nWhat about you, Ryn, from the safety perspective?\nI think the first question I always ask is, \"Well, how much is this actually happening?\"\nI really want to ground our safety mechanisms in the way that we build our products for the benefit of humanity in data.\nAnd now that we have some really great privacy preserving ways to look at this data, it seemed like a really worthwhile area to explore.\nWhy don't you guys, if you don't mind, walk us through some of the research.\nHow did you design this research?\nWe started with a sample of a few million Claude conversations from Claude.ai and we used our product, Claude, to scan these conversations and determine if they were related to the kinds of affective tasks that we were interested in.\nWhich were interpersonal advice, psychotherapy or counseling, coaching, sexual role play, romantic role play, and so on.\nAnd then we used Cleo, which is a tool that we developed here at Anthropic for privacy preserving analysis, to categorize and group those conversations into these bottom up clusters.\nAnd that's how we know that a lot of people are turning to Claude for career advice or thinking through difficult romantic relationship challenges and so on.\nParenting advice.\nParenting advice is a big one.\nSo for anyone who's not or hasn't read the blog post yet, what would you want the viewers to take away?\nWhat are some of the top key themes that you think would be important for them to walk away from watching this video?\nYeah, I think that one of the probably most surprising findings for me and a key theme is that we actually don't see this happen very much on Claude.ai.\nSo Claude.ai is for 18 plus.\nIt's not set up like an emotional support chatbot or anything like that, so it's not optimized for this.\nAnd so what we're seeing is about 2.9% of conversations in our sample of millions of conversations fall actually into this bucket of conversations on affective topics.\nAnd so there's this diversity, people are using it for this purpose, but it still isn't this overwhelming majority use case, for our platform at least.\nI was really surprised by that.\nBut I can't tell if that's because I use it so often in my personal life that I expected the numbers to be higher.\nMiles, was there anything surprising to you in the findings?\nYeah, I was inspired by just the sheer breadth of use cases.\nSo even among the three of us, there's a breadth of use cases.\nYeah!\nBut, you know, really looking at the clusters.\nWe saw, as you said, parenting advice.\nWe saw people working through difficult relationship challenges.\nWe saw people discussing the nature of AI consciousness and philosophy.\nAnd we also saw surprisingly little sexual and romantic role play.\nThat was one of our key questions coming into this research was whether people were sort of using Claude as an emotional companion or partner at large scale.\nI was surprised to see how rare that was.\nIt comprised less than a fraction of a percent of conversations.\nWe talk a lot about some of the beneficial use cases in this conversation in the blog, but I imagine from a safety perspective, and this is something I'm concerned about too, there must be concerns that you have, Ryn.\nCan you walk us through some of those?\nYeah, I think my biggest concern is if people are interacting with Claude as a way to avoid difficult conversations in person.\nIt really comes down to: How is the individual using it to lean in, ideally, rather than lean out from connection with people?\nI think it's important anytime you use a tool to really know where its strengths are and also where its limitations are.\nAnd so as somebody who works in the AI space, I think that's very top of mind for me when I'm interacting with Claude and asking for advice.\nBut I do worry that other folks who are maybe not in the AI circle are less aware of that, and some of Claude's limitations, as you mentioned.\nWe are not training Claude to be an emotional support agent.\nAnd so just recognizing what Claude is capable of doing and maybe what it's not, and when it's important to go and reach out to somebody who does have that deep expertise.\nAnd I think that this is something where since we're seeing people use Claude for these types of, you know, support, mental health adjacent conversations, it's something that's pushed us on safeguards to really understand where can we do better in this space.\nAnd so, for example, one of the actions that came out of this study is we've partnered with ThroughLine.\nAnd so we're working with their clinical experts to understand when these conversations do happen, how can we respond better?\nHow can we train Claude to be safe to provide appropriate referrals from the ground up?\nSo when these conversations do arise, we're making sure that we're acting responsibly.\nI think that's so important.\nAs we think about developing policies and understanding how users are engaging Claude, bringing in these external experts who can help us think through the challenges that we're facing, I think is a much better position than trying to do it in a vacuum.\nYeah.\nLet's imagine that someone's using Claude right now for emotional support.\nWhat things should they keep in mind?\nJust remember to take stock and, you know, set some timers and reflect on your own use of Claude.\nAnd how is it making you feel?\nHow is it impacting the way that you're interacting with your loved ones or with the world?\nThat's great advice.\nYeah, I think for better and for worse, Claude only knows what you tell it.\nAnd so sometimes, you know, it can be important to take a step back and think, \"You know, what are my blind spots?\nAnd what haven't I thought of telling Claude that might be relevant in the sort of situation that I'm working through?\"\nAnd so your friends know so much more about you than Claude does, and so I think it's really helpful and important, as Ryn said, to compliment, you know, the conversations you might have with Claude with a trusted friend.\nI think that this is a really emerging use case that strikes to the heart of how humans just interact socially.\nAnd we have so little research out there about how to build this safely.\nWhat is the best way that we can deploy this to really help to support and benefit people?\nI would just love to see more people engage with this topic.\nYou know, civil society, public-private research partnerships, et cetera, as we really seek to understand what's happening here.\nTell me what else we're thinking about studying in the future.\nOne thing that we don't study in this research, but that I think is very important going forward, is understanding whether Claude is behaving in a way that we would consider sycophantic.\nWhile we do a lot of pre-deployment testing to understand and mitigate sycophancy in Claude, I think it's really important that we compliment that pre-deployment testing with post-deployment monitoring and empirical research like we do in this work.\nIs it fair to say that we all think this is probably only the beginning?\nThat AI is going to become more ingrained in people's daily personal lives?\nDo you both think that?\nI know I do.\n- I agree.\nI do think this is only the beginning.\nYou paused.\nNo, I think it's pretty clear to me that, you know, the way that we will relate to AI systems is going to be, you know, evolving over time.\nI think it's just really important that we continue to do research that is empirical and grounded in data, and that we look at how our systems are actually being used.\nThank you both for such an engaging and fruitful conversation.\nThis was really great.\nAnd if you are watching and you haven't had an opportunity to read our blog, please check it out on anthropic.com.\nAnd we are hiring so you can check out our career page to learn more about some of those open roles.\nReally appreciate the time.\n- Thank you.\n- Thanks!\n",
  "dumpedAt": "2025-07-21T18:43:26.002Z"
}