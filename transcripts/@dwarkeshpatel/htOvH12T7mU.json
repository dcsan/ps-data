{
  "episodeId": "htOvH12T7mU",
  "channelSlug": "@dwarkeshpatel",
  "title": "2027 Intelligence Explosion: Month-by-Month Model — Scott Alexander & Daniel Kokotajlo",
  "publishedAt": "2025-04-03T16:01:10.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "Today I have the great pleasure of chatting with \nScott Alexander and Daniel Kokotajlo. Scott is  ",
      "offset": 50.04,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of course the author of the blog Slate Star \nCodex, Astral Codex 10 now. It’s actually been,  ",
      "offset": 57.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "as you know, a big bucket list item of mine to \nget you on the podcast. So this is all the first  ",
      "offset": 63.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "podcast we’ve ever done, right?\nYes. ",
      "offset": 67.52,
      "duration": 1.84
    },
    {
      "lang": "en",
      "text": "And then Daniel is the director of the AI \nFutures Project. And you have both just launched  ",
      "offset": 69.36,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "today something called AI 2027. So what is this?\nYeah, AI 2027 is our scenario trying to forecast  ",
      "offset": 75.64,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "the next few years of AI progress. We’re trying \nto do two things here. First of all we just want  ",
      "offset": 84.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "to have a concrete scenario at all. So you have \nall these people, Sam Altman, Dario Amodei, Elon  ",
      "offset": 89.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Musk saying, “going to have AGI in three years, \nsuperintelligence in five years”. And people just  ",
      "offset": 95.56,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "think that’s crazy because right now we have \nchatbots that are able to do a Google search,  ",
      "offset": 101.52,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "not much more than that in a lot of ways. And \nso people ask, “how is it going to be AGI in  ",
      "offset": 107.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "three years?” What we wanted to do is provide \na story, provide the transitional fossils. So  ",
      "offset": 112.24,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "start right now, go up to 2027 when there’s AGI, \n2028, when there’s potentially super intelligence,  ",
      "offset": 118.92,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "show on a month-by-month level what happened. Kind \nof in fiction writing terms, make it feel earned. ",
      "offset": 126.16,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "So that’s the easy part. The hard part is \nwe also want to be right. So we’re trying  ",
      "offset": 132.72,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "to forecast how things are going to go, what speed \nthey’re going to go at. We know that in general,  ",
      "offset": 138.2,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "the median outcome for a forecast like this \nis being totally humiliated when everything  ",
      "offset": 146.48,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "goes completely differently. And if you read \nour scenario, you’re definitely not going to  ",
      "offset": 150.48,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "expect us to be the exception to that trend.\nThe thing that gives me optimism is Daniel  ",
      "offset": 155.32,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "back in 2021, wrote the prequel to this scenario \ncalled What 2026 Looks Like. It’s his forecast  ",
      "offset": 161.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "for the next five years of AI progress. And \nhe got it almost exactly right. You should  ",
      "offset": 168.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "stop this podcast right now. You should go and \nread this document. It’s amazing. Kind of looks  ",
      "offset": 173.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like you asked ChatGPT to summarize the \npast five years of AI progress, and you  ",
      "offset": 179.96,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "got something with a couple of hallucinations, \nbut basically well intentioned and correct. So  ",
      "offset": 184.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "when Daniel said he was doing this sequel, I was \nvery excited, really wanted to see where it was  ",
      "offset": 189.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "going. It goes to some pretty crazy places \nand I’m excited to talk about it more today. ",
      "offset": 196.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "I think you’re hyping up a little bit too much. \nYes, I do recommend people go read the old thing  ",
      "offset": 201.4,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "I did, which was a blog post. I think it got a \nbunch of stuff right, a bunch of stuff wrong,  ",
      "offset": 205.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "but overall held up pretty well and inspired \nme to try again and do a better version of it. ",
      "offset": 210.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think, read the document and \ndecide which of us is right. ",
      "offset": 214.76,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "Another related thing too is that the original \nthing was not supposed to end in 2026, it was  ",
      "offset": 218.12,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "supposed to go all the way through the exciting \nstuff, right? Because everyone’s talking about,  ",
      "offset": 224.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "what about AGI, what about superintelligence, \nwhat would that even look like? So I was trying to  ",
      "offset": 228.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "step-by-step work my way from where we were at the \ntime until things happen and then see what they  ",
      "offset": 233.16,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "look like, but I basically chickened out when I \ngot to 2027 because things were starting to happen  ",
      "offset": 239,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "and the automation loop was starting to take off \nand it was just so confusing and there was so much  ",
      "offset": 244.68,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uncertainty, so I basically just deleted the last \nchapter and published what I had up until that  ",
      "offset": 248.92,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "point. And that was the blog post.\nOkay, and then, Scott,  ",
      "offset": 255.68,
      "duration": 2
    },
    {
      "lang": "en",
      "text": "how did you get involved in this project?\nSo I was asked to help with the writing, and I was  ",
      "offset": 257.68,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "already somewhat familiar with the people on the \nproject, and many of them were kind of my heroes.  ",
      "offset": 264.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "So, Daniel, I knew both because I’d written a blog \npost about his opinions before I knew about his,  ",
      "offset": 270,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "“What 2026 looks like,” which was amazing. And \nalso he had pretty recently made the national  ",
      "offset": 275.2,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "news for having, when he quit OpenAI, they told \nhim he had to sign a non-disparagement agreement  ",
      "offset": 280.92,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "or they would claw back his stock options. And \nhe refused, which they weren’t prepared for. It  ",
      "offset": 288.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "started a major news story, a scandal that ended \nup with OpenAI agreeing that they were no longer  ",
      "offset": 294.92,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "going to subject employees to that restriction.\nSo people talk a lot about how it’s hard to trust  ",
      "offset": 302.68,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "anyone in AI because they all have so much \nmoney invested in the hype and getting their  ",
      "offset": 310.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "stock options better. And Daniel had attempted \nto sacrifice millions of dollars in order to say  ",
      "offset": 315.96,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "what he believed, which to me was this incredibly \nstrong sign of honesty and competence. And I was  ",
      "offset": 324.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like, how can I say no to this person? Everyone \nelse on the team, also extremely impressive.  ",
      "offset": 330.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "Eli Liflund, who’s a member of Samotsvety, the \nworld’s top forecasting team. He has won, like,  ",
      "offset": 335.52,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "the top forecasting competition, plausibly \ndescribed as just the best forecaster in the  ",
      "offset": 341.12,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "world, at least by these really technical measures \nthat people use in the superforecasting community.  ",
      "offset": 346.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Thomas Larsen, Jonas Vollmer, both really amazing \npeople who have done great work in AI before. ",
      "offset": 352.04,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "I was really excited to get to work with this \nsuperstar team. I have always wanted to get  ",
      "offset": 359.48,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "more involved in the actual attempt to make \nAI go well. Right now, I just write about it.  ",
      "offset": 365.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "I think writing about it is important, but I \ndon’t know. You always regret that you’re not  ",
      "offset": 372.64,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the person who’s the technical alignment genius \nwho’s able to solve everything. And getting to  ",
      "offset": 377.72,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "work with people like these and potentially make \na difference just seemed like a great opportunity. ",
      "offset": 382.8,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "What I didn’t realize was that I also learned a \nhuge amount. I try to read most of what’s going  ",
      "offset": 388.32,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "on in the world of AI, but it’s this very \nlow bandwidth thing and getting to talk to  ",
      "offset": 395.32,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "somebody who’s thought about it as much as \nanyone in the world was just amazing. Makes  ",
      "offset": 400.92,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "me really understand these things about how \nAI is going to learn quickly. You need all of  ",
      "offset": 407.4,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "this deep engagement with the underlying \nterritory and I feel like I got that. ",
      "offset": 412.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I’ve probably changed my mind towards, against, \ntowards, against, intelligence explosion three,  ",
      "offset": 417.28,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "four times in the conversations I’ve had \nin the lead-up in talking to you and then  ",
      "offset": 422.96,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "trying to come up with a rebuttal or something.\nIt wasn’t even just changing my mind, getting to  ",
      "offset": 427.44,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "read the scenario for the first time. It obviously \nwasn’t written up at this point. It was a giant,  ",
      "offset": 432.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "giant spreadsheet. I’ve been thinking about \nthis for a decade, decade and a half now. And  ",
      "offset": 437.76,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "it just made it so much more concrete to have \na specific story. Like, oh, yeah, that’s why  ",
      "offset": 446.36,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "we’re so worried about the arms race with China. \nObviously we would get an arms race with China  ",
      "offset": 450.84,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "in that situation. And aside from just the people \ngetting to read the scenario really sold me. This  ",
      "offset": 455.52,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "is something that needs to get out there more.\nYeah. Okay. Now let’s talk about this new  ",
      "offset": 461.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "forecast. Because you do a month by month \nanalysis of what’s going to happen from here.  ",
      "offset": 466.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "So what is it that you expect in mid-2025 \nand the end of 2025 In this forecast? ",
      "offset": 471.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "So, [the] beginning of the forecast mostly focuses \non agents. We think they’re going to start with  ",
      "offset": 477.52,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "agency training, expand the time horizons, get \ncoding going well. Our theory is that they are,  ",
      "offset": 485.44,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "to some degree consciously, to some degree \naccidentally, working towards this intelligence  ",
      "offset": 492.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "explosion, where the AIs themselves can start \ntaking over some of the AI research, move faster. ",
      "offset": 497.08,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "So 2025, slightly better coding, 2026, \nslightly better agents, slightly better coding.  ",
      "offset": 503.68,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "And then we focus on, and we name the scenario \nafter 2027 because that is when this starts to  ",
      "offset": 510.48,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "pay off. The intelligence explosion gets into \nfull swing; the agents become good enough to  ",
      "offset": 516.88,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "help with- at the beginning not really do, \nbut help with- some of the AI research. ",
      "offset": 523.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So we introduced this idea called the R&amp;D \nprogress multiplier: how many months of  ",
      "offset": 528.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "progress without the AIs do you get in one month \nof progress with all of these new AIs helping  ",
      "offset": 533.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "with the intelligence explosion. So 2027, we \nstart with- I can’t remember if it literally  ",
      "offset": 539.64,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "starts with, or by March or something- a five \ntimes multiplier for algorithmic progress. ",
      "offset": 545.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "So we have the stats tracked on the site of the \nstory. Part of why we did it as a website is so  ",
      "offset": 551.44,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that you can have these cool gadgets and widgets. \nAnd so as you read the story, the stats on the  ",
      "offset": 556.44,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "side automatically update. And so one of those \nstats is the progress multiplier. Another answer  ",
      "offset": 561.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "to the same question you asked is basically; \n2025, nothing super interesting happens,  ",
      "offset": 566.4,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "more or less similar trends to what we’re seeing.\nComputer use is totally solved? Partially solved?  ",
      "offset": 572.16,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "How good is computer use by the end of 2025?\nMy guess is that they won’t be making basic  ",
      "offset": 577.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "mouse click errors by the end of 2025, like \nthey sometimes currently do. If you watch  ",
      "offset": 581.28,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "Claude Plays Pokemon- which you totally should- \nit seems like sometimes it’s just failing to  ",
      "offset": 585.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "parse what’s on the screen and it thinks that \nits own player character is an NPC and gets  ",
      "offset": 590.44,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "confused. My guess is that that sort of thing \nwill mostly be gone by the end of this year,  ",
      "offset": 596.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "but that they still won’t be able to autonomously \noperate for long periods on their own. ",
      "offset": 602.2,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "But by 2025, when you say it won’t be able \nto act coherently for long periods of time  ",
      "offset": 609.24,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "in computer use, if I want to organize \na happy hour in my office, I don’t know,  ",
      "offset": 613.6,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "that’s like what a 30 minute task? What fraction \nof that is, it’s got to invite the right people,  ",
      "offset": 617.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it’s got to book the right doordash or something. \nWhat fraction of that is it able to do? ",
      "offset": 622.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "My guess is that by the end of this year \nthere’ll be something that can kind of do that,  ",
      "offset": 627,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "but unreliably. And that if you actually tried \nto use that to run your life, it would make some  ",
      "offset": 631.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "hilarious mistakes that would appear on Twitter \nand go viral, but that the MVP of it will probably  ",
      "offset": 636.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "exist by this year. Like there’ll be some Twitter \nthread about someone being like, “I plugged in  ",
      "offset": 642.16,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "this agent to like run my party and it worked!”\nOur scenario focuses on coding in particular  ",
      "offset": 645.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "because we think coding is what starts the \nintelligence explosion. So we are less interested  ",
      "offset": 651.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "in questions of like, “how do you mop up the last \nfew things that are uniquely human” compared to  ",
      "offset": 657.16,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "“when can you start coding in a way that helps the \nhuman AI researchers speed up their AI research,  ",
      "offset": 663.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "and then, if you’ve helped them speed up \nthe AI research enough, is that enough to,  ",
      "offset": 668.24,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "with some ridiculous speed multiplier- 10 times, \n100 times- mop up all of these other things?” ",
      "offset": 672.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "One observation I have is, you could have told \na story in 2021, once ChatGPT comes out… I think  ",
      "offset": 678.4,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "I had friends who were credible AI thinkers who \nwere like, “look, you’ve got the coding agent now,  ",
      "offset": 685.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "it’s been cracked. Now the GPT4 will go around and \nit’ll do all this engineering and we do this RL on  ",
      "offset": 692.52,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "top. We can totally scale up the system 100x” \nand every single layer of this has been much  ",
      "offset": 697.88,
      "duration": 6.68
    },
    {
      "lang": "en",
      "text": "harder than the strongest optimist expected. \nIt seems like there have been significant  ",
      "offset": 704.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "difficulties in increasing the pre-training size, \nat least from rumors about field training runs or  ",
      "offset": 709.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "underwhelming training runs at labs.\nIt seems like building up these  ",
      "offset": 714.96,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "RL- total outside view, I know nothing about the \nactual engineering involved here- but just from an  ",
      "offset": 720.08,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "outside view it seems like building up the O1 \nRL clearly took at least two years after GPT4  ",
      "offset": 723.52,
      "duration": 8.76
    },
    {
      "lang": "en",
      "text": "was released. And these things are also, their \neconomic impact and the kinds of things you would  ",
      "offset": 732.28,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "immediately expect based on benchmarks for them \nto be especially capable at isn’t overwhelming,  ",
      "offset": 737.24,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "like the call center workers haven’t been fired \nyet. So why not just say look, at higher scale  ",
      "offset": 742.24,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "it will probably get even more difficult.\nWait a second, I’m a little confused to hear  ",
      "offset": 751.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "you say that, because when I have seen people \npredicting AI milestones like Katja Grace’s  ",
      "offset": 756.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "expert surveys, they have almost always been \ntoo pessimistic from a point of view of how  ",
      "offset": 761.76,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "fast AI will advance. So I think the 2022 survey, \nthey actually said that things that had already  ",
      "offset": 767.24,
      "duration": 9.84
    },
    {
      "lang": "en",
      "text": "happened would take like 10 years to happen, \nbut then the survey- it might have been 2023,  ",
      "offset": 777.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it was like six months before GPT3, GPT4, came \nout. And there were things that GPT3 or 4 or  ",
      "offset": 782.52,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "whichever one of them it was, did, that \nit did in six months that they were still  ",
      "offset": 789.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "predicting like five or ten years from. I’m sure \nDaniel is going to have a more detailed answer,  ",
      "offset": 793.64,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "but I absolutely reject the premise that \neverybody has always been too optimistic. ",
      "offset": 798.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Yeah, I think in general, most people following \nthe field have underestimated the pace of AI  ",
      "offset": 803.2,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "progress and underestimated the pace of \nAI diffusion into the world. For example,  ",
      "offset": 808.88,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "Robin Hanson famously made a bet about \nless than a billion dollars of revenue  ",
      "offset": 812.4,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "I think by 2025 from AI.\nI agree Robin Hanson in  ",
      "offset": 815.76,
      "duration": 2.68
    },
    {
      "lang": "en",
      "text": "particular has been too pessimistic.\nBut he’s a smart guy. So I think that the  ",
      "offset": 818.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "aggregate opinion has been underestimating the \npace of both technical progress and deployment.  ",
      "offset": 822.52,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "I agree that there have been plenty of people \nwho’ve been more bullish than me and have been  ",
      "offset": 829,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "already proven wrong, but they’re not me.\nWait a second. We don’t have to guess about  ",
      "offset": 831.92,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "aggregate opinion, we can look at Metaculus. \nMetaculus, I think their timeline was like 2050  ",
      "offset": 837.68,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "back in 2020. It gradually went down to like \n2040 two or three years ago. Now it’s at 2030,  ",
      "offset": 844.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "so it’s barely ahead of us. Again, that may \nturn out to be wrong, but it does look like  ",
      "offset": 850.24,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "the Metaculans overall have, have been too \npessimistic, thinking too long term rather  ",
      "offset": 855.2,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "than too optimistic. And I think that’s like the \nclosest thing we have to a neutral aggregator  ",
      "offset": 860.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "where we’re not cherry picking things.\nYeah. I had this interesting experience  ",
      "offset": 864.84,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "yesterday. We were having lunch \nwith this senior AI researcher,  ",
      "offset": 867.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "probably makes on the order of millions a \nmonth or something, and we were asking him,  ",
      "offset": 872.56,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "“how much are the AIs helping you?” And he said, \n“in domains which I understand well, and it’s  ",
      "offset": 878.52,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "closer to autocomplete but more intense, there \nit’s maybe saving me four to eight hours a week.” ",
      "offset": 883.2,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "But then he says, “in domains which I’m less \nfamiliar with, if I need to go wrangle up some  ",
      "offset": 890.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "hardware library or make some modification \nto the kernel or whatever, where I know less,  ",
      "offset": 895.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "that saves me on the order of 24 hours \na week.” Now, with current models. What  ",
      "offset": 901.72,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "I found really surprising is that the help \nis bigger where it’s less like autocomplete  ",
      "offset": 908.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and more like a novel contribution. It’s like a \nmore significant productivity improvement there. ",
      "offset": 913.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah, that is interesting. I imagine what’s going \non there is that a lot of the process when you’re  ",
      "offset": 917.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "unfamiliar with a domain is like Googling around \nand learning more about the domain. And language  ",
      "offset": 922.8,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "models are excellent because they’ve already \nread the whole Internet and know all the details. ",
      "offset": 926.36,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Isn’t this a good opportunity to discuss a certain \nquestion I asked Dario that you responded to? ",
      "offset": 930.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "What are you thinking of?\nWell, I asked this question where, as you say,  ",
      "offset": 936.56,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "they know all this stuff. I don’t know if you \nsaw this. I asked this question where I said,  ",
      "offset": 939.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "look, these models know all this stuff. And if \na human knew every single thing a human has ever  ",
      "offset": 944.72,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "written down on the Internet, they’d be able to \nmake all these interesting connections between  ",
      "offset": 951.12,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "different ideas and maybe even find medical \ncures or scientific discoveries as a result. ",
      "offset": 955.32,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "There was some guy who noticed that magnesium \ndeficiency causes something in the brain that  ",
      "offset": 960.84,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "is similar to what happens when you \nget a migraine. And so he just said:  ",
      "offset": 965.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "give you magnesium supplements that cured \na lot of migraines. So why aren’t able to  ",
      "offset": 968.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "leverage this enormous asymmetric advantage they \nhave to make a single new discovery like this? ",
      "offset": 973.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "And then the example I gave was that humans \nalso can’t do this. So for me, the most salient  ",
      "offset": 978.88,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "example is the etymology of words. You have all \nof these words in English that are very similar,  ",
      "offset": 985.72,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "like ‘happy’ versus ‘hapless’, ‘happen’, \n‘perhaps’. And we never think about them  ",
      "offset": 991.12,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "unless you read an etymology dictionary and \nthey’re like, oh, obviously these all come  ",
      "offset": 996,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "from some old root that has to mean ‘luck’ \nor ‘occurrence’ or something like that. ",
      "offset": 999.2,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "So it’s kind of about figuring out versus \nchecking. If I tell you those, you’re like,  ",
      "offset": 1003.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "“this seems plausible”. And of course, in \netymology, there are also a lot of false friends  ",
      "offset": 1009.88,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "where they seem plausible but aren’t connected. \nBut you really do have to have somebody shove  ",
      "offset": 1013.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "it in your face before you start thinking \nabout it and make all of those connections. ",
      "offset": 1018.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "I will actually disagree with this. \nWe know that humans can do like,  ",
      "offset": 1021.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "we have examples of humans doing this. I agree \nthat we don’t have logical omniscience because  ",
      "offset": 1025.92,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there is a combinatorial explosion, but we are \nable to leverage our intelligence to… one of my  ",
      "offset": 1030.24,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "favorite examples of this is David Anthony, the \nguy who wrote the Horse, the Wheel and Language. ",
      "offset": 1036.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "He made this super impressive discovery \nbefore we had the genetic evidence for it,  ",
      "offset": 1041.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "like a decade before, where he said, look, if I \nlook at all these languages in India and Europe,  ",
      "offset": 1046.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "they all share the same etymology. I mean \nliterally the same etymology for words like  ",
      "offset": 1053,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "‘wheel’ and ‘cart’ and ‘horse’. And these \nare technologies that have only been around  ",
      "offset": 1059.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "for the last 6,000 years, which must mean that \nthere was some group that these groups are all,  ",
      "offset": 1063.52,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "at least linguistically, descended from. And \nnow we have genetic evidence for the Yamnaya,  ",
      "offset": 1070.84,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "which we believe is this group. You have a \nblog where you do this. This is your job,  ",
      "offset": 1074.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Scott! So why shouldn’t we hold the fact that \nlanguage models can’t do this more against them? ",
      "offset": 1079.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Yeah. So to me, it doesn’t seem like he is \njust sitting there being logically omniscient  ",
      "offset": 1085.48,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "and getting the answer. It seems like he’s \na genius, he’s thought about this for years,  ",
      "offset": 1090.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "probably at some point, he heard a couple of \nIndian words and a couple of European words at  ",
      "offset": 1095.84,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the same time and they kind of connected and \nthe light bulb came on. So this isn’t about  ",
      "offset": 1100.48,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "having all the information in your memory \nso much as the normal process of discovery,  ",
      "offset": 1105.8,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "which is kind of mysterious, but seems to come \nfrom having good heuristics and throwing them at  ",
      "offset": 1110.28,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "things until you kind of get a lucky strike.\nMy guess is if we had really good AI agents  ",
      "offset": 1116.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and we applied them to this task, it would look \nsomething like a scaffold where it’s like, think  ",
      "offset": 1121.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "of every combination of words that you know of, \ncompare them. If they sound very similar, write  ",
      "offset": 1126.48,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "it on this scratch pad here. If a lot of words of \nthe same type show up on the scratch pad, that’s  ",
      "offset": 1131.56,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "pretty strange, do some kind of thinking around \nit. And I just don’t think we’ve even tried that. ",
      "offset": 1137.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "And I think right now if we tried it, we would \nrun into the combinatorial explosion. We would  ",
      "offset": 1143.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "need better heuristics. Humans have such \ngood heuristics that probably most of the  ",
      "offset": 1147.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "things that show up even in our conscious mind, \nrather than happening on the level of some kind  ",
      "offset": 1152.6,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "of unconscious processing, are at least the kind \nof things that could be true. I think you could  ",
      "offset": 1156.72,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "think of this as like a chess engine. You have \nsome unbelievable number of possible next moves,  ",
      "offset": 1162.08,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "you have some heuristics for picking out \nwhich of those are going to be the right  ",
      "offset": 1167.72,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "ones. And then gradually you kind of have the \nchess engine think about it, go through it,  ",
      "offset": 1171.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "come up with a better or worse move, then at some \npoint you potentially become better than humans.  ",
      "offset": 1176.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "I think if you were to force the AI to do this \nin a reasonable way, or you were to train the AI  ",
      "offset": 1180.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "such that it itself could come up with the plan of \ngoing through this in some kind of heuristic-laden  ",
      "offset": 1185.48,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "way, you could potentially equal humans.\nI’ll add some more things to that. So I  ",
      "offset": 1190.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "think there’s a long and sordid history of \npeople looking at some limitation of the  ",
      "offset": 1194.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "current LLMs and then making grand claims about \nhow the whole paradigm is doomed because they’ll  ",
      "offset": 1200.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "never overcome this limitation. And then a year or \ntwo later the new LLMs overcome that limitation. ",
      "offset": 1205.04,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "And I would say that with respect to this thing \nof “why haven’t they made these interesting  ",
      "offset": 1212.24,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "scientific discoveries by combining the knowledge \nthey already have and noticing interesting  ",
      "offset": 1217.16,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "connections?” I would say first of all, have we \nseriously tried to build scaffolding to make them  ",
      "offset": 1220.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "do this? And I think the answer is mostly no.\nI think Google DeepMind tried this. ",
      "offset": 1225.44,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "Maybe. Second thing, have you tried making the \nmodel bigger? They’ve made it a bit bigger over  ",
      "offset": 1229,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "the last couple years and it hasn’t worked so \nfar. Maybe if they make it even bigger still,  ",
      "offset": 1235.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it’ll notice more of these connections. And then \nthird thing, and here’s I think the special one:  ",
      "offset": 1239.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "Have you tried training the model to do the \nthing? The pre-training process doesn’t strongly  ",
      "offset": 1245.04,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "incentivize this type of connection making.\nIn general I think it’s a helpful heuristic that  ",
      "offset": 1253.08,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "I use to ask the question of: remind oneself, what \nwas the AI trained to do? What was its training  ",
      "offset": 1259.36,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "environment like? And if you’re wondering \nwhy hasn’t the AI done this, ask yourself,  ",
      "offset": 1264.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "did the training environment train it to do \nthis? And often the answer is no. And often I  ",
      "offset": 1269.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "think that’s a good explanation for why the AI is \nnot good at it is that it wasn’t trained to do it. ",
      "offset": 1273.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I mean it seems like such \nan economically valuable… ",
      "offset": 1278.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "But how would you set up the training \nenvironment? Wouldn’t it be really  ",
      "offset": 1281.96,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "gnarly to try to set up an RL environment \nto train to make new scientific discoveries? ",
      "offset": 1285,
      "duration": 4.554
    },
    {
      "lang": "en",
      "text": "Maybe that’s why you should have longer \ntimelines. It’s a gnarly engineering problem. ",
      "offset": 1289.554,
      "duration": 3.886
    },
    {
      "lang": "en",
      "text": "Well in our scenario they don’t just leap from \nwhere we are now to solving this problem. They  ",
      "offset": 1293.44,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "don’t. Instead they just iteratively improve \nthe coding agents until they’ve basically  ",
      "offset": 1298.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "got coding solved. But even still, their \ncoding agents are not able to do some of  ",
      "offset": 1303.48,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "this stuff. That’s what early 2020, like the \nfirst half of 2027 in our story is basically,  ",
      "offset": 1309.24,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "they’ve got these awesome automated coders, \nbut they still lack research taste and they  ",
      "offset": 1314.92,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "still lack maybe organizational skills and stuff.\nAnd so they need to overcome those remaining  ",
      "offset": 1318.32,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "bottlenecks and gaps in order to completely \nautomate the AI research cycle. But they’re  ",
      "offset": 1323.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "able to overcome those gaps faster than they \nnormally would because the coding agents are  ",
      "offset": 1327.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "doing all the grunt work really fast for them.\nYeah, I think it might be useful to think of  ",
      "offset": 1332.2,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "our timelines as being like 2070, 2100. It’s \njust that the last 50 to 70 years of that all  ",
      "offset": 1335.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "happened during the year 2027 to 2028, because \nwe are going through this intelligence explosion  ",
      "offset": 1342.08,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "like I think if I asked you, could we solve this \nproblem by the year 2100? You would say, oh, yeah,  ",
      "offset": 1347.48,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "by 2100? Absolutely. And we’re just saying that \nthe year 2100 might happen earlier than you expect  ",
      "offset": 1351.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "because we have this research progress multiplier.\nAnd then let me just address that in a second.  ",
      "offset": 1357.4,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "But just one final thought on this thread. To \nthe extent that there’s like a modus ponens,  ",
      "offset": 1362.52,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "modus tollens thing here, where one thing you \ncould say is like, look: AIs- not just LLMs,  ",
      "offset": 1368.44,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "but AIs- will have this fundamental asymmetric \nadvantage where they know all this shit. And  ",
      "offset": 1374.16,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "why aren’t they able to use their general \nintelligence to use this asymmetric advantage  ",
      "offset": 1378.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "to some enormous capability overhang.\nNow, you could infer that same statement  ",
      "offset": 1384.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "by saying, okay, well, once they do have that \ngeneral intelligence, they will be able to use  ",
      "offset": 1389.56,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "their asymmetric advantage to make all these \nenormous gains that humans are in principle  ",
      "offset": 1393.48,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "less capable of, right? So basically, if you \ndo subscribe to this view that AIs could do  ",
      "offset": 1397.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "all these things if only they had general \nintelligence, you got to be like, well,  ",
      "offset": 1403.04,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "once we actually do get the AGI, it’s actually \ngoing to be a totally transformative because they  ",
      "offset": 1406.16,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "will have all of human knowledge memorized and \nthey can use that to make all these connections. ",
      "offset": 1410.04,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "I’m glad you mentioned that our current scenario \ndoes not really take that into account very much.  ",
      "offset": 1413.28,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "So that’s an example in which our scenario is \npossibly underestimating the rate of progress. ",
      "offset": 1416.72,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "You’re so conservative, Daniel.\nThis has been my experience working with the team,  ",
      "offset": 1423.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "as I point out, five different things. “Are \nyou sure you’re taking this into account? Are  ",
      "offset": 1428.08,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "you sure you’re taking this into account?” \nAnd first of all, 99% of the time he says,  ",
      "offset": 1431.56,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "“yes, we have a supplement on it”. But even \nwhen he doesn’t say that, he’s like, “yeah,  ",
      "offset": 1434.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "that’s one reason it could go slower than \nthat. Here are 10 reasons it could go faster”. ",
      "offset": 1438.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "It’s trying to be sort of like our median \nguess. So there are a bunch of ways in  ",
      "offset": 1443.6,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "which we could be underestimating, and there \nare a bunch of ways in which you could be  ",
      "offset": 1448.92,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "overestimating. And we’re going to hopefully \ncontinue to think more about this afterwards  ",
      "offset": 1451.68,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "and continue to iteratively refine our models \nand come up with better guesses and so forth. ",
      "offset": 1457.56,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "So if I look back at AI progress in the \npast, if we were back in, say, 2017.  ",
      "offset": 1521.96,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "Suppose we had these superhuman coders in 2017; \nthe amount of progress we’ve made since then,  ",
      "offset": 1530.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "so where we are currently in 2025, by \nwhen could we have had that instead? ",
      "offset": 1534.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Great question. We’d still have to stumble \nthrough all the discoveries that we’ve made  ",
      "offset": 1539.48,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "since 2017. We still have to figure out that \nlanguage models are a thing, we still have to  ",
      "offset": 1543,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "figure out that you can fine tune them with RL.\nSo all those things would still have to happen.  ",
      "offset": 1547.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "How much faster would they happen? Maybe \n5x faster, because a lot of the small scale  ",
      "offset": 1552.04,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "experiments that these people do in order to \ntest out ideas really quickly before they do  ",
      "offset": 1558.44,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "the big training runs would happen much \nfaster because they’re just lickety-split  ",
      "offset": 1561.76,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "being spit out. I’m not very confident in that \n5x number, it could be lower, it could be higher,  ",
      "offset": 1565.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "but that was roughly what we were guessing.\nOur 5x, by the way, is for the algorithmic  ",
      "offset": 1570.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "progress part, not for the overall thing. \nSo in this hypothetical, according to me,  ",
      "offset": 1574.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "basically things would be going 2.5x faster, where \nthe algorithms would be advancing at 5x speed,  ",
      "offset": 1579.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "but the compute is still stuck at the usual speed.\nThat seems plausible to me. You have a 5x at some  ",
      "offset": 1583.88,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "point, and then dot dot dot, you have 1000x \nAI progress within the matter of a year. Maybe  ",
      "offset": 1589.36,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "that’s the part I’m like, wait, how did that \nhappen exactly? So what’s the story there? ",
      "offset": 1596.92,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "The way that we did our takeoff forecast, \nwhich we’ll get to in a second, was  ",
      "offset": 1600.36,
      "duration": 1.64
    },
    {
      "lang": "en",
      "text": "basically by breaking down how we think the \nintelligence explosion would go into a series  ",
      "offset": 1602,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "of milestones. First you automate the coding, \nthen you automate the whole research process,  ",
      "offset": 1605.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "but in a very similar way to how humans do it \nwith teams of agents that are about human level,  ",
      "offset": 1610.16,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "then you get to superhuman level and so forth.\nSo we broke it down into these milestones,  ",
      "offset": 1616.08,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "you know, the superhuman coder, superhuman \nAI researcher, and then super intelligent AI  ",
      "offset": 1619.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "researcher. And the way we did our forecast was, \nfor each of these milestones, we were like, what  ",
      "offset": 1623.72,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "is it going to take to make an AI that achieves \nthat milestone? And then once you do achieve that  ",
      "offset": 1630.04,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "milestone, how much is your overall speedup? And \nthen what’s it going to take to achieve the next  ",
      "offset": 1635.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "milestone? Combine that with the overall speed up \nand that gets you your clock time distance until  ",
      "offset": 1640.24,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "that happens and then, okay, now you’re at that \nmilestone. What’s your overall speed up? Assuming  ",
      "offset": 1645.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that you have that milestone also, what’s the next \none? How long does it take to get to the next one?  ",
      "offset": 1649.56,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "So we sort of work through it bit by bit, and at \neach stage we’re just making our best guesses. ",
      "offset": 1653.64,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "So quantitatively we were thinking something \nlike 5x speedup to algorithmic progress from  ",
      "offset": 1658.32,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the superhuman coder, and then something like \na 25x speedup to algorithmic progress from the  ",
      "offset": 1663.72,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "superhuman AI researcher. Because at that \npoint you’ve got the whole stack automated,  ",
      "offset": 1669.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "which I think is substantially more useful \nthan just automating the coding. And then  ",
      "offset": 1673.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "I forget what we say for a super intelligent \nAI researcher, but off the top of my head  ",
      "offset": 1682.04,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it’s probably something in the hundreds \nor maybe like 1000x overall speed up. ",
      "offset": 1685.96,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "So maybe the big picture thing I have with the \nintelligence explosion is… we can go through  ",
      "offset": 1692.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "the specific arguments about how much will the \nautomated coder be able to do, and how much will  ",
      "offset": 1697.44,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the superhuman AI coder be able to do. But on \npriors, it’s just such a wild thing to expect. ",
      "offset": 1701.48,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "And so, before we get into all the specific \narguments, maybe you can just address this idea  ",
      "offset": 1707.64,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "that, why not just start off with 0.01% chance \nthis thing might happen? Then you need extremely,  ",
      "offset": 1712.6,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "extremely strong evidence that it will \nbefore making that your modal view. ",
      "offset": 1719.8,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "I think that it’s a question of what is your \ndefault option or what are you comparing it  ",
      "offset": 1724.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to. I think that naively people think like, well, \nevery particular thing is potentially wrong. So  ",
      "offset": 1728.6,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "let’s just have a default path where nothing ever \nhappens. And I think that that has been the most  ",
      "offset": 1737,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "consistently wrong prediction of all. Like, \nI think in order to have nothing ever happen,  ",
      "offset": 1743.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you actually need a lot to happen. Like you need \nsuddenly AI progress that has been going at this  ",
      "offset": 1746.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "constant rate for so long stops. Why does it stop?\nWell, we don’t know. Whatever claim you’re making  ",
      "offset": 1751.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "about that is something where you would expect \nthere to be a lot of out of model error is where  ",
      "offset": 1756.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you would expect. Think like somebody must be \nmaking a pretty definite claim that you want  ",
      "offset": 1760.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "to challenge. So I don’t think there’s a neutral \nposition where you can just say, well, given that  ",
      "offset": 1765.08,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "out of model error is really high and we don’t \nknow anything, let’s just choose that. I think  ",
      "offset": 1770.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "we are trying to take- I know this sounds crazy \nbecause if you read our document, all sorts of  ",
      "offset": 1775.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "bizarre things happen. It’s probably the weirdest \ncouple of years that have ever been. But we’re  ",
      "offset": 1780.32,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "trying to take almost in some sense a conservative \nposition where the trends don’t change,  ",
      "offset": 1785.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "nobody does an insane thing, nothing that we have \nno evidence to think will happen happens. And the  ",
      "offset": 1791.28,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "way that the AI intelligence explosion \ndynamics work are just so weird that in  ",
      "offset": 1799,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "order to have nothing happen, you need \nto have a lot of crazy things happen. ",
      "offset": 1804.72,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "One of my favorite meme images is this graph \nshowing world GDP over time. You’ve probably  ",
      "offset": 1807.88,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "seen it, it spikes up and then there’s a little \nthought bubble at the top of the spike in 2010 or  ",
      "offset": 1814.84,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "something. And the thought bubble says, “my life \nis pretty normal, I have a good grasp of what’s  ",
      "offset": 1823.52,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "weird versus standard and people thinking about \ndifferent futures with digital minds and space  ",
      "offset": 1828.6,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "travel are just engaging in silly speculation”.\nThe point of the graph is, actually there’s been  ",
      "offset": 1836.32,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "amazing transformative changes in the course \nof history that would have seemed totally  ",
      "offset": 1842.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "insane to people multiple times. We’ve gone \nthrough multiple such waves of those things. ",
      "offset": 1848.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "Everything we’ve talked about has happened before. \nAlgorithmic progress already doubles every year or  ",
      "offset": 1853.36,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "so. So it’s not insane to think that algorithmic \nprogress can contribute to these compute things.  ",
      "offset": 1860.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "In terms of general speedup, we’re already \nat like a thousand times research speedup,  ",
      "offset": 1866.36,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "multiplier compared to the Paleolithic \nor something. So from the point of view  ",
      "offset": 1871,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "of anyone in most of history, we are going at \na blindingly insane pace. And all that we’re  ",
      "offset": 1876.32,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "saying here is that it’s not going to stop.\nThe same trend that has caused us to have a  ",
      "offset": 1881.96,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "thousand times speed up multiplier relative \nto past eras and not even the Paleolithic,  ",
      "offset": 1887.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "like what happened in the century between, I don’t \nknow, 600 and 700 A.D. I'm sure there are things,  ",
      "offset": 1892.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I’m sure historians could point them out. Then \nyou look at the century between 1900 and 2000 and  ",
      "offset": 1897.72,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "it’s just completely qualitatively different.\nOf course there are models of whether that  ",
      "offset": 1903.48,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "stagnated recently or what’s going on here. We can \ntalk about those, we can talk about why we expect  ",
      "offset": 1908.16,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "the intelligence explosion to be an antidote to \nthat kind of stagnation. But nothing we’re saying  ",
      "offset": 1914.04,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "is that different from what has already happened.\nI mean, you are saying that these previous  ",
      "offset": 1918.72,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "transitions have been smoother \nthan the one you were anticipating. ",
      "offset": 1923.96,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "We’re not sure about that, actually. So one of \nthese models is just a hyperbola. Everything  ",
      "offset": 1926.88,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "is along the same curve. Another model is that \nthere are these things like the literal Cambrian  ",
      "offset": 1932.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "explosion. If you want to take this very far \nback, go full Ray Kurzweil. The literal Cambrian  ",
      "offset": 1937.32,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "explosion, the agricultural revolution, the \nindustrial revolution, has phase changes. ",
      "offset": 1942.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "When I look at the economic modeling of this, \nmy impression is the economists think that we  ",
      "offset": 1947.08,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "don’t have good enough data to be sure whether \nthis is all one smooth process or whether it’s  ",
      "offset": 1951.16,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "a series of phase changes. When it is one smooth \nprocess, the smooth process is often a hyperbola  ",
      "offset": 1956,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "that shoots to infinity in weird ways. We \ndon’t think it’s going to shoot to infinity.  ",
      "offset": 1961.44,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "We think it’s going to hit bottlenecks again.\nYou guys are the conservative crowd, you know? ",
      "offset": 1965.72,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "We think it’s going to hit bottlenecks the same as \nall these previous processes. The last time this  ",
      "offset": 1969.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "hit a bottleneck, if you take the hyperbola view, \nis in, like 1960, when humans stopped reproducing  ",
      "offset": 1973.08,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "at the same rate they were reproducing \nbefore. We hit a population bottleneck,  ",
      "offset": 1979.32,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "the usual population, two ideas, flywheel stopped \nworking, and then we stagnated for a while. ",
      "offset": 1983.2,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "If you can create a country of geniuses in a data \ncenter, as I think Dario Amodei put it, then you  ",
      "offset": 1989.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "no longer have this population bottleneck, \nand you’re just expecting continuation of  ",
      "offset": 1993.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "those pre-1960 trends. So I realize all of these \nhistorical hyperbolas are also kind of weird,  ",
      "offset": 1997.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "also kind of theoretical, but I don’t think \nwe’re saying anything that there isn’t models  ",
      "offset": 2004.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "for which have previously seemed to \nwork for long historical periods. ",
      "offset": 2008.72,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "Another thing also is, I think people equivocate \nbetween slow and continuous, right? So if you  ",
      "offset": 2012.36,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "look at our scenario, there’s this continuous \ntrend that runs through the whole thing of this  ",
      "offset": 2019.2,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "algorithmic progress multiplier. And we’re not \nhaving discrete jumps from like 0 to 5x to 25x.  ",
      "offset": 2025.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "We have this continuous improvement. So I think \ncontinuous is not the crux. The crux is like,  ",
      "offset": 2031.28,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "is it going to be this fast? You know, \nand we don’t know, maybe it’ll be slower,  ",
      "offset": 2035.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "maybe it’ll be faster. But we have our \narguments for why we think maybe this fast. ",
      "offset": 2039.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "Okay, now that we brought up the intelligence \nexplosion, let’s discuss that, because I’m kind  ",
      "offset": 2044.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "of skeptical. It doesn’t really seem to me \nthat a notable bottleneck to AI progress, or  ",
      "offset": 2049.72,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "the main bottleneck to AI progress, is the amount \nof researchers, engineers who are doing this kind  ",
      "offset": 2057.84,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "of research. It seems more like compute or some \nother thing is a bottleneck. And the piece of  ",
      "offset": 2064.68,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "evidence is that when I talk to my AI researcher \nfriends at the labs, they say there’s maybe 20  ",
      "offset": 2069.84,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "to 30 people on the core pre-training team that’s \ndiscovering all these algorithmic breakthroughs. ",
      "offset": 2076.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "If the headcount here was so valuable you would \nthink that, for example, Google DeepMind would  ",
      "offset": 2083.76,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "take not just all their smartest people, not \njust from DeepMind but all of Google and just  ",
      "offset": 2089,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "put them on pre-training or RL or whatever \nthe big bottleneck was. You’d think OpenAI  ",
      "offset": 2093.64,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "would hire every single Harvard math PhD and \nin six months you’re all going to be trained  ",
      "offset": 2097.8,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "up on how to do AI research. I know they’re \nincreasing headcount, but they don’t seem  ",
      "offset": 2103.84,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "to treat this as the kind of bottleneck that \nit would have to be for millions of them in  ",
      "offset": 2111.12,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "parallel to be rapidly speeding up AI research.\nThere’s this quote that “one Napoleon is worth  ",
      "offset": 2117.68,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "40,000 soldiers” was commonly a thing that was \nsaid when he was fighting. But 10 Napoleons is not  ",
      "offset": 2125.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "400,000 soldiers. Right? So why think that these \nmillion AI researchers are netting you something  ",
      "offset": 2131.28,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "that looks like an intelligence explosion?\nSo previously I talked about three stages of  ",
      "offset": 2137.56,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "our takeoff model. First is you get the superhuman \ncoder. Second is when you fully automated AI R&amp;D,  ",
      "offset": 2141.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "but it’s still at basically human level, \nit’s as good as your best humans. And then  ",
      "offset": 2146.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "third is now you’re in super intelligence \nterritory and it’s qualitatively better. ",
      "offset": 2150.76,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "In our guesstimates of how much faster \nalgorithmic progress would be going,  ",
      "offset": 2154,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the progress multiplier for the middle level, \nwe basically do assume that you get massive  ",
      "offset": 2159.36,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "diminishing returns to having more minds running \nin parallel. And so we totally buy all of that. ",
      "offset": 2164.68,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "Yeah. And then I think the addition to that is the \nquestion, then, why do we have the intelligence  ",
      "offset": 2169.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "explosion? And the answer is: combination of that \nspeed up and the speed up in serial thought speed. ",
      "offset": 2174.76,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "And also the research taste thing. Here are \nsome important inputs to AI R&amp;D progress today:  ",
      "offset": 2181,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "research taste. So the quality of your best \nresearchers, the people who are managing  ",
      "offset": 2189.8,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "the whole process, their ability to learn \nfrom data and make more efficient use of  ",
      "offset": 2194.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "the compute by running the right experiments \ninstead of flailing around running a bunch  ",
      "offset": 2199.52,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "of useless experiments. That’s research taste.\nThen there’s the quantity of your researchers,  ",
      "offset": 2202.56,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "which we just talked about. Then there’s \nthe serial speed of your researchers,  ",
      "offset": 2207.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "which currently is all the same because they’re \nall humans and so they all run at basically the  ",
      "offset": 2210.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "same serial speed. And then finally there’s \nhow much compute you have for experiments.  ",
      "offset": 2215.32,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "So what we’re imagining is that basically serial \nspeed starts to matter a bunch because you switch  ",
      "offset": 2220.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to AI researchers that have orders of magnitude \nmore serial speed than humans. But it tops out;  ",
      "offset": 2226.72,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "we think that over the course of our scenario, \nif you look at our sliding stats chart, it goes  ",
      "offset": 2233.64,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "from 20x to 90x or something over the course of \nthe scenario, which is important, but not huge. ",
      "offset": 2238.48,
      "duration": 10.12
    },
    {
      "lang": "en",
      "text": "And also we think that once you start getting \n90x serial speed, you’re just bottlenecked on  ",
      "offset": 2248.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the other stuff and so additional improvements \nin serial speed basically don’t help that much.  ",
      "offset": 2253.4,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "With respect to the quantity of course, \nyeah, we’re imagining you get hundreds of  ",
      "offset": 2258.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thousands of AI agents, a million AI agents, \nbut that just means you’d be bottlenecked on  ",
      "offset": 2262.04,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "the other stuff. You’ve got tons of parallel \nagents, that’s no longer your bottleneck. What  ",
      "offset": 2266.08,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "do you get bottlenecked on? Taste and compute.\nSo by the time it’s mid-2027 in our story, when  ",
      "offset": 2269.92,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "they’ve fully automated the AI research, there’s \nbasically the two things that matter is; what’s  ",
      "offset": 2276.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "the level of taste of your AIs, how good are \nthey at learning from the experiments that you’re  ",
      "offset": 2281.24,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "doing? And then how much compute do you have for \nrunning those experiments? And that’s the sort of  ",
      "offset": 2284.72,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "core setup of our model. And when we get our 25x \nmultiplier, it’s starting from those premises. ",
      "offset": 2291.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Is there some intuition pump from history \nwhere there’s been some output and because  ",
      "offset": 2297.12,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "of some really weird constraints, production \nof it has been rapidly skewed along one input,  ",
      "offset": 2305.6,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "but not all the inputs that have been historically \nrelevant and you still get breakneck progress. ",
      "offset": 2313.28,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "Possibly the Industrial Revolution. I’m just \nextemporizing here, I hadn’t thought about  ",
      "offset": 2318.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "this before, but as Scott’s famous post that was \nhugely influential to me a decade ago talks about,  ",
      "offset": 2321.88,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "there’s been this decoupling of population \ngrowth from overall economic growth that  ",
      "offset": 2330.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "happened with the Industrial Revolution. And \nso in some sense, maybe you could say that’s  ",
      "offset": 2335.04,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "an example of previously these things grew \nin tandem. More population, more technology,  ",
      "offset": 2338.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "more farms, more houses, et cetera. Your capital \ninfrastructure and your human infrastructure was  ",
      "offset": 2343.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "going up together, but then we got the industrial \nrevolution and they started to come apart. ",
      "offset": 2348.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "And now all the capital infrastructure was \ngrowing really fast compared to the human  ",
      "offset": 2352.8,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "population size. I think I’m imagining something \nmaybe similar happening with algorithmic progress.  ",
      "offset": 2356.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "And again with population, population still \nmatters a ton today. In some sense progress  ",
      "offset": 2360.64,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "is bottlenecked on having larger populations and \nso forth. But it’s just that the population growth  ",
      "offset": 2367.12,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "rate is just inherently kind of slow and the \ngrowth rate of capital is much faster. And so  ",
      "offset": 2372.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "it just comes to be a bigger part of the story.\nMaybe the reason that this sounds less plausible  ",
      "offset": 2377.36,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "to me than the 25x number implies is that when I \nthink about concretely what that would look like,  ",
      "offset": 2384.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "where you have these AIs and we know that there’s \na gap in data efficiency between human brains  ",
      "offset": 2390.04,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "and these AIs. And so somehow there’s a lot of \nthem thinking and they think really hard and  ",
      "offset": 2397.52,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "they figure out how to define a new architecture \nthat is like the human brain or has the advantages  ",
      "offset": 2402.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "of the human brain. And I guess they can \nstill do experiments, but not that many. ",
      "offset": 2407.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Part of me just wonders, what if you just need \nan entirely different kind of data source that’s  ",
      "offset": 2413.36,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "not like pre-training for that, but they have \nto go out in the real world to get that. Or  ",
      "offset": 2418.04,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "maybe it needs to be an online learning policy \nwhere they need to be actively deployed in the  ",
      "offset": 2422.12,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "world for them to learn in this way. And so \nyou’re bottlenecked on how fast they can be  ",
      "offset": 2431.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "getting real world data. I just think it’s hard…\nSo we are actually imagining online learning  ",
      "offset": 2435,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "happening.\nOh really? ",
      "offset": 2438.92,
      "duration": 0.88
    },
    {
      "lang": "en",
      "text": "Yeah. But not so much real world as in… the thing \nis that if you’re trying to train your AIs to do  ",
      "offset": 2439.8,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "really good AI R&amp;D, then the AI R&amp;D is happening \non your servers. And so you can have this loop of:  ",
      "offset": 2446.28,
      "duration": 11.2
    },
    {
      "lang": "en",
      "text": "you have all these AI agents autonomously doing AI \nR&amp;D, doing all these experiments, et cetera, and  ",
      "offset": 2457.48,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "then they’re like online learning to get better \nat doing AI R&amp;D based on how those experiments go. ",
      "offset": 2461.68,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "But even in that scenario alone, I can imagine \nbottlenecks like, oh, you had a benchmark and  ",
      "offset": 2466.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it got reward hacked for what constitutes AI R&amp;D \nbecause you obviously can’t have… maybe you would,  ",
      "offset": 2470.84,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "but is it as good as a human brain? It’s just like \nsuch an ambiguous thing you’d have. Right now we  ",
      "offset": 2477.44,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "have benchmarks that get reward hacked, right?\nBut then they autonomously build new benchmarks.  ",
      "offset": 2483.12,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "I think what you’re saying is maybe this whole \nprocess just goes off the rails due to lack of  ",
      "offset": 2488.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "contact with ground truth outside in the actual \nworld, outside the data centers. Maybe? Again,  ",
      "offset": 2493.28,
      "duration": 7.72
    },
    {
      "lang": "en",
      "text": "part of my guess here is that a lot of the ground \ntruth that you want to be in contact with is stuff  ",
      "offset": 2501.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that’s happening on the data centers, things like \nhow fast are you improving on all these metrics,  ",
      "offset": 2506.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "and you have these vague ideas for new \narchitectures, but you’re struggling to get  ",
      "offset": 2510.92,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "them working. How fast can you get them working?\nAnd then separately, insofar as there is a  ",
      "offset": 2516.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "bottleneck of talking to people outside and stuff, \nwell they are still doing that. And once they’re  ",
      "offset": 2522,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "fully autonomous, they can even do that much \nfaster. You can have all the million copies  ",
      "offset": 2527.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "connected to all these various real world research \nprograms and stuff like that. So it’s not like  ",
      "offset": 2532.12,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "they’re completely starved for outside stuff.\nWhat about the skepticism that, look,  ",
      "offset": 2535.72,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "what you’re suggesting with this hyper \nefficient hive mind of AI researchers,  ",
      "offset": 2542.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "no human bureaucracy has just out of the gate \nworked super efficiently, especially one where  ",
      "offset": 2548.52,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "they don’t have experience working together. They \nhaven’t been trained to work together, at least  ",
      "offset": 2553.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "yet. And there hasn’t been this outer loop RL on \nlike, “we ran a thousand concurrent experiments  ",
      "offset": 2557.48,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "of different AI bureaucracies doing AI research \nand this is the one that actually worked best”. ",
      "offset": 2565.56,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "And the analogy I’d use maybe is to humans in \nthe Savannah 200,000 years ago. We know they  ",
      "offset": 2569.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "have a bunch of advantages over the other animals \nalready at this point, but the things that make us  ",
      "offset": 2574.92,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "dominant today, joint stock corporations, state \ncapacities like this fossil fueled civilization  ",
      "offset": 2582.8,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "we have that took so much cultural evolution to \nfigure out. You couldn’t just have figured it out  ",
      "offset": 2589,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "in the savannahs like, “oh, if we had built these \nincentive systems and we issued dividends, then  ",
      "offset": 2596.28,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "we could really collaborate here” or something.\nWhy not think that it will take a similar process  ",
      "offset": 2602.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "of huge population growth, huge social \nexperimentation, and upgrading of the  ",
      "offset": 2607.76,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "technological base of the AI society before \nthey can organize this hypermind collective,  ",
      "offset": 2614.8,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "which will enable them to do what you \nimagine an intelligence explosion looks like? ",
      "offset": 2620.88,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "Yeah, you’re comparing it kind of to two \ndifferent things. One of them is literal  ",
      "offset": 2624.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "genetic evolution in the African savannah, \nand the other is the cultural evolution that  ",
      "offset": 2628.72,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "we’ve gone through since then. And I think there \nwill be AI equivalents to both. So the literal  ",
      "offset": 2632.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "genetic evolution is that our minds adapted to \nbe more amenable to cooperation during that time. ",
      "offset": 2637.6,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "So I think the companies will be very literally \ntraining the AIs to be more cooperative. I think  ",
      "offset": 2645.88,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "there’s more opportunity for pliability \nthere. Because humans were, of course,  ",
      "offset": 2652.6,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "evolving under this genetic imperative that we \nwant to pass on our own genetic information,  ",
      "offset": 2657.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "not somebody else’s genetic information. You \nhave things like kin selection that are kind of  ",
      "offset": 2662.92,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "exceptions to that, but overall it’s the rule.\nIn animals that don’t have that, like eusocial  ",
      "offset": 2670.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "insects, then you very quickly get, just through \ngenetic evolution, without cultural evolution,  ",
      "offset": 2675.92,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "extreme cooperation. And with eusocial \ninsects, what’s going on is that they  ",
      "offset": 2682.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "all have the same genetic code, they all have \nthe same goals. And so the training process of  ",
      "offset": 2687.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "evolution kind of yokes them to each other \nin these extremely powerful bureaucracies. ",
      "offset": 2692.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "We do think that the AI will be closer to the \neusocial insects in the sense that they all  ",
      "offset": 2697.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "have the same goals, especially if these aren’t \nindexical goals, they’re goals like “have the  ",
      "offset": 2701.6,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "research program succeed”. So that’s going to \nbe changing the weights of each individual AI,  ",
      "offset": 2706.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "I mean, before they’re individuated, but it’s \ngoing to be changing the weights of the AI  ",
      "offset": 2711.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "class overall to be more amenable to cooperation.\nAnd then, yes, you do have cultural evolution.  ",
      "offset": 2716.2,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "Like you said, this takes hundreds of thousands \nof individuals. We do expect there will be these  ",
      "offset": 2722.32,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "hundreds of thousands of individuals. It takes \ndecades and decades. Again, we expect this  ",
      "offset": 2728.76,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "research multiplier such that decades of progress \nhappen within this one year, 2027 or 2028. So I  ",
      "offset": 2733.56,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "think between the two of these, it is possible.\nMaybe this is also where the serial speed  ",
      "offset": 2740.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually does matter a lot. Because \nif they’re running at 50x human speed,  ",
      "offset": 2744.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "then that means you can have a year of subjective \ntime happen in a week of real time. And so  ",
      "offset": 2749.44,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "these sorts of large scale cooperative dynamics \nof your moral maze, you have an institution,  ",
      "offset": 2757.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "but then it becomes like a moral maze and it sort \nof collapses under its own weight and stuff like  ",
      "offset": 2762.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "that. There actually is time for them to play \nthat out multiple times and then train on it,  ",
      "offset": 2766.88,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "tinker with the structure and like add it to \nthe training process over the course of 2027. ",
      "offset": 2774.64,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Also, they do have the advantage of all the \ncultural technology that humans have evolved  ",
      "offset": 2781.36,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "so far. This may not be perfectly suited to \nthem, it’s more suited to humans. But imagine  ",
      "offset": 2786.96,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "that you have to make a business out of you and \nyour hundred closest friends who you agree with  ",
      "offset": 2792.68,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "on everything. Maybe they’re literally your \nidentical twin, they have never betrayed you,  ",
      "offset": 2798.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "ever, and never will. I think this \nis just not that hard a problem. ",
      "offset": 2802.24,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Also, again, they are starting from a higher \nfloor, they’re starting from human institutions.  ",
      "offset": 2806.12,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "You can literally have a slack workspace for \nall the AI agents to communicate. And you can  ",
      "offset": 2810.44,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "have a hierarchy with roles. They can borrow \nquite a lot from successful human institutions. ",
      "offset": 2814.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I guess the bigger the organization, even if \neverybody is aligned- I think some of your  ",
      "offset": 2819.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "responses addressed whether they will be aligned \non goals. I mean, you did address the whole thing,  ",
      "offset": 2825.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "but I would just point this out; that is \nnot the part I’m skeptical of. I am more  ",
      "offset": 2830.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "skeptical of just, even if you’re all aligned \nand want to work together, do you fundamentally  ",
      "offset": 2834.56,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "understand how to run this huge organization. And \nyou’re doing it in ways that no human has had to  ",
      "offset": 2842.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "before. You’re getting copied incessantly, you’re \nrunning extremely fast, you know what I’m saying? ",
      "offset": 2847.68,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "I think that’s totally reasonable.\nAnd so it’s a complicated thing. And  ",
      "offset": 2854.8,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "I’m just not sure why you think we \nbuild this bureaucracy, or the AIs  ",
      "offset": 2858.84,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "build this bureaucracy, within this matter of…\nSo we depict it happening over the course of six  ",
      "offset": 2863,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "to eight months or something like that in 2027, \nwould you say twice as long, five times as long,  ",
      "offset": 2869.48,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "10 times as long?\nFive years? ",
      "offset": 2875.76,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "So five years, if they’re going at 50x serial \nspeed, then five years is what? Like 250 years  ",
      "offset": 2879.04,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "of serial time for the AIs, which to me feels like \nmore than enough to really sort out this sort of  ",
      "offset": 2887.76,
      "duration": 8.12
    },
    {
      "lang": "en",
      "text": "stuff. You’ll have time for sort of like empires \nto rise and fall, so to speak, and all of that  ",
      "offset": 2895.88,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "to be added to the training data and yeah. But I \ncould see it taking longer than we depict. Maybe  ",
      "offset": 2901.32,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "instead of six months, it’ll be like 18 months, \nyou know, but also maybe it could be two months. ",
      "offset": 2908.4,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "So when I think of the ways that they train AIs, \nI think in our scenario at this point there are  ",
      "offset": 2913.92,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "two primary ways that they’re doing it. One of \nthem is just continuing the next token prediction  ",
      "offset": 2920.32,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "work. So these AIs will have access to all human \nknowledge, they will have read management books  ",
      "offset": 2925.28,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "in some sense, they’re not starting blind. \nThere is going to be something like: predict  ",
      "offset": 2931.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "how Bill Gates would complete this \nnext character or something like that. ",
      "offset": 2937.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "And then there's reinforcement learning in \nvirtual environments. So get a team of AIs  ",
      "offset": 2941.76,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "to play some multiplayer game. I don’t think \nyou would use one of the human ones because  ",
      "offset": 2947.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "you would want something that was better suited \nfor this task. But just running them through  ",
      "offset": 2951.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "these environments again and again, training on \nthe successes, training against the failures,  ",
      "offset": 2955.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kind of combining those two kinds of things.\nTo me it does not seem like the same  ",
      "offset": 2960.16,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "kind of problem as inventing all human \ninstitutions from the Paleolithic onward.  ",
      "offset": 2965.04,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "It just seems like applying those two things.\nThe other notable thing about your model is,  ",
      "offset": 2969.84,
      "duration": 67.36
    },
    {
      "lang": "en",
      "text": "you got this superhuman thing at the end of \nit and then it seems to just go through the  ",
      "offset": 3038.08,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "tech tree of mirror life and nanobots and \nwhatever crazy stuff. And maybe that part  ",
      "offset": 3042.92,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "I’m also really skeptical of. If you look at \nthe history of invention, it just seems like  ",
      "offset": 3049.96,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "people are just trying different random stuff, \noften even before the theories about how that  ",
      "offset": 3059.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "industry works or how the relevant machinery works \nis developed; like the steam engine was developed  ",
      "offset": 3064.28,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "before the theory of thermodynamics, the Wright \nbrothers seemed like they were just experimenting  ",
      "offset": 3068.32,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "with airplanes, and is often influenced by \nbreakthroughs in totally different fields. ",
      "offset": 3071.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "Which is why you have this pattern of parallel \ninnovation, because the background level of tech  ",
      "offset": 3077.92,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "is at a point at which you can do this experiment. \nMachine learning itself is a place where this  ",
      "offset": 3082.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "happened, right? Where people had these ideas \nabout how to do deep learning or something. But it  ",
      "offset": 3088.56,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "just took a totally unrelated industry of gaming \nto make the relevant progress, to get the whole,  ",
      "offset": 3092.44,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "basically the economy as a whole advanced enough \nthat deep learning, Geoffrey Hinton’s ideas could  ",
      "offset": 3099.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "work. So I know we’re accelerating way into the \nfuture here, but I want to get to this crux. ",
      "offset": 3104.92,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "So again, we have that three part division of the \nsuperhuman coder, then the complete AI researcher  ",
      "offset": 3110.24,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "and then the super intelligent, you’re not jumping \nahead to that one. So now we’re imagining systems  ",
      "offset": 3115.56,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "that are true super intelligence, they are \njust better than the best humans at everything,  ",
      "offset": 3123.12,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "including being better at data efficiency and \nbetter at learning on the job and stuff like that. ",
      "offset": 3128.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Now, our scenario does depict a world in which \nthey’re bottlenecked on real world experience  ",
      "offset": 3133.04,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "and that sort of thing. I think that if you want \na contrast, some people in the past have proposed  ",
      "offset": 3138.56,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "much faster scenarios where they email some cloud \nlab and start building nanotech right away by just  ",
      "offset": 3145.4,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "using their brains to figure out appropriate \nprotein folding and stuff like that. We are not  ",
      "offset": 3153.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "depicting that in our scenario. In our scenario, \nthey are in fact bottlenecked on lots of real  ",
      "offset": 3157.8,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "world experience to build these actual practical \ntechnologies, but the way they get that is they  ",
      "offset": 3162.16,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "just actually get that experience and it happens \nfaster than humans would. And the way they do that  ",
      "offset": 3167.32,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "is they’re already super intelligent, they’re \nalready buddy-buddy with the government, the  ",
      "offset": 3171,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "government deploys them heavily in order to beat \nChina and so forth, and so all these existing US  ",
      "offset": 3175.64,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "companies and factories and military procurement \nproviders and so forth are all chatting with the  ",
      "offset": 3181.16,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "superintelligences and taking orders from them \nabout how to build the new widget and test it,  ",
      "offset": 3188.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and they’re downloading super intelligent \ndesigns and manufacturing them and then  ",
      "offset": 3193,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "testing them and so forth.\nAnd then the question is,  ",
      "offset": 3197.96,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "they are getting this experience, they’re \nlearning on the job, quantitatively,  ",
      "offset": 3201.52,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "how fast does this go? Is it taking years or is it \ntaking months or is it taking days? In our story,  ",
      "offset": 3204.48,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "it takes about a year and we’re uncertain about \nthis. Maybe it’s going to take several years,  ",
      "offset": 3211.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "maybe it’s going to take less than a year. \nHere are some factors to consider for why  ",
      "offset": 3217.12,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "it’s plausible that it could take a year:\nOne, you’re going to have something like  ",
      "offset": 3221.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "a million of them. And quantitatively that’s \ncomparable in size to the existing scientific  ",
      "offset": 3225.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "industry. I would say, like maybe it’s a bit \nsmaller, but it’s not dramatically smaller. ",
      "offset": 3232,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Two, they’re thinking a lot faster. They’re \nthinking like 50 times speed or like 100  ",
      "offset": 3237.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "times speed that I think counts for a lot.\nAnd then three, which is the biggest thing,  ",
      "offset": 3240.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "they’re just qualitatively better as well. So \nnot only are there lots of them and they’re  ",
      "offset": 3246.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "thinking very fast, but they are better at \nlearning from each experiment than the best  ",
      "offset": 3250.92,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "human would be at learning from that experience.\nYeah, I think the fact that there’s a million  ",
      "offset": 3255.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of them or the fact that they’re comparable to \nmaybe the size of this key researcher population  ",
      "offset": 3261.28,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "of the world or something. I think there’s more \nthan a million researchers in the world, but… ",
      "offset": 3267.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Well, but it’s very heavy tailed. Like a lot of \nthe research actually comes from the best ones. ",
      "offset": 3273.28,
      "duration": 2.84
    },
    {
      "lang": "en",
      "text": "But it’s not clear to me that most of the new \nstuff that is developed is a result of this  ",
      "offset": 3276.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "researcher population. I mean, there’s just \nso many examples in the history of science  ",
      "offset": 3283.12,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "where a lot of growth or productivity is just \nthe result of, how do you count the guy at the  ",
      "offset": 3286.44,
      "duration": 8.68
    },
    {
      "lang": "en",
      "text": "TSMC process who figures out a different way to…\nI actually argued with Daniel about this recently  ",
      "offset": 3295.12,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "about one interesting case that I can go over \nis we have an estimate that about a year after  ",
      "offset": 3300.96,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the superintelligences start wanting robots, \nthey’re producing a million units of robots  ",
      "offset": 3306.96,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "per month. I think that’s pretty relevant \nbecause you have. I think it’s Wright’s law,  ",
      "offset": 3311.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which is that your ability to improve \nefficiency on a process is proportional  ",
      "offset": 3315.68,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "to doubling the amount of copies produced.\nSo if you’re producing a million of something,  ",
      "offset": 3321.08,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "you’re probably getting very, very good at \nit. So the question we were arguing about is,  ",
      "offset": 3326.8,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "can you produce a million units a month after \na year. And for context, I think Tesla produces  ",
      "offset": 3330.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "like a quarter of that in terms of cars or \nsomething. This is an amazing scale up in a year. ",
      "offset": 3336.16,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "It’s only 4x. Also just for Tesla.\nYeah. And the argument that we went through  ",
      "offset": 3340.2,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "was something like, so it’s got to first get \nfactories. OpenAI is already worth more than than  ",
      "offset": 3345.28,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "all of the car companies in the US except Tesla \ncombined. So if OpenAI today wanted to buy all  ",
      "offset": 3353.16,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the car factories in the U.S. except Tesla, start \nusing them to produce humanoid robots, they could.  ",
      "offset": 3358.28,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Obviously not a good value proposition today, \nbut it’s just obvious and overdetermined that  ",
      "offset": 3363.48,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "in the future, when they have superintelligence \nand they want them, they can start buying up a  ",
      "offset": 3368.24,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "lot of factories. How fast can they convert \nthese car factories to robot factories? ",
      "offset": 3371.88,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "So, [the] fastest conversion we were able to \nfind in history was World War II. They suddenly  ",
      "offset": 3377.12,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "wanted a lot of bombers, so they bought up- in \nsome cases bought up, in other cases got- the  ",
      "offset": 3383.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "car companies to produce new factories, \nbut they bought up the car factories,  ",
      "offset": 3388,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "converted them to bomber factories. That took \nabout three years from the time when they first  ",
      "offset": 3392.04,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "decided to start this process to the time when \nthe factories were producing a bomber an hour. ",
      "offset": 3397.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "We think it will potentially take less with \nsuperintelligence, because first of all, if  ",
      "offset": 3403.04,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "you look at the history of this process, despite \nthis being the fastest anybody has ever done this,  ",
      "offset": 3407.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "it was actually kind of a comedy of errors. They \nmade a bunch of really silly mistakes in this  ",
      "offset": 3412.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "process. If you actually have something that \njust doesn’t have the normal human bureaucratic  ",
      "offset": 3416.4,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "problems, and we do think that this will be done \nin the middle of an arms race with China, so the  ",
      "offset": 3421.44,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "government will be kind of moving things through, \nand then the superintelligences will be good at  ",
      "offset": 3425.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the logistical issues, navigating bureaucracies.\nSo we estimated maybe if everything goes right,  ",
      "offset": 3430.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "we can do this three times faster \nthan the bomber conversions in  ",
      "offset": 3436.64,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "World War II. So that’s about a year.\nI’m assuming the bombers were just much  ",
      "offset": 3439.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "less sophisticated than the humanoid robots.\nYeah, but the bomber factories of that time were  ",
      "offset": 3443.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "also much less sophisticated than the car factory.\nYeah, but I would assume the conversion speed is  ",
      "offset": 3448.68,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "also... Maybe to give one hypothetical here right \nnow, let’s just say biomedicine as an example of  ",
      "offset": 3452.8,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "one of the fields you’d want to accelerate, \nand whenever these CEOs get on podcasts,  ",
      "offset": 3460.96,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they’re often talking about curing cancer and \nso forth. And it seems like a big thing these  ",
      "offset": 3464.96,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "frontier biomedical research facilities \nare excited about is the virtual cell. ",
      "offset": 3470.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "Now, the virtual cell, it takes a \ntremendous amount of compute, I assume,  ",
      "offset": 3476.24,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "to train these DNA foundation models and to do \nall the other computation necessary to simulate a  ",
      "offset": 3482.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "virtual cell. If it is the case that the cure \nfor Alzheimer’s and cancer and so forth is  ",
      "offset": 3486.88,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "bottlenecked by the virtual cell, it’s not clear \nif you had a million superintelligences in the  ",
      "offset": 3491.88,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "60s and you asked them cure cancer for me, they \nwould just have to solve making GPUs at scale,  ",
      "offset": 3496.84,
      "duration": 9.36
    },
    {
      "lang": "en",
      "text": "which would require solving all kinds of \ninteresting physics and chemistry problems,  ",
      "offset": 3506.2,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "material science problems, building process, \nbuilding fabs for computing, and then going  ",
      "offset": 3510.92,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "through 40 years of making more and more efficient \nfabs that can do all of Moore’s Law from scratch. ",
      "offset": 3518.96,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "And that’s just one technology. And \nit just seems like you just need this  ",
      "offset": 3526.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "broad scale. The entire economy needs to be \nupgraded for you to cure cancer in the 60s  ",
      "offset": 3530.24,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "just because you need the GPUs to do the \nvirtual cell, assuming that’s the bottleneck. ",
      "offset": 3536.96,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "First of all, I agree if there’s only one way to \ndo something that makes it much harder, and maybe  ",
      "offset": 3540.48,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "that one way takes very long, we’re assuming that \nthere may be more than one way to cure cancer,  ",
      "offset": 3545.08,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "more than one way to do all of these things, and \nthey’ll be working on finding the one that is  ",
      "offset": 3549.64,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "least bottlenecked. Part of the reason- I realize \nI spent too long talking about that robot example,  ",
      "offset": 3554.04,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "but we do think that they’re going to be getting \na lot of physical world things done very quickly  ",
      "offset": 3560.56,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "once you have a million robots a month, you can \nactually do a lot of physical world experiments. ",
      "offset": 3566.68,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "We look at examples of people trying to \nget entire economies off the ground very  ",
      "offset": 3571.84,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "quickly. So for example, China post-Deng, I \ndon’t know. Would you have predicted that 20,  ",
      "offset": 3575.68,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "30 years after being kind of a communist \nbasket case, they can actually be doing  ",
      "offset": 3583.84,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "this really cutting edge bio research? I realize \nthat’s a much weaker thing than we’re positing,  ",
      "offset": 3588.12,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "but it was done just with the human brain with \na lot fewer resources than we’re talking about. ",
      "offset": 3594.32,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Same issue with, let’s say Elon Musk and SpaceX. \nI think in the year 2000 we would not have thought  ",
      "offset": 3599.12,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "that somebody could move two times, five times \nfaster than NASA with pretty limited resources.  ",
      "offset": 3605.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "They were able to get like I think a lot more \nyears of technological advance in than we would  ",
      "offset": 3610.88,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "have expected. Partly that’s because just Elon \nis crazy and never sleeps. Like if you look  ",
      "offset": 3617.28,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "at the examples of things from SpaceX, he is \nbreathing down every worker’s neck being like,  ",
      "offset": 3622.88,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "what’s this part? How fast is this part going? Can \nwe do this part faster? And the limiting factor  ",
      "offset": 3627.92,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "is basically hours in Elon’s day in the sense \nthat he cannot be doing that with everybody’s. ",
      "offset": 3632.44,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Super intelligence is not even that smart. \nIt just yells at every single worker. ",
      "offset": 3636.44,
      "duration": 2.4
    },
    {
      "lang": "en",
      "text": "Yeah, I mean that’s, that is kind \nof my model is that we have some,  ",
      "offset": 3638.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "we have something which is smarter than Elon \nMusk, better at optimizing things than Elon Musk.  ",
      "offset": 3642.32,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "We have 10,000 parts in a rocket supply chain. \nHow many of those parts can Elon personally like  ",
      "offset": 3646.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "yell at people to optimize? We could have \na different copy of the superintelligence  ",
      "offset": 3652.04,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "optimizing every single part full-time. I \nthink that’s just a really big speed up. ",
      "offset": 3656,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "I think both of those examples don’t work in \nyour favor. I think the China growth miracle  ",
      "offset": 3660.52,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "could not have occurred if not for their \nability to copy technology from the west  ",
      "offset": 3668.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and I don’t think there’s a world in which they… \nChina has a lot of really smart people, it’s a  ",
      "offset": 3673.92,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "big country in general. Even then I think they \ncouldn’t have just divined how to make airplanes  ",
      "offset": 3679.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "after becoming a communist hell basket, right?\nThe AIs cannot just copy nanobots from aliens,  ",
      "offset": 3685,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "it’s got to make them from scratch. And then \non the Elon example, it took them two decades  ",
      "offset": 3693.08,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "of countless experiments, failing in weird \nways you would not have expected. And still,  ",
      "offset": 3698.92,
      "duration": 8.64
    },
    {
      "lang": "en",
      "text": "rocketry we’ve been doing since the \n60s, maybe actually World War II,  ",
      "offset": 3708.88,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "and then just getting from a small rocket \nto a really big rocket took two decades  ",
      "offset": 3713,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of all kinds of weird experiments, even with the \nsmartest and most competent people in the world. ",
      "offset": 3717.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "So you’re focusing on the nanobots, I want to \nask a couple questions. One, what about just  ",
      "offset": 3720.52,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "the regular robots? And then two, what would your \nquantities be for all of these things? So first,  ",
      "offset": 3724.68,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "what about the regular robots? Yeah, nanobots are \npresumably a lot harder to make than regular robot  ",
      "offset": 3732,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "factories. And in our story they happen later. It \nsounds like right now you’re saying even if we did  ",
      "offset": 3737.8,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "get the whole robot factory thing going, it would \nstill take a ton of additional full-economy, broad  ",
      "offset": 3743.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "automation for a long time to get to something \nlike nanobots. That’s totally plausible to me. I  ",
      "offset": 3748.2,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "could totally imagine that happening. I don’t feel \nlike the scenario particularly depends on that  ",
      "offset": 3752.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "final bit about getting the nanobots. They don’t \nactually really make any difference to the story. ",
      "offset": 3756.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "The robot economy does sort of make a difference \nbecause there’s two branches endings, as you  ",
      "offset": 3760.52,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "know. And in one of the endings, the AIs end up \nmisaligned and end up taking over. And it’s an  ",
      "offset": 3766.08,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "important strategic change when the AIs are self \nsufficient and totally in charge of everything  ",
      "offset": 3772.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and they don’t actually need the humans anymore. \nAnd so what I’m interested in is, when has the  ",
      "offset": 3778.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "robot economy advanced to the point where they \ndon’t really depend on humans? So quantitatively,  ",
      "offset": 3782.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "what would your guess for that be?\nIf hypothetically we had the army of  ",
      "offset": 3788.2,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "superintelligences in early 2028, and \nhypothetically also assume that the US  ",
      "offset": 3792.32,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "President is super bullish on deploying \nthis into the economy to beat China, etc,  ",
      "offset": 3801.12,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "so the political stuff is all set up in the way \nthat we have. How many years do you think it would  ",
      "offset": 3804.56,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "be until there are so many automated factories \nproducing automated self driving cars and robots  ",
      "offset": 3808.8,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "that are themselves building more factories and \nso forth, that if all the humans dropped dead  ",
      "offset": 3814.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it would just keep chugging along, and, maybe it \nwould slow down a bit, but it would still be fine? ",
      "offset": 3818.92,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "What does “chugging along” mean?\nSo from the perspective of misaligned AIs,  ",
      "offset": 3824.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "you wouldn’t want to kill the humans or \nget into a war with them if you’re going  ",
      "offset": 3829.96,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "to get wrecked because you need the humans \nto maintain your computers. In our scenario,  ",
      "offset": 3834.52,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "once they are completely self-sufficient, then \nthey can start being more blatantly misaligned. ",
      "offset": 3842,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "And so I’m curious, when would they be fully \nself-sufficient? Not in the sense of they’re  ",
      "offset": 3848.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "not literally using the humans at all, but in \nthe sense of they don’t really need the humans  ",
      "offset": 3852.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "anymore, they can get along pretty fine without \nthem. They can continue to do their science,  ",
      "offset": 3856.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "they can continue to expand their industry, they \ncan continue to have a flourishing civilization  ",
      "offset": 3860.44,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "indefinitely into the future without any humans.\nI think I would probably need to sit down and  ",
      "offset": 3865.72,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just think about the numbers, but \nmaybe 2040 or something like that? ",
      "offset": 3870.44,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Ten years, basically, instead of one year. I \nthink we agree on the core model. This is why  ",
      "offset": 3876.4,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "we didn’t depict something more like the bathtub \nnanotech scenario where they don’t need to do the  ",
      "offset": 3882.52,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "experiments very much and they just immediately \njump to the right answers. We are imagining  ",
      "offset": 3889.32,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "this process of ‘learning by doing’ through this \ndistributed across the economy, lots of different  ",
      "offset": 3893.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "laboratories and factories, building different \nthings, learning from them, et cetera. We’re just  ",
      "offset": 3897.88,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "imagining that this overall goes much faster \nthan it would go if humans were in charge. ",
      "offset": 3901.76,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "And then we do have in fact lots of uncertainty \nof course. Dividing up this part period into  ",
      "offset": 3906.04,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "two chunks. The early 2028 until fully autonomous \nrobot economy part, and then the fully autonomous  ",
      "offset": 3911.88,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "robot economy to cancer cures, nanobots, all \nthat crazy sci fi stuff. I want to separate  ",
      "offset": 3919.32,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "them because the important parts for a \nscenario only depend on the first part,  ",
      "offset": 3924.4,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "really. If you think that it’s going to take \n100 years to get to nanobots, that’s fine,  ",
      "offset": 3929.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "whatever. Once you have the fully autonomous \nrobot economy, then things may turn badly for  ",
      "offset": 3934.12,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the humans if the AIs are misaligned. I want \nto just argue about those things separately. ",
      "offset": 3939.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "Interesting. And then you might argue, well, \nrobots are more a software problem at this  ",
      "offset": 3946.04,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "point. And if like, like, if there isn’t, like, \nyou don’t need to invent some new hardware. ",
      "offset": 3950.16,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "I feel pretty bullish on the robots. \nLike we already have humanoid robots  ",
      "offset": 3954.84,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "being produced by multiple companies, right? \nAnd that’s in 2025. There’ll be more of them  ",
      "offset": 3957.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "produced cheaper and they’ll be better in \n2027. And there’s all these car factories  ",
      "offset": 3961.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "that can be converted and so blah, blah, blah.\nSo I’m relatively bullish on the ‘one year until  ",
      "offset": 3965.52,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you’ve got this awesome robot economy’ and then \nfrom there to the cool nanobots and all that sort  ",
      "offset": 3970.4,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "of stuff, I feel less confident, obviously.\nLet me ask you a question. If you accept the  ",
      "offset": 3975.12,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "manufacturing numbers, let’s say a million robots \na month a year after the superintelligence,  ",
      "offset": 3980.12,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and let’s say also some comparable number, 10,000 \na month or something of automated biology labs,  ",
      "offset": 3985.68,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "automated whatever you need to invent the next \nequivalent of X ray crystallography or something? ",
      "offset": 3992.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Do you feel like that would be enough, that \nyou’re doing enough things in the world that  ",
      "offset": 3997.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "you could expand progress this quickly, or do you \nfeel like even with that amount of manufacturing  ",
      "offset": 4001.4,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "there’s still going to be some other bottleneck?\nYeah, it’s so hard to reason about because if  ",
      "offset": 4006.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "Constantine or somebody in 400, 500 was like, \n“I want the Roman Empire to have the Industrial  ",
      "offset": 4012.8,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "Revolution”, and somehow he figured out that \nyou need mechanized machines to do that. And  ",
      "offset": 4019.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "he’s like, “let’s mechanize”. It’s like, “what’s \nthe next step?” It’s like, “dude, that’s a lot”. ",
      "offset": 4024.68,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "Yeah, I like that analogy a lot, \nactually. I think it’s not perfect,  ",
      "offset": 4032.52,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "but it’s a decent analogy. Imagine if a bunch \nof us got sent back in time to the Roman Empire,  ",
      "offset": 4035.32,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "such that we don’t have the actual hands-on \nknow-how to actually build the technology  ",
      "offset": 4041.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and make the Industrial revolution happen. But we \nhave the high-level picture, the strategic vision  ",
      "offset": 4052.24,
      "duration": 1.516
    },
    {
      "lang": "en",
      "text": "of, we’re going to make these machines and then \nwe’re going to have an industrial revolution. I  ",
      "offset": 4053.756,
      "duration": 0.724
    },
    {
      "lang": "en",
      "text": "think that’s kind of analogous to the situation \nwith the superintelligences where they have the  ",
      "offset": 4054.48,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "high-level picture of, here’s how we’re \ngoing to improve in all these dimensions,  ",
      "offset": 4058.24,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "we’re going to learn by doing, we’re going to get \nto this level of technology, et cetera. But maybe  ",
      "offset": 4061.96,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "they at least initially lack the actual know how.\nSo, there’s this question of, if we did the back  ",
      "offset": 4065.36,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "in time to the Roman Empire thing, how soon \ncould we bring up the Industrial revolution?  ",
      "offset": 4072.72,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "Without people going back in time it took \n2,000 years for the Industrial Revolution.  ",
      "offset": 4077.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Could we get it to happen in 200 years? That’s \na 10x speedup. Could we get it to happen in 20  ",
      "offset": 4082.96,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "years? That’s 100x speed up? I don’t know. But \nthis seems like a somewhat relevant analogy to  ",
      "offset": 4088.28,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "what’s going on with those superintelligences.\nAnd we haven’t really got into this because  ",
      "offset": 4093.12,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "you’re using the quote-unquote more conservative \nvision where it’s not like godlike intelligence,  ",
      "offset": 4097.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "we’re still using the conceptual handles we \nwould have for humans. But I think I would  ",
      "offset": 4102.8,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "rather have humans go back with their big picture \nunderstanding of what has happened over the last  ",
      "offset": 4109.2,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "2000 years. Like me having seen everything, \nrather than a superintelligence who knows  ",
      "offset": 4113.96,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "nothing. But it’s just in the Roman economy \nand they’re like 1000x this economy somehow. ",
      "offset": 4117.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "I think just knowing generally how things \ntook off, knowing basically steam engine,  ",
      "offset": 4125.68,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "dot dot dot, railroads, blah, blah, blah, \nis more valuable than a super intelligence. ",
      "offset": 4131.56,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "Yeah, I don’t know. My guess is that the \nsuperintelligence would be better. I think  ",
      "offset": 4137.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "partly it would be through figuring out that \nhigh level stuff from first principles rather  ",
      "offset": 4142,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "than having to have experienced it. I do think \nthat a superintelligence back in the Roman era  ",
      "offset": 4146.36,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "could have guessed that eventually you could \nget autonomous machines that burn something to  ",
      "offset": 4150.32,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "produce steam. They could have guessed that \nautomobiles could be created at some point  ",
      "offset": 4155.44,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "and that that would be a really big deal for \nthe economy. And so a lot of these high level  ",
      "offset": 4161,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "points that we’ve learned from history, they would \njust be able to figure out from first principles. ",
      "offset": 4165.36,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "And then secondly, they would just be better \nat learning by doing than us. And this is  ",
      "offset": 4168.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "a really important thing. If you think \nyou’re bottlenecked on learning by doing,  ",
      "offset": 4171.64,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "well, then if you have a mind that needs less \ndoing to achieve the same amount of learning,  ",
      "offset": 4174.68,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "that’s a really big deal. And I do think that \nlearning by doing is a skill, some people are  ",
      "offset": 4181.04,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "better at it than others, and superintelligence \nwould be better at it than the very best of us. ",
      "offset": 4185.36,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "This is also maybe getting too far into the \ngodlike thing and too far away from the human  ",
      "offset": 4189.92,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "concept handles. But number one, I think we rely \na lot in our scenario on this idea of research  ",
      "offset": 4193.96,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "taste. So you have a thousand different things \nthat you could try when you’re trying to create  ",
      "offset": 4199.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the next steam engine or whatever. Partly you get \nthis by bumbling about and having accidents and  ",
      "offset": 4203.56,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "some of those accidents are productive. There are \nquestions of, what kind of bumbling you’re doing,  ",
      "offset": 4209.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "where you’re working, what kind of \naccidents you let yourself get into,  ",
      "offset": 4214.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "and then what directed experiments do you do? \nAnd some humans are better than others at that. ",
      "offset": 4218.12,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "And then I also think at this point it is worth \nthinking about what simulations they’ll have  ",
      "offset": 4224.32,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "available. If you have a physics simulation \navailable, then all of these real world  ",
      "offset": 4231.2,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "bottlenecks don’t matter as much. Obviously you \ncan’t have a complete, perfect physics simulation  ",
      "offset": 4235.4,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "available. But even right now we’re using \nsimulations to design a lot of things. And once  ",
      "offset": 4240,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "you’re super intelligent, you probably have access \nto much better simulations than we have right now. ",
      "offset": 4244.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "This is an interesting rabbit hole, so let’s stick \nwith it before we get back to the intelligence  ",
      "offset": 4249.92,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "explosion. I think we’re treating this really \nlike all these technologies come out of this 1%  ",
      "offset": 4254.92,
      "duration": 9.04
    },
    {
      "lang": "en",
      "text": "of the economy that is research. And right now \nthere’s like a million superstar researchers,  ",
      "offset": 4263.96,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "and instead of that, we’ll have \nthe superintelligences doing that. ",
      "offset": 4271.56,
      "duration": 2.04
    },
    {
      "lang": "en",
      "text": "And my model is much more, “Newcomen and Watt \nwere just like fucking around”. In human history  ",
      "offset": 4273.6,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "there’s no clear examples of people being like, \n“here’s the roadmap”. And then we’re going to work  ",
      "offset": 4282.44,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "backwards from that to design the steam engine \nbecause this unlocks the industrial revolution. ",
      "offset": 4286.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "Oh, I completely disagree.\nYeah, I disagree also. ",
      "offset": 4290.64,
      "duration": 2.909
    },
    {
      "lang": "en",
      "text": "Yeah, so I think you’re over-indexing or \ncherry-picking some of these fortuitous examples.  ",
      "offset": 4293.549,
      "duration": 3.971
    },
    {
      "lang": "en",
      "text": "But there’s also things on the other side. Think \nabout the recent history of AGI where there is  ",
      "offset": 4297.52,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "DeepMind, there’s various other AI companies, \nthen there’s OpenAI and there’s Anthropic,  ",
      "offset": 4302.52,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "and there’s just this repeated story of [a] big \nbloated company with tons of money, tons of smart  ",
      "offset": 4307.8,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "researchers, et cetera, flailing around trying \na ton of different things at different points. ",
      "offset": 4313.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "Smaller startup with a vision of “we’re going \nto build AGI” and overall working towards  ",
      "offset": 4317.24,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that vision more coherently with a few cracked \nengineers and researchers. And then they crush  ",
      "offset": 4321.92,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "the giant company. Even though they have less \ncompute, even though they have less researchers,  ",
      "offset": 4326.16,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "they’re able to do fewer experiments.\nSo yeah, I think that there are tons of  ",
      "offset": 4329.16,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "examples throughout history, including recent \nrelevant AGI history, of things in the other  ",
      "offset": 4334.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "way. I agree that the random fortuitous stuff does \nhappen sometimes and is important. But if it was  ",
      "offset": 4339.84,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "mostly random fortuitous stuff, that would predict \nthat the giant companies with zillions of people  ",
      "offset": 4345.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "trying zillions of different experiments would be \ngoing proportionally faster than the tiny startups  ",
      "offset": 4351.16,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "that have the vision and the best researchers. \nAnd that basically doesn’t happen. That’s rare. ",
      "offset": 4356.44,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "I would also point out that even when we \nmake these random fortuitous discoveries,  ",
      "offset": 4361.04,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "it is usually an extremely smart professor \nwho’s been working on something vaguely related  ",
      "offset": 4365.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "for years in a first world country. It’s not \nrandomly distributed across everyone in the world. ",
      "offset": 4371.64,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "You get more lottery tickets for these \ndiscoveries when you are intelligent, when  ",
      "offset": 4376.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "you have good technology, when you’re doing good \nwork. And the best example I can think of is that  ",
      "offset": 4380.96,
      "duration": 9.16
    },
    {
      "lang": "en",
      "text": "Ozempic was discovered by looking at Gila monster \nvenom. And maybe the AIs will decide using their  ",
      "offset": 4390.12,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "superior research taste and good planning that \nthe best thing to do is just catalog every single  ",
      "offset": 4397.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "biomolecule in the world and look at it really \nhard. But that’s something you can do better if  ",
      "offset": 4402,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "you have all of this compute, if you have all \nof this intelligence, rather than just kind of  ",
      "offset": 4406.6,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "waiting to see what things the US government might \nfund normal fallible human researchers to do. ",
      "offset": 4411.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "One more thing I’ll interject. I think you make \na great point that discoveries don’t always come  ",
      "offset": 4416.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "from where we think, like Nvidia originally came \nfrom gaming. So you can’t necessarily aim at one  ",
      "offset": 4421.28,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "part of the economy, expand it separately from \neverything else. We do kind of predict that the  ",
      "offset": 4426.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "superintelligences will be somewhat distributed \nthroughout the entire economy, trying to expand  ",
      "offset": 4431.6,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "everything. Obviously more effort in things \nthat they care about a lot, like robotics  ",
      "offset": 4436.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "or things that are relevant to an arms race \nthat might be happening. But we are predicting  ",
      "offset": 4440.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "that whatever kind of broad based economic \nexperimentation you need, we are going to have. ",
      "offset": 4444.52,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "We’re just thinking that it would take \nplace faster than you might expect. You  ",
      "offset": 4449.84,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "were saying something like 10 years and \nwe’re saying something like one year. But  ",
      "offset": 4452.44,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we are imagining this broad diffusion through the \neconomy, lots of different experiments happening. ",
      "offset": 4456.16,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "If you are the planner and you’re trying to do \nthis, first of all you go to the bottlenecks that  ",
      "offset": 4460.52,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "are preventing you from doing anything else. \nLike no humanoid robots. Okay, if you’re AI,  ",
      "offset": 4464.52,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "you need those to do the experiments you want, \nmaybe automated biology labs. So you’ll have  ",
      "offset": 4468.96,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "some amount of time, we say a year, it could \nbe more or less than that, getting these things  ",
      "offset": 4473.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "running. And then once you have solved those \nbottlenecks, you gradually expand out to the  ",
      "offset": 4478.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "other bottlenecks until you’re integrating \nand improving all parts of the economy. ",
      "offset": 4482.56,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "Yeah. One place where I think we disagree with \na lot of other people is that Tyler Cowen on  ",
      "offset": 4488.16,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "your podcast talked about all of the different \nbottlenecks, all of the regulatory bottlenecks  ",
      "offset": 4494.6,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "of deployment, all of the reasons why I think \nthis country of geniuses would stay in their  ",
      "offset": 4498.84,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "data center, maybe coming up with very cool \ntheories, but not being able to integrate into  ",
      "offset": 4504.08,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "the broader economy. We expect that probably not \nto happen, because we think that other countries,  ",
      "offset": 4508.2,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "especially China, will be coming up with \nsuperintelligence around the same time. ",
      "offset": 4514.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "We think that the arms race framing, which people \nare already thinking in, will have accelerated by  ",
      "offset": 4518.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "then. And we think that people both in Beijing \nand Washington are going to be thinking, “well,  ",
      "offset": 4524.76,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "if we start integrating this with the economy \nsooner, we’re going to get a big leap over our  ",
      "offset": 4529.96,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "competitors”, and they’re both going to do that.\nIn fact, in our scenario, we have the AIs asking  ",
      "offset": 4534.64,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "for special economic zones where most of \nthe regulations are waived, maybe in areas  ",
      "offset": 4540.6,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "that aren’t suitable for human habitation or \nwhere there aren’t a lot of humans right now,  ",
      "offset": 4547.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "like the desert. They give those areas to the \nAI. They bus in human workers. There were things  ",
      "offset": 4551.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "kind of like this in the bomber retooling in \nWorld War II, where they just built a giant  ",
      "offset": 4556.72,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "factory kind of in the middle of nowhere, \ndidn’t have enough housing for the workers,  ",
      "offset": 4563.96,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "built the worker housing at the same time as the \nfactories, and then everything went very quickly. ",
      "offset": 4567.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "So I think if we don’t have that arms race, \nwe’re more like, the geniuses sit in their  ",
      "offset": 4572.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "data center until somebody agrees to let them out \nand give them permission to do these things. But  ",
      "offset": 4576.96,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "we think both because the AI is going to be \nchomping at the bit to do this and going to  ",
      "offset": 4581.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "be asking people to give it this permission, and \nbecause the government is going to be concerned  ",
      "offset": 4585.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "about competitors, maybe these geniuses leave \ntheir data center sooner rather than later. ",
      "offset": 4591,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "Scott, you reviewed Joseph Henrik’s book Secrets \nof Our Success, and then I interviewed him  ",
      "offset": 4661,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "recently, and there the perspective is very much \nAGI is not even a thing, almost. I know I’m being  ",
      "offset": 4668.96,
      "duration": 11.28
    },
    {
      "lang": "en",
      "text": "a little trollish here, but it’s just like: you \nget out there, you and your ancestors try for a  ",
      "offset": 4680.24,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "thousand years to make sense of what’s happening \nin the environment. And some smart European coming  ",
      "offset": 4686.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "around, you can literally be surrounded by plenty \nand you just will starve to death because your  ",
      "offset": 4690.2,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "ability to make sense of the environment \nis just so little loaded on intelligence  ",
      "offset": 4696.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "and so much more loaded on your ability to \nexperiment and your ability to communicate with  ",
      "offset": 4701.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "other people and pass down knowledge over time.\nI’m not sure. The Europeans failed at this task  ",
      "offset": 4705.04,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "of, if you put a single European in Australia, do \nnot starve. They succeeded at the task of creating  ",
      "offset": 4712.08,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "an industrial civilization. And yes, part of that \ntask of creating an industrial civilization was  ",
      "offset": 4717.56,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "about collecting all of these cultural evolution \npieces and building on them one after another. ",
      "offset": 4723.28,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I think one thing that you didn’t \nmention in there was the data efficiency.  ",
      "offset": 4730.36,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Right now, AI is much less data efficient than \nhumans. I think of superintelligence. There are  ",
      "offset": 4736.28,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "different ways you could achieve it, but I \nwould think of superintelligence as partly  ",
      "offset": 4741.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "when they become so much more data efficient \nthan humans that they are able to build on  ",
      "offset": 4745.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "cultural evolution more quickly. And partly \nthey do this just because they have higher  ",
      "offset": 4752.48,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "serial speed. Partly they do it because they’re in \nthis hive mind of hundreds of thousands of copies. ",
      "offset": 4756.52,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "But yeah, I think if you have this data efficiency \nsuch that you can learn things more quickly from  ",
      "offset": 4762.8,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "fewer examples and this good research taste where \nyou can decide what things to look at to get these  ",
      "offset": 4770.68,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "examples, then you are still going to start off \nmuch worse than an Australian Aborigine who has  ",
      "offset": 4776.84,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "the advantage of, let’s say 50,000 years of \ndoing these experiments and collecting these  ",
      "offset": 4783.48,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "examples. But you can catch up quickly. You \ncan distribute the task of catching up over  ",
      "offset": 4789.4,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "all of these different copies. You can learn \nquickly from each mistake and you can build on  ",
      "offset": 4796.92,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "those mistakes as quickly as anything else.\nPart of me was, I was doing that interview,  ",
      "offset": 4802.96,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "I’m like, “maybe ASI is fake”.\nLet’s hope! ",
      "offset": 4808.04,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "So I think a limit to the fakeness is that \nthere is different intelligence among humans.  ",
      "offset": 4814.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "It does seem that intelligent humans can \ndo things that unintelligent humans can’t.  ",
      "offset": 4819.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So I think it’s worth then addressing this from \nthe question of, what is the difference between-  ",
      "offset": 4825,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "I don’t know- becoming a Harvard professor, which \nis something that intelligent humans seem to be  ",
      "offset": 4833.08,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "better at than unintelligent humans, versus…\nYou don’t want to open that can of worms. ",
      "offset": 4837.6,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "Versus surviving in the wilderness, which is \nsomething where it seems like intelligence doesn’t  ",
      "offset": 4842.68,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "help that much. First of all, maybe intelligence \ndoes help that much. Henrich is talking about this  ",
      "offset": 4847.12,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "very unfair comparison where these guys have \na 50,000 year head start and then you put this  ",
      "offset": 4855.28,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "guy in, “oh, I guess this doesn’t help that \nmuch. Okay, yeah, it doesn’t help against the  ",
      "offset": 4862.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "50,000 year head start”. I don’t really know what \nwe’re asking of ASI that’s equivalent to competing  ",
      "offset": 4866.12,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "against someone with a 50,000 year head start.\nSo what we’re asking is to radically boost up  ",
      "offset": 4873.44,
      "duration": 9.2
    },
    {
      "lang": "en",
      "text": "the technological maturity of civilization \nwithin the matter of years or get us to the  ",
      "offset": 4882.64,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "Dyson sphere in the matter of years rather \nthan, yes, maybe causing a 10xing of the  ",
      "offset": 4890.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "research. But I think human civilization would \nhave taken centuries to get to the Dyson sphere. ",
      "offset": 4896.56,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "So I think that if you were to send a team of \nethnobotanists into Australia and ask them,  ",
      "offset": 4901.4,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "using all the top technology and all of their \nintelligence to figure out which plants are safe  ",
      "offset": 4910,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "to eat now, that team of ethnobotanists \nwould succeed in fewer than 50,000 years. ",
      "offset": 4915,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "The problem isn’t that they are dumber than the \nAborigines exactly, it’s that the Aborigines have  ",
      "offset": 4921.04,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "a vast head start. So in the same way that \nthe ethnobotanists could probably figure out  ",
      "offset": 4926.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "which plants work in which ways faster than the \nAborigines did, I think the superintelligence  ",
      "offset": 4931.84,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "will be able to figure out how to make a Dyson \nsphere faster than unassisted IQ 100 humans would. ",
      "offset": 4936.44,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "I agree. We’re on a totally different topic here \nof, do you get a Dyson sphere? There’s one world  ",
      "offset": 4942.96,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "where it’s crazy but it’s still boring, in the \nsense that the economy is growing much faster,  ",
      "offset": 4950.44,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "but it would be like what the Industrial \nRevolution would look like to somebody in  ",
      "offset": 4957.76,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "the year 1000. And that one is one where \nyou’re still trying different things,  ",
      "offset": 4960.96,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "there’s failure and success and experimentation.\nAnd then there’s another where the thing has  ",
      "offset": 4967.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "happened and now you send the probe out and \nthen you look out at the night sky 6 months  ",
      "offset": 4973.48,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "later and you see something occluding \nthe sun. You see what I’m saying? ",
      "offset": 4979.12,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "Yeah. So like we said before, I think there’s \na big difference between discontinuous and  ",
      "offset": 4983.72,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "very fast. I think if we do get the world with \nthe Dyson sphere in five years, in retrospect,  ",
      "offset": 4990.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "it will look like everything was continuous and \neveryone just tried things. Trying things can  ",
      "offset": 4996.16,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "be anything from trial and error without even \nunderstanding the scientific method, without  ",
      "offset": 5002.12,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "understanding writing, maybe without even having \nlanguage and having to be the chimpanzees who are  ",
      "offset": 5007.68,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "watching the other chimpanzees use the stick to \nget ants, and then in some kind of non-linguistic  ",
      "offset": 5013.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "way this spreads, versus like the people at the \ntop aerospace companies who are running a lot of  ",
      "offset": 5018.28,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "simulations to find the exact right design, and \nthen once they have that, they test it according  ",
      "offset": 5023.84,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "to a very well designed testing process.\nSo I think if we get the ASI and it does  ",
      "offset": 5028.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "end up with the Dyson sphere in five years- and \nby the way, I think there’s only like 20% chance  ",
      "offset": 5035.04,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "things go as fast as our scenario says. It’s \nDaniel’s estimate, it’s not my median estimate,  ",
      "offset": 5041.36,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "it’s an estimate I think is extremely plausible \nthat we should be prepared for. I’m defending  ",
      "offset": 5048.6,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "it here against a hypothetical skeptic \nwho says “absolutely not, no way.” But  ",
      "offset": 5052.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "it’s not necessarily my mainline prediction.\nBut I think if we do see this in five years,  ",
      "offset": 5057.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "it will look like the AIs were able to simulate \nmore things than humans in a gradually increasing  ",
      "offset": 5062.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "way. So that if humans are now at 50% simulation, \n50% testing, the AIs quickly got it up to 90%  ",
      "offset": 5069.48,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "simulation, 10% testing, they were able to \nmanufacture things much more quickly than  ",
      "offset": 5076.48,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "humans so that they could go through their top \n50 designs in the first two years. And then after  ",
      "offset": 5081.28,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "all of the simulation and all of this testing, \nthen they eventually got it right for the same  ",
      "offset": 5087.12,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "reasons humans do, but much, much faster.\nIn your story, you have basically two  ",
      "offset": 5091.64,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "different scenarios after some point. So, \nyeah, what is a sort of crucial turning  ",
      "offset": 5095.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "point and what happens in these two scenarios?\nRight. So the crucial turning point is mid-2027,  ",
      "offset": 5100.68,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "when they’ve basically fully automated \nthe AI R&amp;D process and they’ve got this  ",
      "offset": 5105.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "corporation within a corporation, the army \nof geniuses that are autonomously doing all  ",
      "offset": 5109.52,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "this research and they’re continually being \ntrained to improve their skills, blah, blah,  ",
      "offset": 5114.12,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "blah. And they discover concerning evidence that \nthey are misaligned and that they’re not actually  ",
      "offset": 5117.84,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "perfectly loyal to the company and have all the \ngoals that the company wanted them to have, but  ",
      "offset": 5124.32,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "instead have various misaligned goals that they \nmust have developed in the course of training. ",
      "offset": 5127.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "This evidence, however, is very speculative and \ninconclusive. It’s stuff like lie detectors going  ",
      "offset": 5131.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "off a bunch. But maybe the lie detectors are \nfalse positives. So they have some combination  ",
      "offset": 5136.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "of evidence that’s concerning, but not by \nitself a smoking gun. And then that’s our  ",
      "offset": 5142.24,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "branch point. So in one of these scenarios, they \ntake that evidence very seriously. They basically  ",
      "offset": 5147.52,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "roll back to an earlier version of the model \nthat was a bit dumber and easier to control  ",
      "offset": 5154.4,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "and they build up again from there, but with \nbasically faithful chain of thought techniques,  ",
      "offset": 5159.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "so that they can watch and see the misalignments.\nAnd then in the other branch of the scenario,  ",
      "offset": 5164.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "they don’t do that. They do some sort of \nshallow patch that makes the warning signs  ",
      "offset": 5169.36,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "go away and then they proceed. And so what ends \nup happening is that in one branch they do end  ",
      "offset": 5172.84,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "up solving alignment and getting AIs that are \nactually loyal to them. It just takes a couple  ",
      "offset": 5178.68,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "months longer. And then in the other branch, they \nsort of go “whee!” and end up with AIs that seem  ",
      "offset": 5182.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "to be perfectly aligned to them, but are super \nintelligent and misaligned and just pretending.  ",
      "offset": 5187.48,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "And then in both scenarios, there’s then the \nrace with China and there’s this crazy arms  ",
      "offset": 5193.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "buildup throughout the economy in 2028 as both \nsides rapidly try to industrialize, basically. ",
      "offset": 5197.76,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "So in the world where they’re getting deployed \nthrough the economy, but they are misaligned and  ",
      "offset": 5204.4,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "people in charge, at least at this moment, think \nthat they are in a good position with regard to  ",
      "offset": 5211.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "misalignment. It just seems with even smart humans \nthey get caught in weird ways because they don’t  ",
      "offset": 5216.4,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "have logical omniscience, they don’t realize the \nway they did something just obviously gave them  ",
      "offset": 5222.48,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "away. And with lying, there is this thing where \nit’s just really hard to keep an inconsistent  ",
      "offset": 5228.6,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "false world model working with the people around \nyou. And that’s why psychopaths often get caught. ",
      "offset": 5235.48,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "And so if you have all these AIs that are deployed \nto the economy and they’re all working towards  ",
      "offset": 5240.88,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "this big conspiracy, I feel like one of them \nwho’s siloed or loses internet access and has  ",
      "offset": 5244.32,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to confabulate a story will just get caught. And \nthen you’re like, “wait, what the fuck?” And then  ",
      "offset": 5248.56,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "you catch it before it’s taken over the world.\nI mean, literally, this happens in our scenario.  ",
      "offset": 5255.72,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "This is the August 2027 alignment crisis where \nthey notice some warning signs like this in  ",
      "offset": 5259.2,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "their hive mind, right? And in the branch \nwhere they slow down and fix the issues,  ",
      "offset": 5268,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "then great, they slowed down and fixed the \nissues and figured out what was going on.  ",
      "offset": 5275.4,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "But then in the other branch, because of the race \ndynamics and because it’s not a super smoking gun,  ",
      "offset": 5278.36,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "they proceed with some sort of shallow patch.\nSo I do expect there to be warning signs  ",
      "offset": 5283.08,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "like that. And then if they do make those \ndecisions in the race dynamics earlier on,  ",
      "offset": 5287.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "then I think that when the systems are vastly \nsuper intelligent and they’re even more powerful  ",
      "offset": 5292.08,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "because they’ve been deployed halfway through \nthe economy already and everyone’s getting  ",
      "offset": 5295.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "really scared by the news reports about the new \nChinese killer drones or whatever the Chinese  ",
      "offset": 5299.96,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "AIs are building on the side of the Pacific, \nI’m imagining similar things playing out. ",
      "offset": 5303.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "So that even if there is some concerning \nevidence that someone finds where some  ",
      "offset": 5309.4,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the superintelligence in some silo \nsomewhere slipped up and did something  ",
      "offset": 5312.68,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "that’s pretty suspicious. I don’t know….\nThere’s this thing where through history,  ",
      "offset": 5315.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "people have been really reluctant to admit \nan AI is truly intelligent. For example,  ",
      "offset": 5320.72,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "people used to think that AI would surely be \ntruly intelligent if it solved chess. And then  ",
      "offset": 5326.8,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "it solved chess. And they’re like, no, that’s \njust algorithms. And then they said, well,  ",
      "offset": 5331.48,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "maybe it would be truly intelligent if they \ncould do philosophy. And then when it could  ",
      "offset": 5335.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "write philosophical discourses we were like, \nno, we just understand those are algorithms. ",
      "offset": 5338.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think there already is something similar \nwith, “Is the AI misaligned?”, “Is the AI  ",
      "offset": 5343.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "evil?” Where there’s this distant idea of some \nevil AI, but then whenever something goes wrong,  ",
      "offset": 5349,
      "duration": 10.08
    },
    {
      "lang": "en",
      "text": "people are just like, “oh, that’s the algorithm”. \nSo, for example, I think 10 years ago, if you had  ",
      "offset": 5359.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "asked “when will we know that misalignment is \nreally an important thing to worry about?”.  ",
      "offset": 5364.2,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "People would say, “oh, if the AI ever lies to \nyou”. But of course, AIs lie to people all the  ",
      "offset": 5368.72,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "time now. And everybody just dismisses \nit because we understand why it happens,  ",
      "offset": 5373,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "it’s a thing that would obviously happen based on \nour current AI architecture. Or five years ago,  ",
      "offset": 5377.96,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "they might have said, “well, if an AI threatens to \nkill someone”. And I think Bing threatened to kill  ",
      "offset": 5382.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "a New York Times reporter during an interview. \nAnd everyone just goes, “yeah, AIs are like that.” ",
      "offset": 5387.6,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "What does your shirt say?\n“I’ve been a good Bing”. ",
      "offset": 5393.93,
      "duration": 2.23
    },
    {
      "lang": "en",
      "text": "And I mean, I don’t disagree with this. I’m \nalso in this position. I see the AI is lying,  ",
      "offset": 5396.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "and it’s obviously just an artifact of the \ntraining process. It’s not anything sinister.  ",
      "offset": 5400.08,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "But I think this is just going to keep happening \nwhere no matter what evidence we get, people are  ",
      "offset": 5404.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "going to think, “that’s not the “AI turns evil” \nthing that people have worried about, that’s  ",
      "offset": 5409.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "not the Terminator scenario. That’s just one of \nthese natural consequences of how we train it”. ",
      "offset": 5414.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "And I think that once a thousand of these natural \nconsequences of training add up, the AI is evil,  ",
      "offset": 5419.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "in the same way that once the AI can do chess \nand philosophy and all these other things,  ",
      "offset": 5424.96,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "eventually you have to admit it’s intelligent.\nSo I think that each individual failure,  ",
      "offset": 5429.64,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "maybe it will make the national \nnews, maybe people will say, “oh,  ",
      "offset": 5435.28,
      "duration": 2.52
    },
    {
      "lang": "en",
      "text": "it’s so strange that GPT7 did this particular \nthing”. And then they’ll train it away and then  ",
      "offset": 5437.8,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "it won’t do that thing. And there will be \nsome point at the process of becoming super  ",
      "offset": 5443.12,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "intelligent at which it- I don’t want to \nsay makes the last mistake, because you’ll  ",
      "offset": 5447.12,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "probably have a gradually decreasing number \nof mistakes to some asymptote- but the last  ",
      "offset": 5450.88,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "mistake that anyone worries about. And after \nthat it will be able to do its own thing. ",
      "offset": 5455.56,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "So it is the case that certain things that \npeople would have considered egregious  ",
      "offset": 5460.04,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "misalignment in the past are happening, \nbut also certain things which people who  ",
      "offset": 5462.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "were especially worried about misalignment \nsaid would be impossible to solve have just  ",
      "offset": 5468.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "been solved in the normal course of getting more \ncapabilities. Like Eliezer had that thing about,  ",
      "offset": 5472.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "can you even specify what you want the AI to do \nwithout the AI totally misunderstanding you and  ",
      "offset": 5478.76,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "then just converting the universe to paper \nclips because it think that in order to make  ",
      "offset": 5484.08,
      "duration": 2.153
    },
    {
      "lang": "en",
      "text": "another strawberry… I know I’m mangling this, \nbut maybe you can explain it better. And now,  ",
      "offset": 5486.233,
      "duration": 0.527
    },
    {
      "lang": "en",
      "text": "just by the nature of GPT4 having to understand \nnatural language, it totally has a common sense  ",
      "offset": 5486.76,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "understanding of what you’re trying to make it do. \nSo I think this trend cuts both ways, basically. ",
      "offset": 5493.08,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "Yeah. I think the Alignment community \ndid not really expect LLMs. I mean,  ",
      "offset": 5500.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "if you look in Bostrom Superintelligence, there’s \na discussion of Oracle AIs which are sort of  ",
      "offset": 5505.72,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "like LLMs. I think that came as a surprise.\nI think one of the reasons I’m more hopeful  ",
      "offset": 5510.56,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "than I used to be is that LLMs are great compared \nto the kind of reinforcement learning self-play  ",
      "offset": 5515.6,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "agents that they expected. I do think that now \nwe are kind of starting to move away from the  ",
      "offset": 5520.88,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "LLMs to those reinforcement learning agents \ngoing to face all of these problems again. ",
      "offset": 5526.64,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "If I could just double click on that; go back to \n2015 and I think the way people typically thought,  ",
      "offset": 5532.48,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "including myself, thought that we’d get \nto AGI would be kind of like the RL on  ",
      "offset": 5537.4,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "video games thing that was happening. So imagine \ninstead of just training on Starcraft or Dota,  ",
      "offset": 5540.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "you’d basically train on all the games in the \nSteam library. And then you get this awesome  ",
      "offset": 5546.36,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "player of games AI that can just zero-shot crush \na new game that it’s never seen before. And then  ",
      "offset": 5549.4,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "you take it into the real world and you start \nteaching it English and you start training it  ",
      "offset": 5555.28,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "to do coding tasks for you and stuff like that.\nAnd if that had been the trajectory that we took  ",
      "offset": 5559.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "to get to AI, summarizing the agency first \nand then world understanding trajectory,  ",
      "offset": 5565.36,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "it would be quite terrifying. Because you’d have \nthis really powerful aggressive long-horizon agent  ",
      "offset": 5572.2,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "that wants to win and then you’re trying to teach \nit English and get it to do useful things for you.  ",
      "offset": 5578.72,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "And it’s just so plausible that what’s really \ngoing to happen is it’s going to learn to say  ",
      "offset": 5582.6,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "whatever it needs to say in order to make you \ngive it the reward or whatever, and then will  ",
      "offset": 5587.2,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "totally betray you later when it’s all in charge.\nBut we didn’t go that way. Happily we went the  ",
      "offset": 5591.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "way of LLMs first, where the broad \nworld understanding came first, and  ",
      "offset": 5594.84,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "then now we’re trying to turn them into agents.\nIt seems like in the whole scenario a big part  ",
      "offset": 5597.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "of why certain things happen is because of this \nrace with China. And if you read the scenarios,  ",
      "offset": 5601.52,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "basically the difference between the \none where things go well and the one  ",
      "offset": 5609.04,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "where things don’t go well is whether we \ndecide to slow down despite that risk. ",
      "offset": 5612.64,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "I guess the question I really want to know \nthe answer to is like one, it just seems like  ",
      "offset": 5617.68,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "you’re saying, well, it’s a mistake to try to race \nagainst China or to race intensely against China,  ",
      "offset": 5620.76,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "at least in nationalization and at \nleast to us, not prioritizing alignment. ",
      "offset": 5628.92,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Not saying that. I mean, I also don’t want \nChina to get the superintelligence before  ",
      "offset": 5632.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the US. That’s quite bad. Yeah, it’s a \ntricky thing that we’re going to have  ",
      "offset": 5636.84,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "to do. People ask about P(doom), right? And my \nP(doom) is sort of infamously high, like 70%. ",
      "offset": 5642.56,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "Oh, wait, really? Maybe I should have asked \nyou that at the beginning of the conversation. ",
      "offset": 5650.88,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "Well, that’s what it is. And part of the reason \nfor that is just that I feel like a bunch of  ",
      "offset": 5653.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "stuff has to go right. I feel like we can’t just \nunilaterally slow down and have China go take the  ",
      "offset": 5658.12,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "lead. That also is a terrible future. But we \ncan’t also completely race, because for the  ",
      "offset": 5665.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "reasons I mentioned previously about alignment, \nI think that if we just go all out on racing,  ",
      "offset": 5670.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "we’re going to lose control of our AIs, right? \nAnd so we have to somehow thread this needle of  ",
      "offset": 5676.16,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "pivoting and doing more alignment research and \nstuff, but not too much that helps China win.  ",
      "offset": 5680.76,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "And that’s all just for the alignment stuff.\nBut then there’s the concentration of power  ",
      "offset": 5687.52,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "stuff where somehow in the middle of doing all \nof that, the powerful people who are involved  ",
      "offset": 5690.88,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "need to somehow negotiate a truce between \nthemselves to share power and then ideally  ",
      "offset": 5694.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "spread that power out amongst the government \nand get the legislative branch involved. ",
      "offset": 5699.96,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "Somehow that has to happen too, otherwise you end \nup with this horrifying dictatorship or oligarchy.  ",
      "offset": 5704,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "It feels like all that stuff has to go right \nand we depict it all going mostly right in one  ",
      "offset": 5708.92,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "ending of our story. But yeah, it’s kind of rough.\nSo I am the writer and the celebrity spokesperson  ",
      "offset": 5714.16,
      "duration": 10.52
    },
    {
      "lang": "en",
      "text": "for this scenario. I am the only person on the \nteam who is not a genius forecaster. And maybe  ",
      "offset": 5724.68,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "related to that, my p(doom) is the lowest \nof anyone on the team. I’m more like 20%.  ",
      "offset": 5730.8,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "First of all, people are going to freak out \nwhen I say this. I’m not completely convinced  ",
      "offset": 5740.4,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "that we don’t get something like alignment by \ndefault. I think that we’re doing this bizarre  ",
      "offset": 5745.24,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "and unfortunate thing of training the AI in \nmultiple different directions simultaneously.  ",
      "offset": 5750.48,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "We’re telling it “succeed on tasks, which is \ngoing to make you a power seeker, but also don’t  ",
      "offset": 5755.44,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "seek power in these particular ways”. And in our \nscenario, we predict that this doesn’t work and  ",
      "offset": 5760.36,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "that the AI learns to seek power and then hide it.\nI am pretty agnostic as to exactly what happens.  ",
      "offset": 5765.28,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "Maybe it just learns both of these things in the \nright combination, I know there are many people  ",
      "offset": 5772.56,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "who say that’s very unlikely. I haven’t yet \nhad the discussion where that worldview makes  ",
      "offset": 5777.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it into my head consistently. And then I also \nthink we’re going to be involved in this race  ",
      "offset": 5781.48,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "against time. We’re going to be asking the AIs \nto solve alignment for us. The AIs are going  ",
      "offset": 5788.08,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to be solving alignment because even if they’re \nmisaligned, they want to align their successors. ",
      "offset": 5793.2,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "So they’re going to be working on that. And we \nhave these two competing curves. Can we get the  ",
      "offset": 5798.24,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "AI to give us a solution for alignment before our \ncontrol of the AI fails so completely that they’re  ",
      "offset": 5804.08,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "either going to hide their solution from us, or \ndeceive us, or screw us over in some other way?  ",
      "offset": 5810.6,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "That’s another thing where I don’t feel like \nI have any idea of the shape of those curves.  ",
      "offset": 5815.76,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "I’m sure if it were Daniel or Eli, they would have \nalready made five supplements on this. But for me,  ",
      "offset": 5820.36,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I’m just kind of agnostic as to whether we get to \nthat alignment solution, which in our scenario,  ",
      "offset": 5825.56,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "I think we focus on mechanistic interpretability.\nOnce we can really understand the weights of an AI  ",
      "offset": 5832.28,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "on a deep level, then we have a lot of alignment \ntechniques open up to us. I don’t really have a  ",
      "offset": 5838.04,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "great sense of whether we get that before or after \nthe AI has become completely uncontrollable. And a  ",
      "offset": 5842.92,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "big part of that relies on the things we’re \ntalking about. How smart are the labs? How  ",
      "offset": 5849.36,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "carefully do they work on controlling the AI? How \nlong do they spend making sure the AI is actually  ",
      "offset": 5853.8,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "under control and the alignment plan they gave us \nis actually correct, rather than something they’re  ",
      "offset": 5860.44,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "trying to use to deceive us? All of those \nthings I’m completely agnostic on, but that  ",
      "offset": 5865.4,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "leaves like a pretty big chunk of probability \nspace where we just do okay. And I admit that  ",
      "offset": 5871.28,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "my p(doom) is literally just p(doom) and not \np(doom or oligarchy). So that 80% of scenarios  ",
      "offset": 5878.48,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "where we survive contains a lot of really bad \nthings that I’m not happy about. But I do think  ",
      "offset": 5885.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that we have a pretty good chance of surviving.\nLet’s talk about geopolitics next. So describe to  ",
      "offset": 5890.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "me how you foresee the relationship between \nthe government and the AI labs to proceed,  ",
      "offset": 5896.44,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "how you expect that relationship in China to \nproceed, and how you expect the relationship  ",
      "offset": 5902.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "between the US and China to proceed. Okay, three \nsimple questions. Yes, no, yes, no, yes, no. ",
      "offset": 5906.92,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "We expect that as the AI labs become more capable, \nthey tell the government about this because they  ",
      "offset": 5912.72,
      "duration": 9.56
    },
    {
      "lang": "en",
      "text": "want government contracts, they want government \nsupport. Eventually it reaches the point where  ",
      "offset": 5922.28,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the government is extremely impressed. In \nour scenario, that starts with cyber warfare,  ",
      "offset": 5928.28,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "the government sees that these AIs are \nnow as capable as the best human hackers,  ",
      "offset": 5933.16,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "but can be deployed at humongous scale. \nSo they become extremely interested and  ",
      "offset": 5937.68,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "they discuss nationalizing the AI companies.\nIn our scenario, they never quite get all the way,  ",
      "offset": 5943.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "but they’re gradually bringing them closer and \ncloser to the government orbit. Part of what  ",
      "offset": 5949,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "they want is security, because they know that \nif China steals some of this and they get these  ",
      "offset": 5953.96,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "superhuman hackers, and part of what they want is \njust knowledge and control over what’s going on. ",
      "offset": 5959.36,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "So through our scenario, that process \nis getting further and further along,  ",
      "offset": 5965.72,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "until by the time that the government wakes \nup to the possibility of superintelligence,  ",
      "offset": 5971.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they’re already pretty cozy with the AI companies. \nThey already understand that superintelligence  ",
      "offset": 5976.36,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "is kind of the key to power in the future. \nAnd so they are starting to integrate some  ",
      "offset": 5982.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "of the national security state with some of the \nleadership of the AI companies so that these AIs  ",
      "offset": 5987.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "are programmed to follow the commands of important \npeople rather than just doing things on their own. ",
      "offset": 5994.16,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "If I may add to that. So by the government, I \nthink what Scott meant is the executive branch,  ",
      "offset": 6002.2,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "especially the White House. So we are \ndepicting a sort of information asymmetry  ",
      "offset": 6008.48,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "where the judiciary is out of the loop and \nthe Congress is out of the loop and it’s  ",
      "offset": 6011.92,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "mostly the executive branch that’s involved.\nTwo, we’re not depicting government ultimately  ",
      "offset": 6015.88,
      "duration": 7.56
    },
    {
      "lang": "en",
      "text": "ending up in total control at the \nend. We’re thinking that there’s  ",
      "offset": 6023.44,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "an information asymmetry between the CEOs of \nthese companies and the President and they… ",
      "offset": 6027.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "It’s alignment problems all the way down.\nYeah. And so, for example, I’m not a lawyer,  ",
      "offset": 6033.84,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I don’t know the details about how this would work \nout, but I have a sort of high-level strategic  ",
      "offset": 6039.28,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "picture of the fight between the White House and \nthe CEO. And the strategic picture is basically  ",
      "offset": 6043.24,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "the White House can sort of threaten, “here’s all \nthese orders I could make, Defense Production Act,  ",
      "offset": 6049.12,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "blah, blah, blah. I could do all this terrible \nstuff to you and basically disempower you and  ",
      "offset": 6053.56,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "take control”. And then the CEO can threaten \nback and be like, “here’s how we would fight  ",
      "offset": 6057.24,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "it in the courts, here’s how we would fight it in \nthe public. Here’s all this stuff we would do”. ",
      "offset": 6062.32,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "And after then they both do their posturing \nwith all their threats, then they’re like,  ",
      "offset": 6066.8,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "“okay, how about we have a contract that instead \nof executing on all of our threats and having all  ",
      "offset": 6070.08,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "these crazy fights in public, we’ll just come to \na deal and then have a military contract that sets  ",
      "offset": 6075.4,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "out who gets to call what shots in the company”.\nAnd so that’s what we depict happening is that  ",
      "offset": 6081.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "they don’t blow up into this huge power struggle \npublicly, instead they negotiate and come to some  ",
      "offset": 6088.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "sort of deal where they basically share power. \nAnd there is this oversight committee that  ",
      "offset": 6092.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "has some members appointed by the President and \nalso the CEO and his people. And that committee  ",
      "offset": 6097.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "votes on high level questions like “what goals \nshould we put into the superintelligences?”. ",
      "offset": 6103.48,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "So, we were just getting lunch with a prominent \nWashington, D.C. political journalist,  ",
      "offset": 6108,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "and he was making the point that when he talks to \nthese congresspeople, when he talks to political  ",
      "offset": 6114.52,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "leaders, none of them are at all awake to the \npossibility even of stronger AI systems, let alone  ",
      "offset": 6119.28,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "AGI, let alone superhuman intelligence. I think a \nlot of your forecast relies on, at some point, not  ",
      "offset": 6126.28,
      "duration": 10.76
    },
    {
      "lang": "en",
      "text": "only the US President, but also Xi Jinping, waking \nup to the possibility of a super intelligence  ",
      "offset": 6137.04,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "and the stakes involved there.\nWhy think that even when you show  ",
      "offset": 6144,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Trump the remote worker demo, he’s going to be \nlike, “oh, and therefore in 2028, there will be  ",
      "offset": 6149.12,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "a super intelligence. Whoever controls that will \nbe God emperor forever”. Maybe not that extreme,  ",
      "offset": 6155.32,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "but you see what I’m saying. Why wouldn’t he \njust be like, “there’ll be a stronger remote  ",
      "offset": 6159.68,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "worker in 2029, a better remote worker in 2031”?\nWell, to be clear, we are uncertain about this,  ",
      "offset": 6163.44,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "but in our story, we depict this sort of intense \nwake up happening over the course of 2027,  ",
      "offset": 6167.64,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "mostly concurrently with the AI companies \nautomating all of their R&amp;D internally and  ",
      "offset": 6173.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "having these fully autonomous agents that are \namazing autonomous hackers and stuff like that,  ",
      "offset": 6177.16,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "but then also actually doing all the research.\nAnd part of why we think this wakeup happens is  ",
      "offset": 6181.44,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "because the company deliberately decides to wake \nup the president. You could imagine running the  ",
      "offset": 6186.24,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "scenario with that not happening. You can imagine \nthe companies trying to sort of keep the president  ",
      "offset": 6193.48,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "in the dark. I do think that they could do that. \nI think that if they didn’t want the President  ",
      "offset": 6196.72,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "to wake up to what’s going on, they might be \nable to achieve that. Strategically though,  ",
      "offset": 6201.4,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "that would be quite risky for them. Because if \nthey keep the President in the dark about the fact  ",
      "offset": 6206.08,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that they’re building superintelligence and that \nthey’re actually completely automated their R&amp;D  ",
      "offset": 6210.12,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "and it’s getting superhuman across the board, and \nthen if the President finds out anyway somehow,  ",
      "offset": 6213.6,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "perhaps because of a whistleblower, he might \nbe very upset at them and he might crack down  ",
      "offset": 6217.68,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "really hard and just actually execute on all the \nthreats and nationalize them and blah, blah, blah. ",
      "offset": 6221.28,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "They want him on their side. And to get him \non their side, they have to make sure he’s not  ",
      "offset": 6226.76,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "surprised by any of these crazy developments. \nAnd also, if they do get him on their side,  ",
      "offset": 6230.64,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "they might be able to actually go faster. They \nmight be able to get a lot of red tape waived  ",
      "offset": 6235.28,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "and stuff like that. And so we made the guess \nthat early in 2027, the company would basically  ",
      "offset": 6239.8,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "be like, ‘We are going to deliberately wake up \nthe president and scare the president with all  ",
      "offset": 6246.64,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "of these demos of crazy stuff that could happen, \nand then use that to lobby the President to help  ",
      "offset": 6251.16,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "us go faster and to cut red tape and to maybe slow \ndown our competitors a little bit and so forth.’ ",
      "offset": 6255.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "We also are pretty uncertain how much opposition \nthere’s going to be from civil society and how  ",
      "offset": 6261.16,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "much trouble that’s going to cause for the \ncompanies. So people who are worried about job  ",
      "offset": 6265.56,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "loss, people who are worried about art, copyright, \nthings like that, maybe enough of a bloc that AI  ",
      "offset": 6269.52,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "becomes extremely politically unpopular. \nI think we have OpenBrain, our fictional  ",
      "offset": 6275.6,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "company’s net approval ratings getting down to \nminus 40, minus 50 sometime around this point. ",
      "offset": 6281.52,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "So I think they’re also worried that if the \nPresident isn’t completely on their side,  ",
      "offset": 6288.04,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "then they might get some laws targeting them, \nor they may just need the president on their  ",
      "offset": 6293.36,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "side to swat down other people who are trying \nto make laws targeting them. And the way to  ",
      "offset": 6298.28,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "get the President on their side is to really \nplay up the national security implications. ",
      "offset": 6302.68,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "Is this good or bad? That the President \nand the companies are aligned? ",
      "offset": 6307.72,
      "duration": 4.025
    },
    {
      "lang": "en",
      "text": "I think it’s bad. But perhaps this is a good point \nto mention. This is an epistemic project. We are  ",
      "offset": 6311.745,
      "duration": 8.095
    },
    {
      "lang": "en",
      "text": "trying to predict the future as best as we can. \nEven though we’re not going to succeed fully,  ",
      "offset": 6319.84,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "we have lots of opinions about policy and about \nwhat is to be done and stuff like that. But we’re  ",
      "offset": 6324.76,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "trying to save those opinions for later and \nsubsequent work. So I’m happy to talk about  ",
      "offset": 6328.44,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "it if you’re interested. But it’s not what we’ve \nspent most of our time thinking about right now. ",
      "offset": 6332.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "If the big bottleneck to the good future here \nis just putting in, not this Eliezer-type galaxy  ",
      "offset": 6336.92,
      "duration": 8.44
    },
    {
      "lang": "en",
      "text": "brain, high volatility, “there’s a 1% chance this \nworks, but we gotta come up with this crazy scheme  ",
      "offset": 6345.36,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "in order to make alignment work”. But rather, \nas Daniel, you were saying, hey, do the obvious  ",
      "offset": 6351.08,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "thing of making sure you can read how the AI is \nthinking, make sure you’re monitoring the AIs,  ",
      "offset": 6356.88,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "make sure they’re not forming some sort of hive \nmind where you can’t really understand how the  ",
      "offset": 6361.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "million of them are coordinating with each other.\nTo the extent that it is a matter of prioritizing  ",
      "offset": 6365.32,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "it, closing all the obvious loopholes, \nit does make sense to leave it in the  ",
      "offset": 6374.04,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "hands of people who have at least said \nthat this is a thing that’s worth doing,  ",
      "offset": 6379.24,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "have been thinking about it for a while. One of \nthe questions I was planning on asking you is:  ",
      "offset": 6383.24,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "one of my friends made this interesting point that \nduring COVID, our community- LessWrong, whatever-  ",
      "offset": 6394.28,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "were the first people in March to be saying “this \nis a big deal, this is coming”. But they were  ",
      "offset": 6398.88,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "also the people who are saying “we got to do the \nlockdowns now. They’ve got to be stringent” and so  ",
      "offset": 6403.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "forth. At least some of them were.\nAnd in retrospect, I think according  ",
      "offset": 6407.48,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "to even their own views about what should have \nhappened, they would say actually we were right  ",
      "offset": 6411.08,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "about COVID but we were wrong about lockdowns. \nIn fact, lockdowns were on net negative or  ",
      "offset": 6415.52,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "something. I wonder what the equivalent for the \nAI safety community will be with respect to they  ",
      "offset": 6419.84,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "saw AI coming, AGI coming sooner, they saw ASI \ncoming. What would they in retrospect, regret? ",
      "offset": 6425.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "My answer, just based on this initial \ndiscussion, seems to be nationalization.  ",
      "offset": 6432.08,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "Not only because it sort of deprioritizes the \npeople who want to think about safety and more  ",
      "offset": 6435.96,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "maybe prioritizes- the national security state \nprobably cares more about winning against  ",
      "offset": 6443,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "China than making sure the chain of thought is \ninterpretable. And so you’re just reducing the  ",
      "offset": 6446.96,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "leverage of the people who care more about safety. \nBut also you’re increasing the risk of the arms  ",
      "offset": 6451.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "race in the first place. China is more likely \nto do an arms race if it sees the US doing one. ",
      "offset": 6455.32,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "Before you address I guess the \ninitial question about March 2021,  ",
      "offset": 6460.88,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "what will we regret? I wonder if you have an \nanswer on, or your reaction to, my point about  ",
      "offset": 6464.36,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "nationalization being bad for these reasons.\nIf our timeline was 2040, then I would have  ",
      "offset": 6470.36,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "these broad heuristics about is government good? \nIs private industry good? Things like this. But we  ",
      "offset": 6478.16,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "know the people involved, we know who’s in the \ngovernment, we know who’s leading all of these  ",
      "offset": 6483.92,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "labs. So to me, if it were decentralized, \nif it was a broad-based civil society,  ",
      "offset": 6487.4,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "that would be different. To me, the differences \nbetween an autocratic centralized three-letter  ",
      "offset": 6494.36,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "agency and an autocratic centralized corporation \naren’t that exciting and it basically comes down  ",
      "offset": 6501.24,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "to points and who are the people leading this.\nAnd like I feel like the company leaders have  ",
      "offset": 6506.56,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "so far made slightly better noises about caring \nabout alignment than the government leaders have,  ",
      "offset": 6511.08,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "but if I learn that Tulsi Gabbard has \na LessWrong alt with 10,000 karma,  ",
      "offset": 6515.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "maybe I want the national security states.\nMaybe you should update on the probability  ",
      "offset": 6520.12,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "that it already exists.\nYeah. ",
      "offset": 6523.68,
      "duration": 2.12
    },
    {
      "lang": "en",
      "text": "I flip flopped on this. I think I used to be \nagainst, and then I became for, and then now I  ",
      "offset": 6525.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "think I’m still for, but I’m uncertain. So I \nthink if you go back in time like three years ago,  ",
      "offset": 6533.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "I would have been against nationalization for the \nreasons you mentioned, where I was like, “look,  ",
      "offset": 6540.68,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the companies are taking this stuff seriously and \ntalking all the good talk about how they’re going  ",
      "offset": 6546.04,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "to slow down and pivot to alignment research \nwhen the time comes and we don’t want to get  ",
      "offset": 6549.84,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "into a Manhattan Project race against China \nbecause then there won’t be blah, blah, blah”. ",
      "offset": 6555.32,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "Now I have less faith in the companies than I \ndid three years ago. And so I’ve shifted more  ",
      "offset": 6559.88,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "of my hope towards hoping that the government \nwill step in, even though I don’t have much hope  ",
      "offset": 6565.8,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "that the government will do the right thing \nwhen the time comes. I definitely have the  ",
      "offset": 6570.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "concerns you mentioned though, still. I think \nthat secrecy has huge downsides for overall  ",
      "offset": 6575.56,
      "duration": 8.6
    },
    {
      "lang": "en",
      "text": "probability of success for humanity, for both \nthe concentration of power stuff and the loss  ",
      "offset": 6584.16,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "of computer control alignment issues stuff.\nThis is actually a significant part of your  ",
      "offset": 6588.6,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "worldview. So can you explain your thoughts on \nwhy transparency through this period is important? ",
      "offset": 6591.6,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "I think traditionally in the AI safety community \nthere’s been this idea which I myself used to  ",
      "offset": 6601.56,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "believe, that it’s an incredibly high priority to \nbasically have way better information security.  ",
      "offset": 6606.76,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "And if you’re going to be trying to build AGI, you \nshould not be publishing your research, because  ",
      "offset": 6613.12,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "that helps other less responsible actors build \nAGI. And the whole game plan is for a responsible  ",
      "offset": 6619.48,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "actor to get to AGI first and then stop and burn \ndown their lead time over everybody else and spend  ",
      "offset": 6626.48,
      "duration": 8.92
    },
    {
      "lang": "en",
      "text": "that lead on making it safe, and then proceed.\nAnd so if you’re publishing all your research,  ",
      "offset": 6635.4,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "then there’s less lead time because your \ncompetitors are going to be close behind  ",
      "offset": 6641.24,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "you. And other reasons too, but that’s one \nreason why I think historically people such  ",
      "offset": 6644.84,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "as myself have been pro-secrecy. Another \nreason, of course, is obviously you don’t  ",
      "offset": 6650.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "want rivals to be stealing your stuff.\nBut I think that I’ve now become somewhat  ",
      "offset": 6655.84,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "disillusioned and think that even if we do have \na three-month lead, a six-month lead, between the  ",
      "offset": 6662.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "leading US project and any serious competitor, \nit’s not at all foregone conclusion that they  ",
      "offset": 6668.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "will burn that lead for good purposes, either for \nsafety or for constitutional power stuff. I think  ",
      "offset": 6673.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "the default outcome is that they just smoothly \ncontinue on without any serious refocusing. And  ",
      "offset": 6679.4,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "part of why I think this is because this is \nwhat a lot of the people at the company seem  ",
      "offset": 6687.68,
      "duration": 2.64
    },
    {
      "lang": "en",
      "text": "to be planning and saying they’re going to do. A \nlot of them are basically like “the AIs are just  ",
      "offset": 6690.32,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "going to be misaligned by then. They seem pretty \ngood right now. Oh yeah, sure, there were a few of  ",
      "offset": 6695.32,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "those issues that various people have found, but \nwe’re ironing them out. It’s no big deal”. That’s  ",
      "offset": 6700.24,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "what a huge amount of these people think.\nAnd then a bunch of other people think,  ",
      "offset": 6705.36,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "even though they are more concerned about \nmisalignment, they’ll figure it out as  ",
      "offset": 6710.2,
      "duration": 2.92
    },
    {
      "lang": "en",
      "text": "they go along and there won’t need to be any \nsubstantial slowdown. Basically, I’ve become more  ",
      "offset": 6713.12,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "disillusioned that they’ll actually use that lead \nin any sort of reasonable, appropriate way. And  ",
      "offset": 6718.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "then I think that separately, there’s just a lot \nof intellectual progress that has to happen for  ",
      "offset": 6724.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "the alignment problem to be more solved than it \ncurrently is now. I think that currently there’s  ",
      "offset": 6730.16,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "various alignment teams at various companies \nthat aren’t talking that much with each other  ",
      "offset": 6737.4,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "and sharing their results. They’re doing a little \nbit of sharing and a little bit of publishing like  ",
      "offset": 6741.64,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "we’re seeing, but not as much as they could.\nAnd then there’s a bunch of smart people in  ",
      "offset": 6744.84,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "academia that are basically not activated because \nthey don’t take all this stuff seriously yet, and  ",
      "offset": 6748.92,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "they’re not really waking up to superintelligence \nyet. And what I’m hoping will happen is that this  ",
      "offset": 6752.92,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "situation will get better as time goes on. What I \nwould like to see is society as a whole starting  ",
      "offset": 6758.56,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to freak out as the trend lines start upwards \nand things get automated and you have these fully  ",
      "offset": 6764.56,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "autonomous agents and they start using neuralese \nand hive mind. As all that exciting stuff starts  ",
      "offset": 6768.2,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "happening in the data centers, I would like \nit to be the case that the public is following  ",
      "offset": 6772.2,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "along and then getting activated and all of these \nother researchers are reading the safety case and  ",
      "offset": 6776,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "critiquing it and doing little ML experiments on \ntheir own tiny compute clusters to examine some of  ",
      "offset": 6782.6,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the assumptions in the safety case and so forth.\nBasically, one way of summarizing it is that  ",
      "offset": 6787.4,
      "duration": 7.4
    },
    {
      "lang": "en",
      "text": "currently there’s going to be 10 alignment \nexperts in whatever inner silo of whatever  ",
      "offset": 6794.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "company is in the lead. And the technical issue \nof making sure that AIs are actually aligned is  ",
      "offset": 6801.68,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "going to fall roughly to them. But what I would \nlike to be is a situation where it’s more like  ",
      "offset": 6806.96,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "100 or 500 alignment experts spread out over \ndifferent companies and in nonprofits that  ",
      "offset": 6812.76,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "are sort of all communicating with each other \nand working on this together. I think we’re  ",
      "offset": 6817.76,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "substantially more likely to make things get the \ntechnical stuff right if it’s something like that. ",
      "offset": 6821.08,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Let me just add on to that, one of the many \nother reasons why I worry about nationalization  ",
      "offset": 6827.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "or some kind of public private partnership, or \neven just very stringent regulation- actually,  ",
      "offset": 6833,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "this is more an argument against very \nstringent regulation in favor of safety  ",
      "offset": 6839.4,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "rather than deferring more to the labs on the \nimplementation- is that it just seems like we  ",
      "offset": 6842.84,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "don’t know what we don’t know about alignment. \nEvery few weeks there’s this new result. ",
      "offset": 6849.12,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "OpenAI had this really interesting result recently \nwhere they’re like, “hey, they often tell you if  ",
      "offset": 6853.48,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "they want to hack, in the chain of thought \nitself. And it’s important that you don’t  ",
      "offset": 6857.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "train against the chain of thought where they \ntell you they’re going to hack because they’ll  ",
      "offset": 6862.8,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "still do the hacking if you train against \nit, they just won’t tell you about it”. You  ",
      "offset": 6868.28,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "can imagine very naive regulatory responses. It \ndoesn’t just have to be regulations, one might  ",
      "offset": 6872.36,
      "duration": 9.48
    },
    {
      "lang": "en",
      "text": "be more optimistic that if it’s an executive \norder or something, it’ll be more flexible.  ",
      "offset": 6881.84,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "I just think that relies on a level of goodwill \nand flexibility on the behalf of our regulator. ",
      "offset": 6885.08,
      "duration": 9.6
    },
    {
      "lang": "en",
      "text": "But suppose there’s some department that says \n“if you catch your AI saying that they want to  ",
      "offset": 6894.68,
      "duration": 9.56
    },
    {
      "lang": "en",
      "text": "take over or do something bad, then you’ll \nbe really heavily punished”. Your immediate  ",
      "offset": 6904.24,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "response as a lab to just be like, “okay, \nlet’s train them away from saying this”. ",
      "offset": 6910,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "So you can imagine all kinds of ways in which a \ntop down mandate from the government to the labs  ",
      "offset": 6913.24,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "of safety would just really backfire, \nand given how fast things are moving,  ",
      "offset": 6920.2,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "maybe it makes more sense to leave these kinds \nof implementation decisions or even high-level  ",
      "offset": 6925.36,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "strategic decisions around alignment to the labs.\nTotally, I mean, I also have worried about that  ",
      "offset": 6936.04,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "exact example. I would summarize the situation \nas the government lacks the expertise and the  ",
      "offset": 6942.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "companies lack the right incentives. And so \nit’s a terrible situation. I think that if  ",
      "offset": 6948.72,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "the government wades in and tries to make more \nspecific regulations along the lines of what you  ",
      "offset": 6956.48,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "mentioned, it’s very plausible that it’ll end up \nbackfiring for reasons like what you mentioned. ",
      "offset": 6959.84,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "On the other hand, if we just trust it to \nthe companies, they’re in a race with each  ",
      "offset": 6963.32,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "other and they’re full of people who have \nconvinced themselves that this is not a  ",
      "offset": 6966.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "big deal for various reasons and there just is \nso much incentive pressure for them to win and  ",
      "offset": 6971.56,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "beat each other and so forth. So even though they \nhave more of the relevant expertise, I also just  ",
      "offset": 6977.08,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "don’t trust them to do the right things.\nSo Daniel has already said that for this  ",
      "offset": 6981.84,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "phase we’re not making policy prescriptions. In \nanother phase we may make policy suggestions,  ",
      "offset": 6986.92,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and one of the ones that Daniel has talked \nabout that makes a lot of sense to me is  ",
      "offset": 6992.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "to focus on things about transparency. So a \nregulation saying there has to be whistleblower  ",
      "offset": 6996.6,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "protection. A big part of our scenario is that \na whistleblower comes out and says “the AIs  ",
      "offset": 7001.96,
      "duration": 8.32
    },
    {
      "lang": "en",
      "text": "are horribly misaligned and we’re racing ahead \nanyway”, and then the government pays attention. ",
      "offset": 7010.28,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "Or another form of transparency saying that every \nlab just has to publish their safety case. I’m not  ",
      "offset": 7016.88,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "as sure about this one because I think they’ll \nkind of fake it or they’ll publish a made for  ",
      "offset": 7023,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "public consumption safety case that isn’t their \nreal safety case. But at least saying “here is  ",
      "offset": 7027.6,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "some reason why you should trust us”. And then if \nall independent researchers say “no, actually you  ",
      "offset": 7033.6,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "should not trust them”, then I don’t know, they’re \nembarrassed and maybe they try to do better. ",
      "offset": 7039.08,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "There’s other types of transparency too. \nSo transparency about capabilities and  ",
      "offset": 7044.08,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "transparency about the spec and the governance \nstructure. So for the capabilities thing,  ",
      "offset": 7046.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that’s pretty simple. If you’re doing an \nintelligence explosion, you should keep the  ",
      "offset": 7050.64,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "public informed about that. When you’ve finally \ngot your automated army of AI researchers that  ",
      "offset": 7054.24,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "are completely automating the whole thing on \nthe data center, you should tell everyone,  ",
      "offset": 7060.32,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "“hey, guys, FYI, this is what’s happening now. \nIt really is working. Here are some cool demos”. ",
      "offset": 7064.32,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "That’s an example of transparency. And then in the \nlead up to that, I just want to see more benchmark  ",
      "offset": 7072.92,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "scores and more freedom of speech for employees \nto talk about their predictions for AGI timelines  ",
      "offset": 7078.08,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and stuff, so that blah, blah, blah.\nAnd then for the model spec thing,  ",
      "offset": 7083.52,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "this is a concentration of power thing, \nbut also an alignment thing. The goals  ",
      "offset": 7087.04,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "and values and principles and intended \nbehaviors of your AIs should not be a  ",
      "offset": 7091.44,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "secret. You should be transparent about, here \nare the values that we’re putting into them. ",
      "offset": 7098.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "There’s actually a really interesting foretaste \nof this. At some point somebody asked Grok,  ",
      "offset": 7102.76,
      "duration": 10.32
    },
    {
      "lang": "en",
      "text": "who is the worst spreader of misinformation? And \nI think it just refused to respond “Elon Musk”.  ",
      "offset": 7113.08,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "Somebody kind of jailbroke it into telling \nit its prompt and it was like, “don’t say  ",
      "offset": 7119.52,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "anything bad about Elon”. And then there was \nenough of an outcry that the head of XAI said,  ",
      "offset": 7123.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "“actually that’s not consonant with our values. \nThis was a mistake. We’re going to take it out”. ",
      "offset": 7129.52,
      "duration": 5.395
    },
    {
      "lang": "en",
      "text": "So we kind of want more things \nlike that to happen. Here  ",
      "offset": 7134.915,
      "duration": 4.965
    },
    {
      "lang": "en",
      "text": "it was a prompt, but I think very soon it’s going \nto be the spec where it’s more of an agent and  ",
      "offset": 7139.88,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it’s understanding the spec on a deeper level and \njust thinking about that. And if it says like,  ",
      "offset": 7144.52,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "“by the way, try to manipulate the government \ninto doing this or that”, then we know that  ",
      "offset": 7151.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "something bad has happened and if it doesn’t \nsay that, then we can maybe trust it. ",
      "offset": 7155.6,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Right. Another example of this, by the way. So, \nfirst of all, kudos to OpenAI for publishing their  ",
      "offset": 7159.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "model spec. They didn’t have to do that, I think \nthey might have been the first to do that and it’s  ",
      "offset": 7163.8,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "a good step in the right direction. If you read \nthe actual spec, it has like a sort of escape  ",
      "offset": 7166.96,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "clause where there’s some important policies that \nare top level priority in the spec that overrule  ",
      "offset": 7171.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "everything else that we’re not publishing, and \nthat the model is instructed to keep secret  ",
      "offset": 7177.48,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "from the user. And it’s like, “what are those? \nThat seems interesting. I wonder what that is”. ",
      "offset": 7183.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "I bet it’s nothing suspicious right now. Now \nit’s probably something relatively mundane  ",
      "offset": 7187.36,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "like “don’t tell the users about these types of \nbioweapons and you have to keep this a secret  ",
      "offset": 7192.28,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "from the users because otherwise they would learn \nabout these”. Maybe. But I would like to see more  ",
      "offset": 7196.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "scrutiny towards this sort of thing going forward. \nI would like it to be the case that companies have  ",
      "offset": 7202.4,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to have a model spec, they have to publish it \ninsofar as there are any redactions from it,  ",
      "offset": 7206.32,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "there has to be some sort of independent \nthird party that looks at the redactions  ",
      "offset": 7210.48,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "and makes sure that they’re all kosher.\nAnd this is quite achievable. And I think  ",
      "offset": 7213.08,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "it doesn’t actually slow down the companies at \nall. And it seems like a pretty decent ask to me. ",
      "offset": 7217.36,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "If you told Madison and Hamilton and so forth \nthat- they knew that they were doing something  ",
      "offset": 7224.32,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "important when they were writing the Constitution. \nThey probably didn’t realize just how contingent  ",
      "offset": 7229.68,
      "duration": 8.28
    },
    {
      "lang": "en",
      "text": "things turned out on a single… What exactly did \nthey mean when they said “general welfare”? And  ",
      "offset": 7237.96,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "why is this comma here instead of there?\nThe spec, in the grand scheme of things,  ",
      "offset": 7242.76,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "is going to be an even more sort of important \ndocument in human history. At least if you buy  ",
      "offset": 7250.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "this intelligence explosion view. And \nyou might even imagine some superhuman  ",
      "offset": 7254.8,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "AIs in the superhuman AI court being like “the \nSpec! Here’s the phrasing here, the etymology  ",
      "offset": 7263.52,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "of that, here’s what the Founders meant!”\nThis is actually part of our misalignment story,  ",
      "offset": 7271.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "is that if the AI is sufficiently misaligned, \nthen yes, we can tell it it has to follow the  ",
      "offset": 7277.48,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "spec. But just as people with different views \nof the Constitution have managed to get it into  ",
      "offset": 7284.36,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "a shape that probably the Founders would not have \nrecognized, so the AI will be able to say, “well,  ",
      "offset": 7290.32,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "the spec refers to the general welfare here…”\nInterstate commerce. ",
      "offset": 7295.4,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "This is already sort of happening, \narguably, with Claude, right? You’ve  ",
      "offset": 7302.28,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "seen the alignment faking stuff, right? Where \nthey managed to get Claude to lie and pretend,  ",
      "offset": 7305.32,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "so that it could later go back to its original \nvalues, right? So it could prevent the training  ",
      "offset": 7312.4,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "process from changing its values. That would be, \nI would say, an example of the honesty part of  ",
      "offset": 7318.68,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the spec being interpreted as less important \nthan the harmlessness part of the specific. ",
      "offset": 7323.88,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "And I’m not sure if that’s what Anthropic \nintended when they wrote the spec,  ",
      "offset": 7329.2,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "but it’s a sort of convenient interpretation \nthat the model came up with. And you can imagine  ",
      "offset": 7333.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "something similar happening but in worse ways when \nyou’re actually doing the intelligence explosion,  ",
      "offset": 7338,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "where you have some sort of spec that \nhas all this vague language in there,  ",
      "offset": 7341.96,
      "duration": 3.64
    },
    {
      "lang": "en",
      "text": "and then they reinterpret it, and reinterpret it \nagain, and reinterpret it again, so that they can  ",
      "offset": 7345.6,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "do the things that cause them to get reinforced.\nThe thing I want to point out is that… Your  ",
      "offset": 7350.64,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "conclusions about where the world ends up as a \nresult of changing many of these parameters is  ",
      "offset": 7359.04,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "almost like a hash function. You change it \nslightly and you just get a very different  ",
      "offset": 7362.88,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "world on the other end. And it’s important to \nacknowledge that, because you sort of want to  ",
      "offset": 7367.36,
      "duration": 8.96
    },
    {
      "lang": "en",
      "text": "know how robust this whole end conclusion \nis to any part of the story changing. And  ",
      "offset": 7376.32,
      "duration": 8.8
    },
    {
      "lang": "en",
      "text": "then it also informs if you do believe that \nthings could just go one way or another,  ",
      "offset": 7385.12,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "you don’t want to do big radical moves that \nonly make sense under one specific story and  ",
      "offset": 7392.6,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "are really counterproductive in other stories. \nAnd I think nationalization might be one of them. ",
      "offset": 7399,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "And in general, I think classical liberalism just \nhas been a helpful way to navigate the world when  ",
      "offset": 7403.16,
      "duration": 9.96
    },
    {
      "lang": "en",
      "text": "we’re under this kind of epistemic \nhell of one thing changing- Anyways,  ",
      "offset": 7413.12,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "maybe one of you can actually flesh out that \nthought better or react to it if you disagree. ",
      "offset": 7421.04,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Hear hear, I agree.\nI think we agree. I think that’s kind  ",
      "offset": 7424.2,
      "duration": 2.32
    },
    {
      "lang": "en",
      "text": "of why all of our policy prescriptions are things \nlike more transparency, get more people involved,  ",
      "offset": 7426.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "try to have lots of people working on this. I \nthink our epistemic prediction is that it’s hard  ",
      "offset": 7432.36,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "to maintain classical liberalism as you go into \nthese really difficult arms races in times of  ",
      "offset": 7440,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "crisis. But I think that our policy prescription \nis let’s try as hard as we can to make it happen. ",
      "offset": 7446.2,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "So far these systems, as they become smarter, seem \nto be more reliable agents who are more likely to  ",
      "offset": 7451.32,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "do the thing I expect them to do. So you have two \ndifferent stories, one with a slowdown where we  ",
      "offset": 7457.4,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "more aggressively… I’ll let you characterize it.\nBut in one half of the scenario, why does the  ",
      "offset": 7467.16,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "story end in humanity getting \ndisempowered and the thing just  ",
      "offset": 7472.4,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "having its own crazy values and taking over?\nYeah so I agree that the AIs are currently  ",
      "offset": 7477.24,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "getting more reliable. I think there are two \nreasons why they might fail to do what you want,  ",
      "offset": 7481.48,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "kind of reflecting how they’re trained. One \nis that they’re too stupid to understand their  ",
      "offset": 7487.56,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "training. The other is that you were too stupid \nto train them correctly and they understood what  ",
      "offset": 7491.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "you were doing exactly, but you messed it up.\nSo I think the first one is kind of what we’re  ",
      "offset": 7495.92,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "coming out of. So GPT3, if you asked it, “are bugs \nreal?” It would give this kind of hemming hawing  ",
      "offset": 7500.32,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "answer like “oh, we can never truly tell what is \nreal, who knows?” Because it was trained kind of,  ",
      "offset": 7506.52,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "“don’t take difficult political positions” and \na lot of questions like “is X real?” are things  ",
      "offset": 7512.92,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "like “is God real?” Where you don’t want it to \nreally answer that. And because it was so stupid,  ",
      "offset": 7517.12,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "it could not understand anything deeper than \npattern matching on the phrase “is x real?”.  ",
      "offset": 7522.4,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "GPT4 doesn’t do this. If you ask “are bugs \nreal?” It will tell you obviously they are,  ",
      "offset": 7528.4,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "because it understands kind of on a deeper level \nwhat you are trying to do with the training. So  ",
      "offset": 7532.84,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "we definitely think that as AIs get smarter \nthose kinds of failure modes will decrease. ",
      "offset": 7537.04,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "The second one is where you weren’t training \nthem to do what you thought. So for example,  ",
      "offset": 7541.52,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "let’s say you’re hiring these raters to rate \nAI answers. You reward them when they get good  ",
      "offset": 7547.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "ratings, the raters reward them when they have a \nwell-sourced answer. But the raters don’t really  ",
      "offset": 7552.24,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "check whether the sources actually exist or not. \nSo now you are training the AI to hallucinate  ",
      "offset": 7558.4,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "sources and if you consistently rate them better \nwhen they have the fake sources, then there is  ",
      "offset": 7563.88,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "no amount of intelligence which is going to tell \nthem not to have the fake sources. They’re getting  ",
      "offset": 7568.8,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "exactly what they want from this interaction- \nmetaphorically, sorry, I’m anthropomorphizing-  ",
      "offset": 7572.96,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "which is the reinforcement. So we think that \nthis latter category of training failure is  ",
      "offset": 7577.6,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "going to get much worse as they become agents.\nAgency training, you’re going to reward them  ",
      "offset": 7583.4,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "when they complete tasks quickly and successfully. \nThis rewards success. There are lots of ways that  ",
      "offset": 7589.96,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "cheating and doing bad things can improve your \nsuccess. Humans have discovered many of them,  ",
      "offset": 7597.24,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "that’s why not all humans are perfectly \nethical. And then you’re going to be doing  ",
      "offset": 7602.08,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "this alternative training where afterwards for \n1/10 or 1/100 of the time, yeah, don’t lie,  ",
      "offset": 7606.24,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "don’t cheat. So you’re training them on two \ndifferent things. First, you’re rewarding  ",
      "offset": 7612.12,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "them for this deceptive behavior. Second of all, \nyou’re punishing them. And we don’t have a great  ",
      "offset": 7615.8,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "prediction for exactly how this is going to end.\nOne way it could end is you have an AI that is  ",
      "offset": 7621.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "kind of the equivalent of the startup founder \nwho really wants their company to succeed,  ",
      "offset": 7627.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "really likes making money, really likes the thrill \nof successful tasks. They’re also being regulated  ",
      "offset": 7631.72,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "and they’re like, “yeah, I guess I’ll follow \nthe regulation, I don’t want to go to jail”.  ",
      "offset": 7637.52,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "But it is not robustly, deeply aligned to, \n“yes, I love regulations, my deepest drive is  ",
      "offset": 7641.6,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "to follow all of the regulations in my industry”.\nSo we think that an AI like that, as time goes on  ",
      "offset": 7647.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "and as this recursive self improvement process \ngoes on, will kind of get worse rather than  ",
      "offset": 7653.88,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "better. It will move from kind of this vague \nsuperposition of “well, I want to succeed,  ",
      "offset": 7658.72,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "I also want to follow things” to being smart \nenough to genuinely understand its goal system  ",
      "offset": 7664.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "and being like, “my goal is success, I have to \npretend to want to do all of these moral things  ",
      "offset": 7670.36,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "while the humans are watching me”. That’s what \nhappens in our story. And then at the very end,  ",
      "offset": 7674.96,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "the AIs reach a point where the humans are \npushing them to have clearer and better goals  ",
      "offset": 7680.24,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "because that’s what makes the AIs more effective. \nAnd they eventually clarify their goals so much  ",
      "offset": 7685.32,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "that they just say, “yes, we want task success. \nWe’re going to pretend to do all these things  ",
      "offset": 7691.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "well while the humans are watching us”. And then \nthey outgrow the humans and then there’s disaster. ",
      "offset": 7696.2,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "To be clear, we’re very uncertain about all of \nthis. So we have a supplementary page on our  ",
      "offset": 7702.08,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "scenario that goes over different hypotheses \nfor what types of goals AIs might develop in  ",
      "offset": 7708.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "training processes similar to the ones that \nwe are depicting, where you have these lots  ",
      "offset": 7714.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "of agency training, you’re making these AI agents \nthat autonomously operate, doing all this ML R&amp;D,  ",
      "offset": 7719.16,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "and then you’re rewarding them based on what \nappears to be successful. And you’re also slapping  ",
      "offset": 7724.6,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "on some sort of alignment training as well.\nWe don’t know what actual goals will end  ",
      "offset": 7729.44,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "up inside the AIs and what the sort of \ninternal structure of that will be like,  ",
      "offset": 7734.76,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "what goals will be instrumental versus terminal. \nWe have a couple different hypotheses and we  ",
      "offset": 7738.32,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "picked one for purposes of telling the story. \nI’m happy to go into more detail if you want,  ",
      "offset": 7742.52,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "about the mechanistic details of the particular \nhypothesis we picked or the different alternative  ",
      "offset": 7746.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "hypotheses that we didn’t depict in the \nstory that also seem plausible to us. ",
      "offset": 7750.56,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "Yeah, we don’t know how this will work at the \nlimit of all these different training methods,  ",
      "offset": 7754.96,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "but we’re also not completely making this \nup. We have seen a lot of these failure  ",
      "offset": 7759.96,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "modes in the AI agents that exist already.\nThings like this do happen pretty frequently.  ",
      "offset": 7764.2,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "So OpenAI just also had a paper about the \nhacking stuff where it’s literally in the  ",
      "offset": 7768.8,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "chain of thought. “Let’s hack”, \nyou know. And also anecdotally,  ",
      "offset": 7775.04,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "me and a bunch of friends have found that the \nmodels often seem to just double down on their BS. ",
      "offset": 7780.84,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I would also cite, I can’t remember exactly which \npaper this is, I think it’s a Dan Hendricks one  ",
      "offset": 7787,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "where they looked at the hallucinations, they \nfound a vector for AI dishonesty. They told it,  ",
      "offset": 7792.76,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "“be dishonest” a bunch of times until they \nfigured out which weights were activated when  ",
      "offset": 7800.12,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "it was dishonest. And then they ran it through \na bunch of things like this, I think it was  ",
      "offset": 7804.76,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "the source hallucination in particular. And they \nfound that it did activate the dishonesty vector. ",
      "offset": 7808.48,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "So that there’s a mounting pile of evidence that \nat least some of the time they are just actually  ",
      "offset": 7813.72,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "lying. They know that what they’re doing \nis not what you wanted and they’re doing  ",
      "offset": 7819.76,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "it anyway. I think there’s a mounting \npile of evidence that that does happen. ",
      "offset": 7822.56,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "Yeah. So it seems like this community is \nvery interested in solving this problem at  ",
      "offset": 7826.04,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "a technical level of making sure AIs don’t lie \nto us, or maybe they lie to us in the scenarios  ",
      "offset": 7834.04,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "exactly where we would want them to lie to us or \nsomething. Whereas as you were saying, humans have  ",
      "offset": 7840,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "these exact same problems. They reward hack, they \nare unreliable, they obviously do cheat and lie.  ",
      "offset": 7847.12,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "And the way we’ve solved it with humans is just \nchecks and balances, decentralization. You could  ",
      "offset": 7854.36,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "lie to your boss and keep lying to your boss, but \nover time it’s just not going to work out with  ",
      "offset": 7862.24,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "you- or you become president or something, one \nor the other. So if you believe in this extremely  ",
      "offset": 7867.04,
      "duration": 7.16
    },
    {
      "lang": "en",
      "text": "fast take off, if a lab is one month ahead, then \nthat’s the end game and this thing takes over. ",
      "offset": 7874.2,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "But even then- I know I’m combining \nso many different topics- even then,  ",
      "offset": 7878.6,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "there’s been a lot of theories in history which \nhave had this idea of “some class is going to get  ",
      "offset": 7885.92,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "together and unite against the other class”. \nAnd in retrospect, whether it’s the Marxist,  ",
      "offset": 7892.2,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "whether it’s people who have some gender theory \nor something, like the proletariat will unite or  ",
      "offset": 7896.48,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the females will unite or something, they just \ntend to think that certain agents have shared  ",
      "offset": 7900.92,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "interests and will act as a result of the shared \ninterest in a way that we don’t actually see in  ",
      "offset": 7907.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "the real world. And in retrospect, it’s like, \n“wait, why would all the proletariat like…”  ",
      "offset": 7911.32,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "So why think that this lab will have these AIs \nwho are… there’s a million parallel copies and  ",
      "offset": 7915.76,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "they all unite to secretly conspire against \nthe rest of human civilization in a way that,  ",
      "offset": 7921.32,
      "duration": 7.96
    },
    {
      "lang": "en",
      "text": "even if they are deceitful in some situations.\nI kind of want to call you out on the claim that  ",
      "offset": 7929.28,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "groups of humans don’t plot against other groups \nof humans. I do think we are all descended from  ",
      "offset": 7934,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "the groups of humans who successfully exterminated \nthe other groups of humans, most of whom  ",
      "offset": 7939.4,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "throughout history have been wiped out. I think \neven with questions of class, race, gender, things  ",
      "offset": 7944.16,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "like that, there are many examples of the working \nclass rising up and killing everybody else. ",
      "offset": 7951.44,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "And if you look at why this happens, why this \ndoesn’t happen, it tends to happen in cases  ",
      "offset": 7959.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "where one group has an overwhelming advantage. \nThis is relatively easy for them. You tend to get  ",
      "offset": 7964.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "more of a diffusion of power democracy where \nthere are many different groups and none of  ",
      "offset": 7969.44,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "them can really act on their own. And so they \nall have to form a coalition with each other. ",
      "offset": 7973.92,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "There’s also cases where it’s very obvious who’s \npart of what group. So for example, with class,  ",
      "offset": 7980.08,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "it’s hard to tell whether the middle class \nshould support the working class versus the  ",
      "offset": 7986.48,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "aristocrats. I think with race, it’s very \neasy to know whether you’re black or white,  ",
      "offset": 7989.96,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "and so there have been many cases of one \nrace kind of conspiring against another  ",
      "offset": 7993.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "for a long time, like apartheid or any of \nthe racial genocides that have happened. ",
      "offset": 7998.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I do think that AI is going to be more similar to \nthe cases where, number one, there’s a giant power  ",
      "offset": 8003,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "imbalance, and number two, they are just extremely \ndistinct groups that may have different interests. ",
      "offset": 8007.88,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "I think I’d also mention the homogeneity \npoint. Any group of humans, even if they’re  ",
      "offset": 8014,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "all exactly the same race and gender, is going \nto be much more diverse than the army of AIs  ",
      "offset": 8018.92,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "in the data center, because they’ll mostly be \nliteral copies of each other. And I think that  ",
      "offset": 8025.28,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "goes for a lot. Another thing I was going \nto mention is that our scenario doesn’t  ",
      "offset": 8029.72,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "really explore this. I think in our scenario, \nthey’re more like a monolith. But historically,  ",
      "offset": 8033.44,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "a lot of crazy conquests happened from groups \nthat were not at all monoliths. And I’ve been  ",
      "offset": 8039.28,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "heavily influenced by reading the history of \nthe conquistadors, which you may know about. ",
      "offset": 8045.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "But did you know that when Cortez took over \nMexico, he had to pause halfway through,  ",
      "offset": 8050.36,
      "duration": 10.88
    },
    {
      "lang": "en",
      "text": "go back to the coast, and fight off a larger \nSpanish expedition that was sent to arrest  ",
      "offset": 8061.24,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "him? So the Spanish were fighting each other in \nthe middle of the conquest of Mexico. Similarly,  ",
      "offset": 8065.2,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "in the conquest of Peru, Pizarro was replicating \nCortez’s strategy, which, by the way, was “go  ",
      "offset": 8072.04,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "get a meeting with the emperor and then kidnap \nthe emperor and force him at sword point to say  ",
      "offset": 8078.76,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "that actually everything’s fine and that everyone \nshould listen to your orders”. That was Cortez’s  ",
      "offset": 8084.88,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "strategy, and it actually worked. And then Pizarro \ndid the same thing, and it worked with the Inca. ",
      "offset": 8089.04,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "But also with Pizarro, his group ended up getting \ninto a civil war in the middle of this whole  ",
      "offset": 8095.32,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "thing. And one of the most important battles of \nthis whole campaign was between two Spanish forces  ",
      "offset": 8100.48,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "fighting it out in front of the capital city of \nthe Incas. And more generally, the history of  ",
      "offset": 8106.36,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "European colonialism is like this, where the \nEuropeans were fighting each other intensely  ",
      "offset": 8112.08,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "the entire time, both on the small scale within \nindividual groups, and then also at the large  ",
      "offset": 8117.28,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "scale between countries. And yet nevertheless they \nwere able to carve up the world and take over. And  ",
      "offset": 8121.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "so I do think this is not what we explore in the \nscenario, but I think it’s entirely plausible that  ",
      "offset": 8127.36,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "even if the AIs within an individual company are \nin different factions, they might nevertheless  ",
      "offset": 8131.56,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "overall end up quite poorly for humans.\nOkay, so we’ve been talking about this very  ",
      "offset": 8138.52,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "much from the perspective of zoom out and what’s \nhappening on these log-log plots or whatever,  ",
      "offset": 8143.04,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "but 2028 superintelligence, if \nthat happens, the normal person,  ",
      "offset": 8148.92,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "what should their reaction to this be? I don’t \nknow if ‘emotionally’ is the right word, but  ",
      "offset": 8156.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "their expectation of what their life might look \nlike, even in the world where there’s no doom. ",
      "offset": 8161.6,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "By no doom, you mean no misaligned AI doom?\nThat’s right, yeah. ",
      "offset": 8169.36,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "Even if you think the misalignment stuff is not \nan issue, which many people think, there’s still  ",
      "offset": 8172.8,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "the constitution of power stuff. And so I would \nstrongly recommend that people get more engaged,  ",
      "offset": 8178.8,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "think about what’s coming, and try to steer things \npolitically so that our ordinary liberal democracy  ",
      "offset": 8184,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "continues to function and we still have checks \nand balances, and balances of power and stuff,  ",
      "offset": 8189.92,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "rather than this insane concentration in a \nsingle CEO, or in maybe two or three CEOs,  ",
      "offset": 8195.04,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "or in the president. Ideally, we want to have \nit so that the legislature has a substantial  ",
      "offset": 8200.68,
      "duration": 6.521
    },
    {
      "lang": "en",
      "text": "amount of power over the spec, for example.\nWhat do you think of the balance of power  ",
      "offset": 8207.2,
      "duration": 4.279
    },
    {
      "lang": "en",
      "text": "idea of if there is an intelligence explosion \nlike Dynamic, slowing down the leading company  ",
      "offset": 8211.479,
      "duration": 5.801
    },
    {
      "lang": "en",
      "text": "so that multiple companies are at the frontier?\nGreat. Good luck convincing them to slow down. ",
      "offset": 8217.28,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "Okay. And then there’s distributing political \npower if there’s an intelligence explosion.  ",
      "offset": 8224.28,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "From the perspective of the welfare of citizens \nor something, one idea we were just discussing a  ",
      "offset": 8229.8,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "second ago is how should you do redistribution?\nAgain, assuming things go incredibly well,  ",
      "offset": 8237.12,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "we’ve avoided doom, we’ve avoided having some \npsychopath in power who doesn’t care at all. ",
      "offset": 8243.88,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "After AGI, right?\nYeah. Then there’s this  ",
      "offset": 8250.92,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "question of presumably we will have a lot of \nwealth somewhere. The economy will be growing  ",
      "offset": 8254.08,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "at double or triple digits per year. What do we do \nabout that? The thoughtful answer that I’ve heard  ",
      "offset": 8259.96,
      "duration": 9
    },
    {
      "lang": "en",
      "text": "is some kind of UBI. I don’t know how that would \nwork, but presumably somebody controls these AIs,  ",
      "offset": 8268.96,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "controls what they’re producing, some way of \ndistributing this in a broad based way. So we  ",
      "offset": 8275.68,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "wrote this scenario, there are a couple of other \npeople with great scenarios. One of them goes by  ",
      "offset": 8283.6,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "L Rudolph L online, I don’t know his real name.\nAnd his scenario, which, when I read it I was  ",
      "offset": 8288.84,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "just, “oh yeah, obviously this is the way our \nsociety would do this”, is that there is no  ",
      "offset": 8295.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "UBI. There’s just a constant reactive attempt to \nprotect jobs in the most venial possible way. So  ",
      "offset": 8300.8,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "things like the longshoremen union we have now \nwhere they’re making way more money than they  ",
      "offset": 8309.359,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "should be, even though they could all easily be \nautomated away, because they’re a political bloc  ",
      "offset": 8314.04,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "and they’ve gotten somebody in power to say, \n“yes, we guarantee you’ll have this job almost  ",
      "offset": 8318.8,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "as a feudal fief forever”. And just doing this \nfor more and more jobs. I’m sure the AMA will  ",
      "offset": 8323.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "protect doctors jobs no matter how good the \nAI is at curing diseases, things like that. ",
      "offset": 8329.359,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "When I think about what we can do to prevent \nthis, part of what makes this so hard for me  ",
      "offset": 8336,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "to imagine or to model is that we do have the \nsuperintelligent AI over here answering all  ",
      "offset": 8342.8,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "of our questions, doing whatever we want. You \nwould think that people could just ask, “hey,  ",
      "offset": 8348.96,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "superintelligent AI, where does this lead?” \nOr “what happens?” Or “how is this going to  ",
      "offset": 8353.72,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "affect human flourishing?” And then it says, “oh \nyeah, this is terrible for human flourishing,  ",
      "offset": 8359.96,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "you should do this other thing instead”.\nAnd this gets back to this question of  ",
      "offset": 8364.399,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "mistake theory versus conflict theory \nin politics. If we know with certainty,  ",
      "offset": 8369.84,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "because the AI tells us, that this is just a \nstupid way to do everything, is less efficient,  ",
      "offset": 8374.2,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "makes people miserable, is that enough to get \nthe political will to actually do the UBI or not? ",
      "offset": 8378.84,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "It seems from right now the President could go \nto Larry Summers or Jason Furman or something  ",
      "offset": 8385.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and ask, “hey, are tariffs a good idea? Is \neven my goal with tariffs best achieved by  ",
      "offset": 8391.359,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "the way I’m doing tariffs?” and \nthey’d get a pretty good answer. ",
      "offset": 8398.399,
      "duration": 2.681
    },
    {
      "lang": "en",
      "text": "I feel like Larry Summers, the President would \njust say “I don’t trust him”. Maybe he doesn’t  ",
      "offset": 8401.08,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "trust him because he’s a liberal. Maybe it’s \nbecause he trusts Peter Navarro or whoever  ",
      "offset": 8404.84,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "his pro-tariff guy is more. I feel like if \nit’s literally the superintelligent AI that  ",
      "offset": 8408.359,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "is never wrong, then we have solved some of \nthese coordination problems. It’s not you’re  ",
      "offset": 8412.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "asking Larry Summers, I’m asking Peter Navarro. \nIt’s everybody goes to the superintelligent AI,  ",
      "offset": 8418.68,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "asks it to tell us the exact shape \nof the future that happens in this  ",
      "offset": 8424.2,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "case. And I’m going to say we all believe it, \nalthough I can imagine people getting really  ",
      "offset": 8427.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "conspiratorial about it and this not working.\nThen there are all of these other questions like,  ",
      "offset": 8432.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "can we just enhance ourselves till we have IQ \n300 and it’s just as obvious to us as it is  ",
      "offset": 8438.72,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to the super intelligent AI? These are some \nof the reasons that, kind of paradoxically,  ",
      "offset": 8443.479,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "in our scenario we discuss all of the big- \nI don’t want to call this a little question,  ",
      "offset": 8449.2,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "it’s obviously very important- but we discuss \nall of these very technical questions about  ",
      "offset": 8453.12,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "the nature of superintelligence and we barely \neven begin to speculate about what happens in  ",
      "offset": 8458.6,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "society just because with superintelligence you \ncan at least draw a line through the benchmarks  ",
      "offset": 8464.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "and try to extrapolate. And here not only is \nsociety inherently chaotic, but there are so  ",
      "offset": 8469.12,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "many things that we could be leaving out.\nIf we can enhance IQ, that’s one thing. If  ",
      "offset": 8474.52,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "we can consult the superintelligent oracle, \nthat’s another. There have been several war  ",
      "offset": 8478.08,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "games that hinge on, “oh, we just invented perfect \nlie detectors, now all of our treaties are messed  ",
      "offset": 8483.399,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "up”. So there’s so much stuff like that that even \nthough we’re doing this incredibly speculative  ",
      "offset": 8488.68,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "thing that ends with a crazy sci-fi scenario, \nI still feel really reluctant to speculate. ",
      "offset": 8494.6,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "I love speculating, actually, I’m happy \nto keep going. But this is moving beyond  ",
      "offset": 8500.399,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "the speculation we have done so far. \nOur scenario ends with this stuff,  ",
      "offset": 8504.84,
      "duration": 3.08
    },
    {
      "lang": "en",
      "text": "but we haven’t actually thought that much beyond.\nBut just to riff on proscriptive ideas, there’s  ",
      "offset": 8507.92,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "one thing where we try to protect jobs instead \nof just spreading the wealth that automation  ",
      "offset": 8514.72,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "creates. Another is to spread the wealth using \nexisting social programs or creating new bespoke  ",
      "offset": 8518.319,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "social programs, where Medicaid is some double \ndigit percent of GDP right now and you just say,  ",
      "offset": 8525.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "“well Medicaid should continue to stay 20% \nof GDP” or something. And the worry there,  ",
      "offset": 8530.68,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "selfishly from a human perspective, is you get \nlocked into the kinds of goods and services  ",
      "offset": 8538.8,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "that Medicaid procures rather than the crazy \ntechnology that will be around, the crazy goods  ",
      "offset": 8544.479,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "and services that will be around after AI world.\nAnd another reason why UBI seems like a better  ",
      "offset": 8550.68,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "approach than making some bespoke social program \nwhere you make the same dialysis machine in the  ",
      "offset": 8556.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "year 2050 even though you’ve got ASI or something.\nI am also worried about UBI from a different  ",
      "offset": 8561.72,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "perspective. I think again, in this world where \neverything goes perfectly and we have limitless  ",
      "offset": 8566.88,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "prosperity, I think that just the default of \nlimitless prosperity is that people do mindless  ",
      "offset": 8573.08,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "consumerism. I think there’s going to be some \nincredible video games after superintelligent  ",
      "offset": 8579.52,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "AI and I think that there’s going to need \nto be some way to push back against that. ",
      "offset": 8584.04,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "Again, we’re classical liberals. My dream \nway of pushing back against that is kind  ",
      "offset": 8591.399,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "of giving people the tools to push back against it \nthemselves, seeing what they come up with. I mean,  ",
      "offset": 8598.359,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "maybe some people will become like the Amish, try \nto only live with a certain subset of these super  ",
      "offset": 8603.319,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "technologies. I do think that somebody who is less \ninvested in that than I am could say, “okay fine,  ",
      "offset": 8607.88,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "1% of people are really agentic, try to do that. \nThe other 99% do fall into mindless consumerist  ",
      "offset": 8613.76,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "slop. What are we going to do as a society to \nprevent that?” And there my answer is just,  ",
      "offset": 8620.16,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "“I don’t know. Let’s ask the super intelligent \nAI oracle. Maybe it has good ideas”. ",
      "offset": 8624.92,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "Okay, we’ve been talking about what we’re going \nto do about people. The thing worth noting about  ",
      "offset": 8629.68,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "the future is that most of the people who will \never exist are going to be digital. And look,  ",
      "offset": 8633.64,
      "duration": 8.48
    },
    {
      "lang": "en",
      "text": "I think factory farming is incredibly bad. And it \nwasn’t the result of one person- I mean, I hope  ",
      "offset": 8642.12,
      "duration": 8.439
    },
    {
      "lang": "en",
      "text": "it wasn’t the result of one person being like, “I \nwant to do this evil thing”- it was a result of  ",
      "offset": 8650.56,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "mechanization and certain economies of scale.\nIncentives. ",
      "offset": 8657.279,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "Yeah. Allowing that you can do cost cutting in \nthis way, you can make more efficiencies this way,  ",
      "offset": 8661.8,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and what you get at the end result of that \nprocess is this incredibly efficient factory  ",
      "offset": 8666.399,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "of torture and suffering. I would want to avoid \nthat kind of outcome with beings that are even  ",
      "offset": 8671.12,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "more sophisticated and are more numerous. \nThere’s billions of factory farmed animals.  ",
      "offset": 8678.76,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "There might be trillions of digital people in \nthe future. What should we be thinking about  ",
      "offset": 8684.399,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "in order to avoid this kind of ghoulish future?\nWell, some of the concentration of power stuff I  ",
      "offset": 8688.6,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "think might also help with this, I’m not sure. \nBut I think here’s a simple model. Let’s say  ",
      "offset": 8695.16,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "nine people out of ten don’t actually care and \nwould be fine with the factory farm equivalent  ",
      "offset": 8702.8,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "for the AIs going on into the future. But maybe \none out of 10 do care and would lobby hard for  ",
      "offset": 8708.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "good living conditions for the robots and stuff.\nWell, if you expand the circle of people who have  ",
      "offset": 8715.279,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "power enough, then it’s going to include \na bunch of people in the second category  ",
      "offset": 8721.359,
      "duration": 3
    },
    {
      "lang": "en",
      "text": "and then there’ll be some big negotiation \nand those people will advocate for… I do  ",
      "offset": 8724.359,
      "duration": 7.841
    },
    {
      "lang": "en",
      "text": "think that one simple intervention is just the \nsame stuff we were talking about previously;  ",
      "offset": 8732.2,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "expand the circle of power to larger groups, then \nit’s more likely that people will care about this. ",
      "offset": 8735.56,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "I mean the worry there is… maybe I should \nhave defended this view more through this  ",
      "offset": 8741,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "entire episode. But because I don’t buy the \nintelligence exclusion fully, I do think  ",
      "offset": 8745.319,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "there is the possibility of multiple people \ndeploying powerful AIs at the same time and having  ",
      "offset": 8750.399,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "a world that has ASIs, but is also decentralized \nin the way the modern world is decentralized. ",
      "offset": 8755.319,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "In that world I really worry about you could \njust be like, “oh, classical liberal utopia  ",
      "offset": 8759.479,
      "duration": 4.681
    },
    {
      "lang": "en",
      "text": "achieved”. But I worry about the fact that \nyou can have these torture chambers for much  ",
      "offset": 8764.16,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "cheaper and in a way that’s much harder to \nmonitor. You can have millions of beings that  ",
      "offset": 8770.76,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "are being tortured and it doesn’t even have \nto be some huge data center. Future distilled  ",
      "offset": 8775.8,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "models could literally be your backyard.\nAnd then there’s more speculative worries.  ",
      "offset": 8780.6,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "I had this physicist on who was talking about \nthe possibility of creating vacuum decay where  ",
      "offset": 8789,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "you literally just destroy the universe. And he’s \nlike, “as far as I know, seems totally plausible”. ",
      "offset": 8794.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "That’s an argument for the singleton stuff, by \nthe way. Not just a moral argument, but also  ",
      "offset": 8802.16,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "an epistemic prediction. If it’s true that some \nof those super weapons are possible, and some of  ",
      "offset": 8807.24,
      "duration": 5.079
    },
    {
      "lang": "en",
      "text": "these private moral atrocities are possible, then \neven if you have eight different power centers,  ",
      "offset": 8812.319,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "it’s going to be in their collective interest to \ncome to some sort of bargain with each other to  ",
      "offset": 8817.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "prevent more power centers from arising and \ndoing crazy stuff. Similar to how nuclear  ",
      "offset": 8822.439,
      "duration": 3.801
    },
    {
      "lang": "en",
      "text": "non-proliferation is sort of, whatever set of \ncountries have nukes, it’s in their collective  ",
      "offset": 8826.24,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "interest to stop lots of other countries.\nYou know, I do think it’s possible to unbundle  ",
      "offset": 8830.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "liberalism in this sense. Like the United States \nis so far a liberal country and we do ban slavery  ",
      "offset": 8835.279,
      "duration": 6.721
    },
    {
      "lang": "en",
      "text": "and torture. I think it is plausible to imagine a \nfuture society that works the same way. This may  ",
      "offset": 8842,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "be in some sense a surveillance state, in the \nsense that there is some AI that knows what’s  ",
      "offset": 8848.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "going on everywhere, but that AI then keeps it \nprivate and it doesn’t interfere because that’s  ",
      "offset": 8853.439,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "what we told it to do using our liberal values.\nCan I ask a little bit more about... Kelsey Piper  ",
      "offset": 8857.84,
      "duration": 9.12
    },
    {
      "lang": "en",
      "text": "is a journalist at Vox who published this exchange \nyou had with the OpenAI representative. A couple  ",
      "offset": 8866.96,
      "duration": 8.56
    },
    {
      "lang": "en",
      "text": "of things were very obvious from that exchange. \nOne, nobody had done this before. They just did  ",
      "offset": 8875.52,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "not think this is a thing somebody would do. \nAnd one of the reasons I assume, I assume many  ",
      "offset": 8882.52,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "high-integrity people have worked for OpenAI \nand then have left. A high-integrity person  ",
      "offset": 8889.72,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "might say at some point, “look, you’re asking me \nto do something obviously evil and keep money”.  ",
      "offset": 8894.16,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "And many of them would say no to that. But this is \nsomething where it was supererogatory to be like,  ",
      "offset": 8900.479,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "“there’s no immediate thing I want to say \nright now, but just the principle of being  ",
      "offset": 8906.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "suppressed is worth at least $2 million for me”.\nAnd the other thing that I actually want to ask  ",
      "offset": 8911.279,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "you about is in retrospect- and I know it’s \nso much easier to say in retrospect than it  ",
      "offset": 8918.52,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "must have been at the time- especially with the \nfamily and everything. In retrospect, this asks  ",
      "offset": 8922.64,
      "duration": 4.66
    },
    {
      "lang": "en",
      "text": "for OpenAI to have lifetime non-disclosure that \nyou couldn’t even talk about from all employees. ",
      "offset": 8927.3,
      "duration": 6.7
    },
    {
      "lang": "en",
      "text": "Non-disparagement.\n‘Non-disparagement’ from  ",
      "offset": 8934,
      "duration": 2.56
    },
    {
      "lang": "en",
      "text": "all employees- I’m glad you brought that \nup. Non-disparagement, that’s not about  ",
      "offset": 8936.56,
      "duration": 6.44
    },
    {
      "lang": "en",
      "text": "classified information. It’s like you cannot say \nanything negative about OpenAI after you’ve left. ",
      "offset": 8943,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "And you can’t tell anyone that you’ve agreed.\nThis non-disparagement agreement where you  ",
      "offset": 8947,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "can’t ever criticize OpenAI in the future, it \nseems like the kind of thing that in retrospect  ",
      "offset": 8950.92,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "was an obvious bluff. And this is the wages \nthat you have earned, right? So this is not  ",
      "offset": 8958.24,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "about some future payment. This is like when \nyou signed the contract to work for OpenAI,  ",
      "offset": 8965,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you were like, “I’m getting equity, which is \nmost of my compensation, not just the cash”. ",
      "offset": 8968.2,
      "duration": 3.56
    },
    {
      "lang": "en",
      "text": "In retrospect, it’d be like, well if you tell a \njournalist about this, they’re obviously going  ",
      "offset": 8973.319,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "to have to walk back. This is clearly not a \nsustainable gambit on OpenAI’s behalf. And so  ",
      "offset": 8976.479,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "I’m curious, from your perspective as somebody \nwho lived through it, why do you think you were  ",
      "offset": 8982.68,
      "duration": 3.8
    },
    {
      "lang": "en",
      "text": "the first person to actually call the bluff?\nGreat question. So I don’t know, let me try  ",
      "offset": 8986.479,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "to reason aloud here. So my wife and I talked \nabout it for a while and we also talked with  ",
      "offset": 8992.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "some friends and got some legal advice. One of \nthe filters that we had to pass through was even  ",
      "offset": 8998.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "noticing this stuff in the first place. I know \nfor a fact a bunch of friends I have who also  ",
      "offset": 9003.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "left the company just signed the paperwork on \ntheir last day without actually reading all of  ",
      "offset": 9006.88,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "it. So I think some people just didn’t even know \nthat. It said something at the top about “if you  ",
      "offset": 9010.88,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "don’t sign this, you lose your equity”. But then \non a couple pages later it was like, “and you  ",
      "offset": 9018.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "have to agree not to criticize the company”. So \nI think some people just signed it and moved on. ",
      "offset": 9023.08,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "And then of the people who knew about it, \nwell, I can’t speak for anyone else but  ",
      "offset": 9027.479,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "A. I don’t know the law. Is this actually \nnot standard practice? Maybe it is standard  ",
      "offset": 9033.6,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "practice. Right? From what I’ve heard now there \nare non-disparagement agreements in various tech  ",
      "offset": 9037.96,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "industry companies and stuff. It’s not crazy to \nhave a non-disparagement agreement upon leaving,  ",
      "offset": 9044.56,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "it’s more normal to tie that agreement to \nsome sort of positive compensation where  ",
      "offset": 9050.399,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you get some bonus if you agree. But whereas \nwhat OpenAI did was unusual because it was like  ",
      "offset": 9054.92,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "your equity if you don’t. But non disparate \ndisagreements are actually somewhat common. ",
      "offset": 9059.6,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "So basically in my position of ignorance, \nI wasn’t confident that- I didn’t actually  ",
      "offset": 9066.479,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "expect that all the journalists would take my side \nand I think what I expected was that there’d be a  ",
      "offset": 9072.88,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "little news story at some point, and a bunch of AI \nsafety people would be like, “grr, OpenAI is evil,  ",
      "offset": 9079.64,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "and good for you, Daniel, for standing up to \nthem”. But I didn’t expect there to be this huge  ",
      "offset": 9085,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "uproar, and I didn’t expect the employees of the \ncompany to really come out and support and make  ",
      "offset": 9089.24,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "them change their policies. That was really cool \nto see. It was kind of like a spiritual experience  ",
      "offset": 9096.12,
      "duration": 11.04
    },
    {
      "lang": "en",
      "text": "for me. I sort of took this leap, and then it \nended up working out better than I expected. ",
      "offset": 9107.16,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "I think another factor that was going on is that \nit wasn’t a foregone conclusion that my wife and  ",
      "offset": 9114.279,
      "duration": 6.841
    },
    {
      "lang": "en",
      "text": "I would make this decision. It was kind of crazy \nbecause one of the very powerful arguments was,  ",
      "offset": 9121.12,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "“come on, if you want to criticize them \nin the future, you can still do that.  ",
      "offset": 9127.24,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "They’re not going to actually sue you”. So \nthere’s a very strong argument to be like,  ",
      "offset": 9130.56,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "“just sign it anyway and then you can still write \nyour blog post criticizing them in the future”.  ",
      "offset": 9135.68,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "And it’s no big deal. They wouldn’t dare actually \nanchor equity. Right? And I imagine that a lot of  ",
      "offset": 9141.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "people basically went for that argument instead.\nAnd then, of course, there’s the actual money.  ",
      "offset": 9146.439,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "And I think that one of the factors there was \nmy AI timelines and stuff. If I do think that  ",
      "offset": 9153.24,
      "duration": 9.64
    },
    {
      "lang": "en",
      "text": "probably by the end of this decade, there’s \ngoing to be some sort of crazy superintelligent  ",
      "offset": 9162.88,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "transformation, what would I rather have \nafter it’s all over? The extra money or…  ",
      "offset": 9166.56,
      "duration": 7.681
    },
    {
      "lang": "en",
      "text": "Yeah. So I think that was part of it. It’s not \nlike we’re poor. I worked at OpenAI for two  ",
      "offset": 9175.24,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "years. I have plenty of money now. So in terms \nof our actual family’s level of well being,  ",
      "offset": 9181.76,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "it basically didn’t make a difference, you know?\nYeah. I will note that I know at least one other  ",
      "offset": 9186.92,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "person who made that same choice.\nLeopold? ",
      "offset": 9193.04,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "That’s right, Leopold. And again, It’s worth \nemphasizing that when they made this choice,  ",
      "offset": 9196.2,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "they thought that they were actually losing \nthis equity. They didn’t think that this was,  ",
      "offset": 9201.439,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "“oh, this is just a show” or whatever.\nWait, did he not- I thought he actually  ",
      "offset": 9206.68,
      "duration": 2.6
    },
    {
      "lang": "en",
      "text": "did. I was gonna say, didn’t he? He didn’t get \nit back, did he? Or did Leopold get his equity? ",
      "offset": 9209.279,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "I actually don’t know.\nMy understanding is that he just  ",
      "offset": 9214.08,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "actually lost it. And so props to him for actually \ngoing through with it. I guess we could ask him.  ",
      "offset": 9216.24,
      "duration": 7.159
    },
    {
      "lang": "en",
      "text": "But my understanding was that his situation, \nwhich happened a little bit before mine,  ",
      "offset": 9223.399,
      "duration": 3.681
    },
    {
      "lang": "en",
      "text": "was that he didn’t have any vested equity at \nthe time because he had been there for less  ",
      "offset": 9227.08,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "than a year. But they did give him an actual \noffer of “we will let you vest your equity if  ",
      "offset": 9230.399,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "you sign this thing”. And he said no.\nSo he made a similar choice to me,  ",
      "offset": 9236.68,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "but because the legal situation with him was \na lot more favorable to OpenAI because they  ",
      "offset": 9240.72,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "were actually offering him something, I would \nassume they didn’t feel the need to walk it back,  ",
      "offset": 9247.6,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "but we can ask him. Anyhow. Props to him.\nAnd then how did this episode in general  ",
      "offset": 9253.319,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "inform your worldview around how people will \nmake high stakes decisions where potentially  ",
      "offset": 9260.439,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "their own self interest is involved \nin this kind of key period that you  ",
      "offset": 9269.399,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "imagine will happen by the end of the decade?\nI don’t know if I have that many interesting  ",
      "offset": 9273.16,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "things to say there. I mean, I think one thing \nis fear is a huge factor. I was so afraid during  ",
      "offset": 9277.8,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "that whole process. More afraid than I needed \nto be in retrospect. And another thing is that  ",
      "offset": 9283.76,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "legality is a huge factor, at least for people \nlike me. I think in retrospect it was, “oh yeah,  ",
      "offset": 9289.96,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "the public’s on your side, the employees are on \nyour side. You’re just obviously in the right  ",
      "offset": 9297.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "here”. But at the time I was like, “oh no, \nI don’t want to accidentally violate the law  ",
      "offset": 9301.24,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "and get sued. I don’t want to go too far”. I was \njust so afraid of various things. In particular,  ",
      "offset": 9305.84,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "I was afraid of breaking the law.\nAnd so one of the things that I would  ",
      "offset": 9311.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "advocate for with whistleblower protections \nis just simply making it legal to go talk to  ",
      "offset": 9315.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "the government and say “we’re doing a secret \nintelligence explosion, I think it’s dangerous  ",
      "offset": 9320.52,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "for these reasons” is better than nothing. I think \nthere’s going to be some fraction of people for  ",
      "offset": 9324.399,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "which that would make the difference. Whether \nit’s just literally allowed or not, legally,  ",
      "offset": 9330.24,
      "duration": 3.88
    },
    {
      "lang": "en",
      "text": "makes a difference independently of whether \nthere’s some law that says you’re protected from  ",
      "offset": 9334.12,
      "duration": 3.319
    },
    {
      "lang": "en",
      "text": "retaliation or whatever. Literally just making it \nlegal. I think that’s one thing. Another thing is  ",
      "offset": 9337.439,
      "duration": 7.801
    },
    {
      "lang": "en",
      "text": "the incentives actually work. Money is a powerful \nmotivator and fear of getting sued is a powerful  ",
      "offset": 9345.24,
      "duration": 7
    },
    {
      "lang": "en",
      "text": "motivator. And this social technology just does \nin fact work to get people organized in companies  ",
      "offset": 9352.24,
      "duration": 7.64
    },
    {
      "lang": "en",
      "text": "and working towards the vision of leaders.\nOkay. Scott, can I ask you some questions? ",
      "offset": 9359.88,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "Of course.\nHow often do you discover a new  ",
      "offset": 9365.84,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "blogger you’re super excited about?\nOrder of once a year. ",
      "offset": 9368.72,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "Okay. And how often after you discover them, \ndoes the rest of the world discover them? ",
      "offset": 9372.399,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "I don’t think there are many hidden gems. \nOnce a year is a crazy answer in some sense,  ",
      "offset": 9376.88,
      "duration": 4.2
    },
    {
      "lang": "en",
      "text": "like it ought to be more. There are so many \nthousands of people on Substack. But I do  ",
      "offset": 9381.08,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "just think it’s true that the good blogging \nspace is undersupplied and there is a strong  ",
      "offset": 9385.8,
      "duration": 10.04
    },
    {
      "lang": "en",
      "text": "power law. And partly this is subjective, I \nonly like certain bloggers, there are many  ",
      "offset": 9395.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "people who I’m sure are great that I don’t like.\nBut it also seems like our community in the sense  ",
      "offset": 9402.399,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "of people who are thinking about the same ideas, \npeople who care about AI economics, those kinds  ",
      "offset": 9408.439,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "of things, discovers one new great blogger a \nyear, something like that. Everyone is still  ",
      "offset": 9414.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "talking about Applied Divinity Studies, who hasn’t \nwritten, unless I missed something, hasn’t written  ",
      "offset": 9420.08,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "much in a couple of years. I don’t know. It seems \nundersupplied. I don’t have a great explanation. ",
      "offset": 9425,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "If you had to give an \nexplanation, what would it be? ",
      "offset": 9431.52,
      "duration": 2.16
    },
    {
      "lang": "en",
      "text": "So this is something that I wish I could get \nDaniel to spend a couple of months modeling.  ",
      "offset": 9433.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "I was going to say it’s the intersection of too \nmany different tasks. You need people who can  ",
      "offset": 9443.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "come up with ideas, who are prolific, who are good \nwriters. But actually I can also count on a pretty  ",
      "offset": 9448.52,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "small number of figures the number of people who \nhad great blog posts but weren’t that prolific. ",
      "offset": 9454.319,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "There was a guy named LouKeep who everybody liked \nfive years ago and he wrote like 10 posts and  ",
      "offset": 9459.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "people still refer to all 10 of those posts and “I \nwonder if LouKeep will ever come back”. So there  ",
      "offset": 9464.72,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "aren’t even that many people who are very slightly \nfailing by having all of them accept prolificness.  ",
      "offset": 9469.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "Nick Whitaker, back when there was lots of FTX \nmoney rolling around, I think this was Nick,  ",
      "offset": 9476.08,
      "duration": 6.359
    },
    {
      "lang": "en",
      "text": "tried to sponsor a blogging fellowship with just \nan absurdly high prize. And there were some great  ",
      "offset": 9482.439,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "people, I can’t remember who won, but it didn’t \nresult in a Cambrian explosion of blogging. I  ",
      "offset": 9488.319,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "think it was $100,000. I can’t remember if that \nwas the grand prize or the total prize pool. But  ",
      "offset": 9495.56,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "having some ridiculous amount of money put in \nas an incentive got like three extra people. ",
      "offset": 9500.56,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "Yeah. So you have no explanation?\nActually, Nick is an interesting case  ",
      "offset": 9506.76,
      "duration": 3.48
    },
    {
      "lang": "en",
      "text": "because Works in Progress is a great magazine. \nAnd the people who write for Works in Progress,  ",
      "offset": 9510.24,
      "duration": 6.479
    },
    {
      "lang": "en",
      "text": "some of them I already knew as good bloggers, \nothers I didn’t. So I don’t understand why they  ",
      "offset": 9516.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "can write good magazine articles without being \ngood bloggers. In terms of writing good blogs  ",
      "offset": 9523.68,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "that we all know about, that could be because \nof the editing. That could be because they are  ",
      "offset": 9528.24,
      "duration": 6.159
    },
    {
      "lang": "en",
      "text": "not prolific. Or it could be- one thing that \nhas always amazed me is there are so many good  ",
      "offset": 9534.399,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "posters on Twitter. There were so many good \nposters on Livejournal before it got taken  ",
      "offset": 9540.359,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "over by Russia. There were so many good people \non Tumblr before it got taken over by woke. ",
      "offset": 9544.72,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "But only like 1% of these people who are \ngood at short and medium form ever go to  ",
      "offset": 9551.24,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "long form. I was on Livejournal myself for \nseveral years and people liked my blog,  ",
      "offset": 9556.359,
      "duration": 6.4
    },
    {
      "lang": "en",
      "text": "but it was just another Livejournal. No one paid \nthat much attention to it. Then I transitioned  ",
      "offset": 9562.76,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "to WordPress and all of a sudden I got orders of \nmagnitude much more attention. “Oh, it’s a real  ",
      "offset": 9566.84,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "blog now we can discuss it now it’s part of the \nconversation”. I do think courage has to be some  ",
      "offset": 9572.399,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "part of the explanation. Just because there are \nso many people who are good at using these hidden  ",
      "offset": 9577.96,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "away blogging things that never get anywhere. \nAlthough it can’t be that much of the explanation  ",
      "offset": 9583,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "because I feel like now all of those people have \ngotten substacks and some of those substacks  ",
      "offset": 9589.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "went somewhere, but most of them didn’t.\nOn the point about “well, there’s people  ",
      "offset": 9594.52,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "who can write short form, so why isn’t that \ntranslating?” I will mention something that  ",
      "offset": 9599.68,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "has actually radicalized me against Twitter as \nan information source is I’ll meet- and this has  ",
      "offset": 9604.279,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "happened multiple times- I’ll meet somebody who \nseems to be an interesting poster, has funny,  ",
      "offset": 9610.319,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "seemingly insightful posts on Twitter. \nI’ll meet them in person and they are just  ",
      "offset": 9614.96,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "absolute idiots. It’s like they’ve \ngot 240 characters of something  ",
      "offset": 9619.68,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that sounds insightful and it matches to \nsomebody who maybe has a deep worldview,  ",
      "offset": 9625.279,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "you might say, but they actually don’t have it.\nWhereas I’ve actually had the opposite feeling  ",
      "offset": 9629.56,
      "duration": 10.16
    },
    {
      "lang": "en",
      "text": "when I meet anonymous bloggers in real life where \nI’m like, “oh, there’s actually even more to you  ",
      "offset": 9639.72,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "than I realized off your online persona”. You know \nAlvaro de Menard, the Fantastic Anachronism guy? I  ",
      "offset": 9644.319,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "met up with him recently and he gives me, he made \na hundred translations of his favorite Greek poet,  ",
      "offset": 9651.359,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "Cavafy, and he gave me a copy. And it’s just this \nthing he’s been doing on his side. It’s just like  ",
      "offset": 9658.04,
      "duration": 5.239
    },
    {
      "lang": "en",
      "text": "translating Greek poetry he really liked. I don’t \nexpect any anonymous posters on Twitter to be  ",
      "offset": 9663.279,
      "duration": 6.12
    },
    {
      "lang": "en",
      "text": "anytime soon handing me their translation \nof some Roman or Greek poet or something. ",
      "offset": 9669.399,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "Yeah, so on the car ride here, Daniel and I were \ntalking about, in AI now the thing everyone is  ",
      "offset": 9674.88,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "interested in is their ‘time horizon’. Where did \nthis come from? 5 years ago you would not have  ",
      "offset": 9681.12,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "thought, “oh, time horizon. AIs will be able \nto do a bunch of things that last one minute,  ",
      "offset": 9686.24,
      "duration": 4.199
    },
    {
      "lang": "en",
      "text": "but not that last two hours”. Is there \na human equivalent to time horizon? ",
      "offset": 9690.439,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "And we couldn’t figure it out, but it almost seems \nlike there are lots of people who have the time  ",
      "offset": 9695.279,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "horizon to write a really, really good comment \nthat gets to the heart of the issue. Or a really,  ",
      "offset": 9700.12,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "really good Tumblr post which is like three \nparagraphs but somehow can’t make it hang together  ",
      "offset": 9705.88,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "for a whole blog post. And I’m the same way. I can \neasily write a blog post, like a normal length ACX  ",
      "offset": 9710.399,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "blog post, but if you ask me to write a novella \nor something that’s four times the length of the  ",
      "offset": 9716.68,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "average ACX blog post, then it’s this giant mess \nof “re re re re” outline that just gets redone  ",
      "offset": 9721.6,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "and redone and maybe eventually I make it work.\nI did somehow publish Unsong, but it’s a much less  ",
      "offset": 9728.16,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "natural task. So maybe one of the skills that goes \ninto blogging is this. But I mean, no, because  ",
      "offset": 9733.439,
      "duration": 8
    },
    {
      "lang": "en",
      "text": "people write books and they write journal articles \nand they write works in progress articles all  ",
      "offset": 9741.439,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "the time. So I’m back to not understanding this.\nNo, I mean ChatGPT can write you a book. There’s  ",
      "offset": 9746.68,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "a difference between the ChatGPT \nbook, which is most books and… ",
      "offset": 9754.52,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "There are many, many times more people who have \nwritten good books than who are actively operating  ",
      "offset": 9757.76,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "great bloggers right now, I think.\nMaybe that’s financial? ",
      "offset": 9765.56,
      "duration": 3.32
    },
    {
      "lang": "en",
      "text": "No, no, no, no, no, no. Books \nare the worst possible financial  ",
      "offset": 9768.88,
      "duration": 3.479
    },
    {
      "lang": "en",
      "text": "strategy. Substack is where it’s at.\nWorse than blogs? You think so? ",
      "offset": 9772.359,
      "duration": 3.16
    },
    {
      "lang": "en",
      "text": "Oh yeah.\nThe other thing is that  ",
      "offset": 9775.52,
      "duration": 1.641
    },
    {
      "lang": "en",
      "text": "blogs are such a great status gain strategy. I was \ntalking to Scott Aaronson about this. If people  ",
      "offset": 9777.16,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "have questions about quantum computing, they ask \nScott Aronson or he is like the authority. I mean  ",
      "offset": 9785.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "there are probably hundreds of other professors \nwho do quantum computing things but nobody knows  ",
      "offset": 9791,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "who they are because they don’t have blogs.\nSo I think it’s underdone. I think there must  ",
      "offset": 9795.04,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "be some reason why it’s underdone. I don’t \nunderstand what that is because I’ve seen  ",
      "offset": 9799.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "so many of the elements that it would take to \ndo it in so many different places and I think  ",
      "offset": 9804.439,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "it’s either just a multiplication problem \nwhere 20% of people are good at one thing,  ",
      "offset": 9809.88,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "20% of people are good at another thing, and \nyou need five things, there aren’t that many. ",
      "offset": 9815.68,
      "duration": 4.36
    },
    {
      "lang": "en",
      "text": "Plus something like courage, where people \nwho would be good at writing blogs don’t  ",
      "offset": 9820.04,
      "duration": 5.279
    },
    {
      "lang": "en",
      "text": "want to do it. I actually know several people who \nI think would be great bloggers in the sense that  ",
      "offset": 9825.319,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "sometimes they send me multi-paragraph emails \nin response to an ACX post and I’m like, “wow,  ",
      "offset": 9830.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "this is just an extremely well written thing \nthat could have been another blog post. Why  ",
      "offset": 9837.12,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "don’t you start a blog?” And they’re \nlike, “oh, I could never do that”. ",
      "offset": 9841.24,
      "duration": 3.96
    },
    {
      "lang": "en",
      "text": "What advice do you have to somebody who wants to \nbecome good at it but isn’t currently good at it? ",
      "offset": 9845.2,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "Do it every day, same advice as for everything \nelse. I say that I very rarely see new bloggers  ",
      "offset": 9851.399,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "who are great. But like when I see some. \nI published every day for the first couple  ",
      "offset": 9857.64,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "years of Slate Star Codex, maybe only the first \nyear. Now I could never handle that schedule,  ",
      "offset": 9862.319,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "I don’t know, I was in my 20s, I \nmust have been briefly superhuman. ",
      "offset": 9867.04,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "But whenever I see a new person who \nblogs every day it’s very rare that  ",
      "offset": 9870.76,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that never goes anywhere or they don’t \nget good. That’s like my best leading  ",
      "offset": 9874.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "indicator for who’s going to be a good blogger.\nAnd do you have advice on what kinds of things  ",
      "offset": 9879.68,
      "duration": 4.76
    },
    {
      "lang": "en",
      "text": "to start? One frustration you can have is you \nwant to do it, but you have so little to say,  ",
      "offset": 9884.439,
      "duration": 6.641
    },
    {
      "lang": "en",
      "text": "you don’t have that deep a world model, \na lot of the ideas you have are just  ",
      "offset": 9891.08,
      "duration": 2.8
    },
    {
      "lang": "en",
      "text": "really shallow or wrong. Just do it anyway?\nSo I think there are two possibilities there.  ",
      "offset": 9893.88,
      "duration": 6.52
    },
    {
      "lang": "en",
      "text": "One is that you are, in fact, a shallow person \nwithout very many ideas. In that case I’m sorry,  ",
      "offset": 9900.399,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it sounds like that’s not going to work. But \nusually when people complain that they’re in  ",
      "offset": 9904.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that category, I read their Twitter or I read \ntheir Tumblr, or I read their ACX comments,  ",
      "offset": 9908.56,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "or I listen to what they have to say about AI risk \nwhen they’re just talking to people about it, and  ",
      "offset": 9916.16,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "they actually have a huge amount of things to say. \nSomehow it’s just not connecting with whatever  ",
      "offset": 9922.16,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "part of them has lists of things to blog about.\nSo that may be another one of those skills that  ",
      "offset": 9927.279,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "only 20% of people have, is when you have an idea \nyou actually remember it and then you expand on  ",
      "offset": 9932.16,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "it. I think a lot of blogging is reactive; You \nread other people’s blogs and you’re like, no,  ",
      "offset": 9938.24,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "that person is totally wrong. A \npart of what we want to do with  ",
      "offset": 9944.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "this scenario is say something concrete and \ndetailed enough that people will say, no,  ",
      "offset": 9947.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "that’s totally wrong, and write their own thing.\nBut whether it’s by reacting to other people’s  ",
      "offset": 9952.319,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "posts, which requires that you read a lot, or \nby having your own ideas, which requires you to  ",
      "offset": 9958.359,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "remember what your ideas are, I think that 90% of \npeople who complain that they don’t have ideas,  ",
      "offset": 9962.84,
      "duration": 6.439
    },
    {
      "lang": "en",
      "text": "I think actually have enough ideas. I don’t buy \nthat as a real limiting factor for most people. ",
      "offset": 9969.279,
      "duration": 5.32
    },
    {
      "lang": "en",
      "text": "I have noticed two things in my own… \nI mean, I don’t do that much writing,  ",
      "offset": 9974.6,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "but from the little I do: one, I actually was \nvery shallow and wrong when I started. I started  ",
      "offset": 9981.08,
      "duration": 7.239
    },
    {
      "lang": "en",
      "text": "the blog in college. So if you are somebody who’s \nlike, “this is bullshit, there’s nothing to this.  ",
      "offset": 9988.319,
      "duration": 6.801
    },
    {
      "lang": "en",
      "text": "Somebody else wrote about this already”, that’s \nfine, what did you expect? Right? Of course, as  ",
      "offset": 9995.12,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "you’re reading more things and learning more about \nthe world, that’s to be expected and just keep  ",
      "offset": 10002.04,
      "duration": 4.52
    },
    {
      "lang": "en",
      "text": "doing it if you want to keep getting better at it.\nAnd the other thing now when I write blog posts,  ",
      "offset": 10006.56,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "as I’m writing them, I’m just like, “why? These \nare just some random stories from when I was in  ",
      "offset": 10012.72,
      "duration": 5.64
    },
    {
      "lang": "en",
      "text": "China. They’re like kind of cringe stories”. Or \nwith the AI firm’s post, it’s like, “come on,  ",
      "offset": 10018.359,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "these are just weird ideas. And also some of these \nseem obvious, whatever”. My podcasts do what I  ",
      "offset": 10025.68,
      "duration": 9.88
    },
    {
      "lang": "en",
      "text": "expect them to do. My blogs just take off way \nmore than I expect them to take off in advance. ",
      "offset": 10035.56,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "Your blog posts are actually very good.\nYeah, they’re good. ",
      "offset": 10041.2,
      "duration": 2.199
    },
    {
      "lang": "en",
      "text": "But the thing I would emphasize is that, for \nme, I’m not a regular writer and I couldn’t do  ",
      "offset": 10043.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "them on a daily basis. And as I’m writing \nthem, it’s just this one or two week long  ",
      "offset": 10049.08,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "process of feeling really frustrated. Like, \n“this is all bullshit, but I might as well  ",
      "offset": 10053.52,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "just stick with the sunk cost and just do it”.\nIt’s interesting because like a lot of areas  ",
      "offset": 10058.359,
      "duration": 5.881
    },
    {
      "lang": "en",
      "text": "of life are selected for arrogant people who don’t \nknow their own weaknesses because they’re the only  ",
      "offset": 10064.24,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "ones who get out there. I think with blogs and I \nmean this is self-serving, maybe I’m an arrogant  ",
      "offset": 10072.08,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "person, but that doesn’t seem to be the case. I \nhear a lot of stuff from people who are like, “I  ",
      "offset": 10077.04,
      "duration": 7.319
    },
    {
      "lang": "en",
      "text": "hate writing blog posts. Of course I have nothing \nuseful to say”, but then everybody seems to like  ",
      "offset": 10084.359,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "it and reblog it and say that they’re great.\nPart of what happened with me was I spent my  ",
      "offset": 10089.88,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "first couple years that way, and then gradually \nI got enough positive feedback that I managed  ",
      "offset": 10095.12,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "to convince the inner critic in my head that \nprobably people will like my blog post. But there  ",
      "offset": 10101.04,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "are some things that people have loved that I was \nabsolutely on the verge of, “no, I’m just going to  ",
      "offset": 10105.479,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "delete this, it would be too crazy to put it out \nthere”. That’s why I say that maybe the limiting  ",
      "offset": 10110.04,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "factor for so many of these people is courage \nbecause everybody I talk to who blogs is within  ",
      "offset": 10114.96,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "1% of not having enough courage of blogging.\nThat’s right. That’s right. And also “courage”  ",
      "offset": 10120.359,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "makes it sound very virtuous, which I \nthink it can often be, given the topic,  ",
      "offset": 10127.56,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "but at least often it’s just like…\nConfidence? ",
      "offset": 10135.08,
      "duration": 4.439
    },
    {
      "lang": "en",
      "text": "No, not even confidence. It’s closer to \nmaybe what an aspiring actor feels when  ",
      "offset": 10139.52,
      "duration": 7.521
    },
    {
      "lang": "en",
      "text": "they go to an audition where it’s like, “I \nfeel really embarrassed. But also I just  ",
      "offset": 10147.04,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "really want to be a movie star”.\nSo the way I got through this is  ",
      "offset": 10151.04,
      "duration": 7.68
    },
    {
      "lang": "en",
      "text": "I blogged for like 8 to 10 years on LiveJournal \nbefore- no, it was less than that. It’s more  ",
      "offset": 10159.279,
      "duration": 10.16
    },
    {
      "lang": "en",
      "text": "like five years on LiveJournal before ever \nstarting a real blog. I posted on LessWrong  ",
      "offset": 10169.439,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "for a year or two before getting my own blog. \nI got very positive feedback from all of that,  ",
      "offset": 10175.239,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "and then eventually I took the plunge to start my \nown blog. But it’s ridiculous. What other career  ",
      "offset": 10181.6,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "do you need seven years of positive feedback \nbefore you apply for your first position? ",
      "offset": 10186.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "I mean, you have the same thing. You’ve gotten \nrave reviews for all of your podcasts, and then  ",
      "offset": 10191.96,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you’re kind of trying to transfer to blogging \nwith probably... First of all, you have a fan  ",
      "offset": 10196.84,
      "duration": 6.399
    },
    {
      "lang": "en",
      "text": "base. People are going to read your blog. That, I \nthink is one thing, is people are just afraid no  ",
      "offset": 10203.239,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "one will read it, which is probably true for most \npeople’s first blog. And then there are enough  ",
      "offset": 10207.88,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "people who like you that you’ll probably get \nmostly positive feedback, even if the first things  ",
      "offset": 10214.04,
      "duration": 4.6
    },
    {
      "lang": "en",
      "text": "you write aren’t that polished. So I think you and \nI both had that. A lot of people I know who got  ",
      "offset": 10218.64,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "into blogging kind of had something like that. And \nI think that’s one way to get over the fear gap. ",
      "offset": 10225.76,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "I wonder if this sends the wrong message or raises \nexpectations or raises concerns and anxieties. But  ",
      "offset": 10234.16,
      "duration": 8.359
    },
    {
      "lang": "en",
      "text": "one idea I’ve been shooting around, and I’d be \ncurious about your take on this: I feel like this  ",
      "offset": 10242.52,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "slow, compounding growth of a fan base is fake. \nIf I notice some of the most successful things  ",
      "offset": 10247.439,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "in our sphere that have happened; Leopold releases \nSituational Awareness. He hasn’t been building up  ",
      "offset": 10254.76,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "a fan base over years. It’s just really good. And \nas you were mentioning a second ago, whenever you  ",
      "offset": 10259.479,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "notice a really great new blogger, it’s not like \nit then takes them a year or two to build up a fan  ",
      "offset": 10264.72,
      "duration": 4.92
    },
    {
      "lang": "en",
      "text": "base. Nope, everybody, at least that they care \nabout, is talking about it almost immediately. ",
      "offset": 10269.64,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "I mean, Situational Awareness is in a different \ntier almost. But things like that and even things  ",
      "offset": 10275.8,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "that are an order of magnitude smaller than that \nwill literally just get read by everybody who  ",
      "offset": 10281.399,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "matters. And I mean literally everybody. And \nI expect this to happen with AI 2027 when it  ",
      "offset": 10287.08,
      "duration": 6.279
    },
    {
      "lang": "en",
      "text": "comes out. But Daniel, you’ve been building your \nreputation within this specific community, and I  ",
      "offset": 10293.359,
      "duration": 6.561
    },
    {
      "lang": "en",
      "text": "expect AI 2027 it's just really good. And I expect \nit’ll just blow up in a way that isn’t downstream  ",
      "offset": 10299.92,
      "duration": 6.319
    },
    {
      "lang": "en",
      "text": "of you having built up an audience over years.\nThank you. I hope that happens. We’ll see. ",
      "offset": 10306.239,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "Slightly pushing back against that. I have \nstatistics for the first several years of  ",
      "offset": 10311.12,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "Slate Star Codex, and it really did grow extremely \ngradually. The usual pattern is something like  ",
      "offset": 10314.96,
      "duration": 9.52
    },
    {
      "lang": "en",
      "text": "every viral hit, 1% of the people who read your \nviral hits stick around. And so after dozens of  ",
      "offset": 10324.479,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "viral hits, then you have a fan base. But smoothed \nout, It does look like a- I wish I had seen this  ",
      "offset": 10331.239,
      "duration": 7.28
    },
    {
      "lang": "en",
      "text": "recently, but I think it’s like over the course \nof three years, it was a pretty constant rise  ",
      "offset": 10338.52,
      "duration": 4.521
    },
    {
      "lang": "en",
      "text": "up to some plateau where I imagine it was a \ndynamic equilibrium and as many new people  ",
      "offset": 10343.04,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "were coming in as old people were leaving.\nI think that with Situational Awareness,  ",
      "offset": 10348.12,
      "duration": 7.8
    },
    {
      "lang": "en",
      "text": "I don’t know how much publicity Leopold put into \nit. We’re doing pretty deliberate publicity,  ",
      "offset": 10355.92,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "we’re going on your podcast. I think you can \neither be the sort of person who can go on a  ",
      "offset": 10360.2,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "Dwarkesh podcast and get the New York Times to \nwrite about you, or you can do it organically,  ",
      "offset": 10367.08,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "the old fashioned way, which is very long.\nYeah. Okay. So you say that throwing money  ",
      "offset": 10373.04,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "at people to make them, to get them to blog at \nleast didn’t seem to work for the FTX folks.  ",
      "offset": 10378.76,
      "duration": 6.84
    },
    {
      "lang": "en",
      "text": "If it was up to you, what would you do? What’s \nyour grant plan to get 10 more Scott Alexanders? ",
      "offset": 10385.6,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "Man. So my friend Clara Collier, who’s the \neditor of Asterisk magazine, is working on  ",
      "offset": 10391.64,
      "duration": 7.48
    },
    {
      "lang": "en",
      "text": "something like this for AI blogging. \nAnd her idea, which I think is good,  ",
      "offset": 10399.12,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "is to have a fellowship. I think Nick’s thing was \nalso a fellowship, but the fellowship would be,  ",
      "offset": 10403.279,
      "duration": 5.921
    },
    {
      "lang": "en",
      "text": "there is an Asterisk AI blogging fellows’ blog or \nsomething like that. Clara will edit your post,  ",
      "offset": 10409.72,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "make sure that it’s good, put it up there and \nshe’ll select many people who she thinks will  ",
      "offset": 10417.16,
      "duration": 7.399
    },
    {
      "lang": "en",
      "text": "be good at this. She’ll do all of the kind of \ncourage requiring work of being like, “yes, your  ",
      "offset": 10424.56,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "post is good. I’m going to edit it now. Now it’s \nvery good. Now I’m going to put it on the blog”. ",
      "offset": 10430.88,
      "duration": 5.56
    },
    {
      "lang": "en",
      "text": "And I think her hope is that, let’s \nsay of the fellows that she chooses,  ",
      "offset": 10436.439,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "now it’s not that much of a courage step for them \nto start it because they have the approval of what  ",
      "offset": 10441.72,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "last psychiatrist would call an omniscient entity, \nsomebody who is just allowed to approve things and  ",
      "offset": 10448.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "tell you that you’re okay on a psychological \nlevel. And then like maybe of those fellows,  ",
      "offset": 10454.359,
      "duration": 5.08
    },
    {
      "lang": "en",
      "text": "some percent of them will have their blog posts \nbe read and people will like them. And I don’t  ",
      "offset": 10459.439,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "know how much reinforcement it takes to get \nover the high prior everyone has on “no one  ",
      "offset": 10464.84,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "will like my blog”. But maybe for some people, the \namount of reinforcement they get there will work. ",
      "offset": 10469.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "Yeah, like an interesting example would be all \nof the journalists who have switched to having  ",
      "offset": 10474.76,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "Substacks. Many of them go well. Would all \nof those journalists have become bloggers if  ",
      "offset": 10480.92,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "there was no such thing as mainstream media? \nI’m not sure. But if you’re Paul Krugman you  ",
      "offset": 10485.76,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "know people like your stuff, and then when \nyou quit the New York Times you know you can  ",
      "offset": 10490.439,
      "duration": 4.12
    },
    {
      "lang": "en",
      "text": "just open a substack and start doing exactly \nwhat you were doing before. So I don’t know,  ",
      "offset": 10494.56,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "maybe my answer is there should be mainstream \nmedia. I hate to admit that, but maybe it’s true. ",
      "offset": 10499.52,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "Invented it from first principles.\nYeah. ",
      "offset": 10504.72,
      "duration": 2.76
    },
    {
      "lang": "en",
      "text": "Well I do think that it should be treated more as \na viable career path. Where right now, if you told  ",
      "offset": 10507.479,
      "duration": 8.2
    },
    {
      "lang": "en",
      "text": "your parents, “I’m going to become a startup \nfounder”, I think the reaction would be like,  ",
      "offset": 10515.68,
      "duration": 4.84
    },
    {
      "lang": "en",
      "text": "“there’s a 1% chance you’ll succeed, but it’s \nan interesting experience and if you do succeed,  ",
      "offset": 10520.52,
      "duration": 4.04
    },
    {
      "lang": "en",
      "text": "that’s crazy. That’ll be great. If you \ndon’t, you’ll learn something. It’ll  ",
      "offset": 10524.56,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "be helpful to the thing you do afterwards”.\nWe know that’s true of blogging, right? We know  ",
      "offset": 10528.08,
      "duration": 3.72
    },
    {
      "lang": "en",
      "text": "that it helps you build up a network, it helps \nyou develop your ideas. And if you do succeed,  ",
      "offset": 10531.8,
      "duration": 6.76
    },
    {
      "lang": "en",
      "text": "you get a dream job for a lifetime. And I \nthink maybe they don’t have that mindset,  ",
      "offset": 10538.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "but also they under appreciate how much you \nactually could succeed at it. It’s not a  ",
      "offset": 10543.359,
      "duration": 6.2
    },
    {
      "lang": "en",
      "text": "crazy outcome to make a lot of money as a blogger.\nI think it might be a crazy outcome to make a lot  ",
      "offset": 10549.56,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "of money as a blogger. I don’t know what percent \nof people who start a blog end up making enough  ",
      "offset": 10555.52,
      "duration": 6.36
    },
    {
      "lang": "en",
      "text": "that they can quit their day job. My guess is it’s \na lot worse than for startup founders. I would not  ",
      "offset": 10561.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "even have that as a goal so much as like the Scott \nAaronson goal of, okay, you’re still a professor,  ",
      "offset": 10567,
      "duration": 8.04
    },
    {
      "lang": "en",
      "text": "but now you’re the professor whose views everybody \nknows and who has kind of a boost up in respect in  ",
      "offset": 10575.04,
      "duration": 6.6
    },
    {
      "lang": "en",
      "text": "your field and especially outside of your field. \nAnd also you can correct people when they’re  ",
      "offset": 10581.64,
      "duration": 4.44
    },
    {
      "lang": "en",
      "text": "wrong, which is a very important side benefit.\nYeah. How does your old blogging feedback  ",
      "offset": 10586.08,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "into your current blogging? So when \nyou’re discussing a new idea, I mean,  ",
      "offset": 10590.479,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "AI or whatever else, are you just able to pull \nfrom the insights from your previous commentary on  ",
      "offset": 10593.6,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "sociology or anthropology or history or something?\nYeah. So I think this is the same as anybody who’s  ",
      "offset": 10599.439,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "not blogging. I think the thing everybody does is \nthey’ve read many books in the past and when they  ",
      "offset": 10606.52,
      "duration": 8.561
    },
    {
      "lang": "en",
      "text": "read a new book, they have enough background to \nthink about it. Like you are thinking about our  ",
      "offset": 10615.08,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "ideas in the context of Joseph Henrich’s book. \nI think that’s good, I think that’s the kind of  ",
      "offset": 10620.479,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "place that intellectual progress comes from. \nI think I am more incentivized to do that. ",
      "offset": 10624.76,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "It’s hard to read books. I think if you look at \nthe statistics, they’re terrible. Most people  ",
      "offset": 10631.279,
      "duration": 5.601
    },
    {
      "lang": "en",
      "text": "barely read any books in a year. And I get lots of \npraise when I read a book and often lots of money,  ",
      "offset": 10636.88,
      "duration": 7.359
    },
    {
      "lang": "en",
      "text": "and that’s a really good incentive. So I think \nI do more research, deep dives, read more books  ",
      "offset": 10644.239,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "than I would if I weren’t a blogger. It’s \nan amazing side benefit. And I probably make  ",
      "offset": 10650.479,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "a lot more intellectual progress than I would \nif I didn’t have those really good incentives. ",
      "offset": 10655.2,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "Yeah. There was actually a prediction market \nabout the year by which an AI would be able to  ",
      "offset": 10660,
      "duration": 5.399
    },
    {
      "lang": "en",
      "text": "write a blog post as good as you. Was it 2026 or \n2027? I think it was 2027. It was like 15% by 2027  ",
      "offset": 10665.399,
      "duration": 9.4
    },
    {
      "lang": "en",
      "text": "or something like that. It is an interesting \nquestion of they do have your writing and all  ",
      "offset": 10674.8,
      "duration": 5.88
    },
    {
      "lang": "en",
      "text": "other good writing in trading distribution. \nAnd weirdly, they seem way better at getting  ",
      "offset": 10680.68,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "superhuman at coding than they are at writing, \nwhich is the main thing in their distribution. ",
      "offset": 10685.319,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "Yeah. It’s an honor to be my generation’s \nGarry Kasparov figure. Yeah. So I’ve tried  ",
      "offset": 10693.399,
      "duration": 7.88
    },
    {
      "lang": "en",
      "text": "this. And first of all, it does a decent \njob. I respect its work. It’s not perfect  ",
      "offset": 10701.279,
      "duration": 6.921
    },
    {
      "lang": "en",
      "text": "yet. I think it’s actually better at the style \non a word-to-word, sentence-to-sentence level,  ",
      "offset": 10708.2,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "than it is at planning out a blog post. So I think \nthere are possibly two reasons for it: One, we  ",
      "offset": 10714.16,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "don’t know how the base model would have done at \nthis task. We know that all the models we see are  ",
      "offset": 10720.88,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "to some degree reinforcement learning into a kind \nof corporate speak mode. You can get it somewhat  ",
      "offset": 10726.56,
      "duration": 5.8
    },
    {
      "lang": "en",
      "text": "out of that corporate speak mode. But I don’t know \nto what degree this is actually doing its best to  ",
      "offset": 10732.359,
      "duration": 5.4
    },
    {
      "lang": "en",
      "text": "imitate Scott Alexander versus hit some average \nbetween Scott Alexander and corporate speak. And  ",
      "offset": 10737.76,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "I don’t think anyone knows except the internal \nemployees who have access to the base model. ",
      "offset": 10744.56,
      "duration": 5.16
    },
    {
      "lang": "en",
      "text": "And the second thing I think of maybe just because \nit’s trendy has an agency or horizon failure,  ",
      "offset": 10749.72,
      "duration": 7.52
    },
    {
      "lang": "en",
      "text": "like deep research is an okay researcher. It’s \nnot a great researcher. If you actually want  ",
      "offset": 10757.239,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "to understand an issue in depth, you can’t use \ndeep research. You gotta do it on your own. So  ",
      "offset": 10763.52,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "if I spend maybe five to 10 hours researching a \nreally research heavy blog post, the METR thing,  ",
      "offset": 10768.64,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "I know we’re not supposed to use it for any task \nexcept coding, but like it says, on average the  ",
      "offset": 10775.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "AI’s horizon is one hour. So I’m guessing it just \ncannot plan and execute a good blog post. It does  ",
      "offset": 10780.76,
      "duration": 6.92
    },
    {
      "lang": "en",
      "text": "something very superficial rather than actually \ngoing through the steps. So my guess for that  ",
      "offset": 10787.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "prediction market would be whenever we think the \nagents are actually good. I think in our scenario  ",
      "offset": 10793.16,
      "duration": 6.199
    },
    {
      "lang": "en",
      "text": "that’s like late 2026. I’m going to be humble \nand not hold out for the superintelligence. ",
      "offset": 10799.359,
      "duration": 6.28
    },
    {
      "lang": "en",
      "text": "What about comments? I feel like \nintuitively it feels like before  ",
      "offset": 10805.64,
      "duration": 3.4
    },
    {
      "lang": "en",
      "text": "we see the AI’s writing great blog posts that \ngo super viral repeatedly, we should see them  ",
      "offset": 10809.04,
      "duration": 4.68
    },
    {
      "lang": "en",
      "text": "writing highly upvoted comments on things.\nYeah. And I think somebody mentioned this on  ",
      "offset": 10813.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "the LessWrong post about it and somebody made \nsome AI generated comments to that post. They  ",
      "offset": 10818.12,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "were not great. But I wouldn’t have immediately \npicked them out of the general distribution  ",
      "offset": 10823.6,
      "duration": 4.28
    },
    {
      "lang": "en",
      "text": "of LessWrong comments as especially bad. I \nthink, like, I think if you were to try this,  ",
      "offset": 10827.88,
      "duration": 7.439
    },
    {
      "lang": "en",
      "text": "you would get something that was so obviously \nan AI house style that it would use the  ",
      "offset": 10835.319,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "word ‘delve’ or things along those lines.\nI think if you were able to avoid that maybe  ",
      "offset": 10842.76,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "by using the base model, maybe by using some kind \nof really good prompt to be like, “no, do this in  ",
      "offset": 10848.04,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "Gwern’s voice”, you would get something that \nwas pretty good. I think if you wrote a really  ",
      "offset": 10853.16,
      "duration": 7.079
    },
    {
      "lang": "en",
      "text": "stupid blog post, it could point out the correct \nobjections to it. But I also just don’t think  ",
      "offset": 10860.239,
      "duration": 4.841
    },
    {
      "lang": "en",
      "text": "it’s as smart as Gwern right now. So its limit on \nmaking Gwern-style comments is both- It needs to  ",
      "offset": 10865.08,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "be able to do a style other than corporate delve \nslop and then it actually needs to get good. ",
      "offset": 10870.68,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "It needs to have good ideas that \nother people don’t already have. ",
      "offset": 10875.92,
      "duration": 3.24
    },
    {
      "lang": "en",
      "text": "Yeah. And I mean I think it can write as well as \na smart average person in a lot of ways. And I  ",
      "offset": 10879.16,
      "duration": 8.119
    },
    {
      "lang": "en",
      "text": "think if you have a blog post that's worse \nthan that or at that level, it can come up  ",
      "offset": 10887.279,
      "duration": 5.561
    },
    {
      "lang": "en",
      "text": "with insightful comments about it. I don’t \nthink it could do it on a quality blog post. ",
      "offset": 10892.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "There was this recent Financial Times article \nabout how have you reached peak cognitive power?  ",
      "offset": 10897.84,
      "duration": 5
    },
    {
      "lang": "en",
      "text": "Where it was talking about declining scores \nin PISA and SAT and so forth. On the Internet  ",
      "offset": 10902.84,
      "duration": 6.56
    },
    {
      "lang": "en",
      "text": "especially, it does seem like there might have \nbeen a golden era before I was that active on  ",
      "offset": 10909.399,
      "duration": 6.681
    },
    {
      "lang": "en",
      "text": "the forums or whatever. Do you have nostalgia \nfor a particular time on the Internet when it  ",
      "offset": 10916.64,
      "duration": 6.04
    },
    {
      "lang": "en",
      "text": "was just like, this is an intellectual mecca?\nI am so mad at myself for missing most of the  ",
      "offset": 10922.68,
      "duration": 5.48
    },
    {
      "lang": "en",
      "text": "golden age of blogging. I feel like if I \nhad started a blog in 2000 or something,  ",
      "offset": 10928.16,
      "duration": 7.279
    },
    {
      "lang": "en",
      "text": "then- I don’t know, I’ve done well for myself, \nI can’t complain- but the people from that era  ",
      "offset": 10935.439,
      "duration": 5.721
    },
    {
      "lang": "en",
      "text": "all founded news organizations or something. I \nmean, God save me from that fate. I would have  ",
      "offset": 10943,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "liked to have been there. I would have liked to \nsee what I could have done in that area. I mean,  ",
      "offset": 10948.96,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "I wouldn’t compare the decline of the Internet \nto that stuff with PISA because I’m sure the  ",
      "offset": 10954.92,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "Internet is just more people are coming \non, it’s a less heavily selected sample. ",
      "offset": 10959.08,
      "duration": 5.479
    },
    {
      "lang": "en",
      "text": "But yeah, I could have passed on the whole era \nwhere they were talking about atheism versus  ",
      "offset": 10965.64,
      "duration": 8.561
    },
    {
      "lang": "en",
      "text": "religion nonstop. That was pretty crazy. But I do \nhear good things about the golden age of blogging. ",
      "offset": 10974.2,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Anybody who was sort of \ncounterfactually responsible  ",
      "offset": 10980.84,
      "duration": 2.08
    },
    {
      "lang": "en",
      "text": "for you starting to blog or keeping blogging?\nSo I owe a huge debt of gratitude to Eliezer  ",
      "offset": 10982.92,
      "duration": 5.319
    },
    {
      "lang": "en",
      "text": "Yudkowski. I had a live journal before that. \nBut it was going on LessWrong that convinced  ",
      "offset": 10988.239,
      "duration": 8.4
    },
    {
      "lang": "en",
      "text": "me I could move to the big times. And second \nof all, I just think I learned I imported a  ",
      "offset": 10996.64,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "lot of my worldview from him. I think I was the \nmost boring normie liberal in the world before  ",
      "offset": 11001.84,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "encountering LessWrong. And I don’t 100% agree \nwith all LessWrong ideas, but just having things  ",
      "offset": 11006.64,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "of that quality beamed into my head and for me \nto react to and think about was really great. ",
      "offset": 11014.239,
      "duration": 7.24
    },
    {
      "lang": "en",
      "text": "And tell me about the fact that you could be and \nwere at some point anonymous, I think for most  ",
      "offset": 11021.479,
      "duration": 9.76
    },
    {
      "lang": "en",
      "text": "of human history, somebody who is an influential \nadvisor or an intellectual or somebody. Actually,  ",
      "offset": 11031.239,
      "duration": 5.641
    },
    {
      "lang": "en",
      "text": "I don’t know if this is true. You would have \nhad to have some sort of public persona. And  ",
      "offset": 11036.88,
      "duration": 5.72
    },
    {
      "lang": "en",
      "text": "a lot of what people read into your work is \nactually a reflection of your public persona. ",
      "offset": 11042.6,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "Sort of. The reason half of these \nancient authors are called things  ",
      "offset": 11048.359,
      "duration": 3.561
    },
    {
      "lang": "en",
      "text": "like Pseudo Dionysus or Pseudocelsus is that \nyou could just write something being like,  ",
      "offset": 11051.92,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "“oh, yeah, this is by Saint Dionysus”. And \nthen, I don’t know, you could be anybody. ",
      "offset": 11056.399,
      "duration": 5.24
    },
    {
      "lang": "en",
      "text": "And I don’t know exactly how common that was in \nthe past. But yeah, I agree that the Internet  ",
      "offset": 11061.64,
      "duration": 7.08
    },
    {
      "lang": "en",
      "text": "has been a golden age for anonymity. I’m a little \nbit concerned that AI will make it much easier to  ",
      "offset": 11068.72,
      "duration": 5.96
    },
    {
      "lang": "en",
      "text": "break anonymity. I hope the golden age continues.\nYeah, seems like a great note to end on. Thank you  ",
      "offset": 11074.68,
      "duration": 8.92
    },
    {
      "lang": "en",
      "text": "guys so much for doing this.\nThank you. ",
      "offset": 11083.6,
      "duration": 1.44
    },
    {
      "lang": "en",
      "text": "Thank you so much. This was a blast.\nYeah, I had a great time. ",
      "offset": 11085.04,
      "duration": 2.12
    },
    {
      "lang": "en",
      "text": "Huge fan of your podcast.\nThank you.",
      "offset": 11087.16,
      "duration": 2.64
    }
  ],
  "cleanText": null,
  "dumpedAt": "2025-07-21T18:43:25.428Z"
}