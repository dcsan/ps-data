{
  "episodeId": "fWsaU6_A4XU",
  "channelSlug": "@superdatasciencewithjonkrohn",
  "title": "Why Data-Centric Machine Learning Is the Future of AI (with Lilith Bat-Leah)",
  "publishedAt": "2025-07-06T11:01:17.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2.35,
      "duration": 4.449
    },
    {
      "lang": "en",
      "text": "the impetus uh for having an episode",
      "offset": 4.24,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "when we talked about it on the train",
      "offset": 6.799,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "already a year ago was this idea of",
      "offset": 8.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "datacentric machine learning and so this",
      "offset": 10.559,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "is now a topic that is you know it's not",
      "offset": 13.36,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "this isn't just like oh there's some",
      "offset": 15.759,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "analogies here that might be relevant to",
      "offset": 17.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "your industry datacentric ML is relevant",
      "offset": 19.039,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "to every listener anybody who's working",
      "offset": 21.6,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "with data um this is relevant and so",
      "offset": 24.72,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "tell us about datacentric machine",
      "offset": 27.92,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "learning research, DMLR, and um and my",
      "offset": 30.32,
      "duration": 8.88
    },
    {
      "lang": "en",
      "text": "understanding is that you fell into DMLR",
      "offset": 35.04,
      "duration": 8.24
    },
    {
      "lang": "en",
      "text": "as as a result of how how messy the data",
      "offset": 39.2,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "are in the legal space.\n Yeah, that's",
      "offset": 43.28,
      "duration": 8.08
    },
    {
      "lang": "en",
      "text": "right. So um in my first R&amp;D role, I was",
      "offset": 46.719,
      "duration": 8.081
    },
    {
      "lang": "en",
      "text": "really focused on on algorithms and on",
      "offset": 51.36,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "finding the best classification",
      "offset": 54.8,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "algorithms",
      "offset": 56.32,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "uh for for these classification tasks",
      "offset": 57.84,
      "duration": 4.559
    },
    {
      "lang": "en",
      "text": "that we've discussed. Um, at a certain",
      "offset": 59.92,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "point I realized that the label data I",
      "offset": 62.399,
      "duration": 6.881
    },
    {
      "lang": "en",
      "text": "was working with was so noisy, just had",
      "offset": 65.84,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "so many uh mislabeled",
      "offset": 69.28,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "instances and and all of that um that it",
      "offset": 72.72,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "really curtailed my ability to evaluate",
      "offset": 76.88,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "the performance of of the of the",
      "offset": 80.56,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "algorithm um just because I couldn't",
      "offset": 84.24,
      "duration": 7.6
    },
    {
      "lang": "en",
      "text": "necessarily trust my data. Uh so that",
      "offset": 87.52,
      "duration": 8.72
    },
    {
      "lang": "en",
      "text": "led me to be very interested in what",
      "offset": 91.84,
      "duration": 8.319
    },
    {
      "lang": "en",
      "text": "Andrew Ing coined datacentric AI. Um and",
      "offset": 96.24,
      "duration": 6.879
    },
    {
      "lang": "en",
      "text": "I ended up getting involved with a",
      "offset": 100.159,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "working group at ML Commons called data",
      "offset": 103.119,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "perf. Uh where we were looking to",
      "offset": 105.68,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "benchmark datacentric machine learning.",
      "offset": 108,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "Um that ended up leading to a few",
      "offset": 110.56,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "different workshops that we've organized",
      "offset": 114.64,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "at Icleair and ICML.",
      "offset": 116.64,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "Um data proof also became a Nurup's",
      "offset": 119.759,
      "duration": 7.601
    },
    {
      "lang": "en",
      "text": "paper. Um and uh",
      "offset": 122.56,
      "duration": 7.199
    },
    {
      "lang": "en",
      "text": "yeah basically it turned into a whole",
      "offset": 127.36,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "community. So now there's a DMLR",
      "offset": 129.759,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "journal, there are the DMLR workshops at",
      "offset": 131.52,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "these conferences and then data proerf",
      "offset": 134,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "morphed into the datacentric machine",
      "offset": 136.319,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "learning research working group with ML",
      "offset": 137.92,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "common. So we have a lot of different",
      "offset": 140.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "things going on. We're working in",
      "offset": 142.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "partnership with common crawl, the",
      "offset": 144.64,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "foundation that curates the data sets",
      "offset": 147.2,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "that most LLMs have been trained on. Um",
      "offset": 149.52,
      "duration": 6.96
    },
    {
      "lang": "en",
      "text": "we're we're partnering with them on a on",
      "offset": 152.959,
      "duration": 7.121
    },
    {
      "lang": "en",
      "text": "a challenge that will result in a low",
      "offset": 156.48,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "resource language data set that that",
      "offset": 160.08,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "will be publicly available. So if you're",
      "offset": 163.84,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "interested in joining the working group,",
      "offset": 165.76,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "please do get involved. Uh again, it's",
      "offset": 168.16,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "with ML Commons. Um you can go to that",
      "offset": 170.08,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "site and and sign up working group.",
      "offset": 173.36,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "We'll be sure to have a link to ML",
      "offset": 176.08,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "Commons in the show notes. And so when",
      "offset": 179.12,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "you say a low resource language, this",
      "offset": 181.519,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "is, you know, languages that for which",
      "offset": 183.36,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "there are not many data available",
      "offset": 185.519,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "online. Uh they could be, you know,",
      "offset": 187.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "rarely spoken languages or for whatever",
      "offset": 189.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "reason, languages that even even if",
      "offset": 192,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "they're spoken relatively commonly, they",
      "offset": 194.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "aren't represented on the internet.",
      "offset": 196.48,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "Exactly. Exactly.\n Nice. That sounds",
      "offset": 198.56,
      "duration": 4.319
    },
    {
      "lang": "en",
      "text": "really cool. And so those acronyms that",
      "offset": 201.12,
      "duration": 3.039
    },
    {
      "lang": "en",
      "text": "you were that you were saying there",
      "offset": 202.879,
      "duration": 3.601
    },
    {
      "lang": "en",
      "text": "earlier where this DMLR initiative was",
      "offset": 204.159,
      "duration": 4.561
    },
    {
      "lang": "en",
      "text": "getting traction. So uh conferences like",
      "offset": 206.48,
      "duration": 5.119
    },
    {
      "lang": "en",
      "text": "Iclear, ICML, NURIPS, these are the",
      "offset": 208.72,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "biggest conferences that there are,",
      "offset": 211.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "academic conferences that there are and",
      "offset": 213.76,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "so really cool that you get such an",
      "offset": 216,
      "duration": 2.72
    },
    {
      "lang": "en",
      "text": "impact there. And it's also interesting",
      "offset": 217.36,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "to hear the the connection to Andrew Ing",
      "offset": 218.72,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "there. Um because he so he I have in my",
      "offset": 221.76,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "notes here somewhere. I'm kind of",
      "offset": 225.76,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "scrolling around in here. Yeah. So uh at",
      "offset": 227.44,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "the inaugural DMLR workshop, Andrew Ing",
      "offset": 229.519,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "was the keynote.\n Yes. Yes. Exactly. and",
      "offset": 233.2,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "he was involved with data perf as well.",
      "offset": 235.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "He's on that data perf.",
      "offset": 237.76,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "Okay. So, I'm I'm I'm now very clear on",
      "offset": 240.72,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "the importance of DMLR and the traction",
      "offset": 243.28,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's getting and big wigs like Andrew",
      "offset": 245.84,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Ing being involved. Probably most of our",
      "offset": 247.84,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "listeners know who Andrew Ing is. He's",
      "offset": 250.56,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "one of the biggest names in data science",
      "offset": 252.72,
      "duration": 4.479
    },
    {
      "lang": "en",
      "text": "period. And if uh if you aren't already",
      "offset": 254.319,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "familiar with him, we he was on our show",
      "offset": 257.199,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "in December. So episode 841 you can go",
      "offset": 259.199,
      "duration": 5.841
    },
    {
      "lang": "en",
      "text": "back to we'll have a link to that in the",
      "offset": 262.639,
      "duration": 5.681
    },
    {
      "lang": "en",
      "text": "show notes as well. Um so yeah so now I",
      "offset": 265.04,
      "duration": 5.76
    },
    {
      "lang": "en",
      "text": "have a clear understanding of you know",
      "offset": 268.32,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "datacentric machine learning being very",
      "offset": 270.8,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "important gaining traction but uh our",
      "offset": 272.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "listeners still might not have a great",
      "offset": 275.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "understanding of what it is.\n Yeah. Yeah.",
      "offset": 277.36,
      "duration": 6.48
    },
    {
      "lang": "en",
      "text": "So um the best way I can explain it is",
      "offset": 280.8,
      "duration": 5.839
    },
    {
      "lang": "en",
      "text": "that in traditional",
      "offset": 283.84,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "machine learning paradigms you're",
      "offset": 286.639,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "iterating on the model. you're iterating",
      "offset": 288.479,
      "duration": 7.44
    },
    {
      "lang": "en",
      "text": "on the model architecture, on the um on",
      "offset": 290.56,
      "duration": 8.16
    },
    {
      "lang": "en",
      "text": "the learning algorithm, all of those",
      "offset": 295.919,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "sorts of pieces. And that's where you're",
      "offset": 298.72,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "really focused on on improving",
      "offset": 300.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "performance is by iterating on the",
      "offset": 303.68,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "model. With datacentric machine",
      "offset": 305.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "learning, you're iterating on the data.",
      "offset": 307.84,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "So, you're holding the model fixed and",
      "offset": 310.16,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "you're improving the data. You're",
      "offset": 311.919,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "systematically engineering better data.",
      "offset": 313.68,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "And then there are all these different",
      "offset": 317.039,
      "duration": 3.281
    },
    {
      "lang": "en",
      "text": "questions, right? So there's the the",
      "offset": 318.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "question of whether to aggregate labels",
      "offset": 320.32,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "or not. Um there's a really interesting",
      "offset": 322.24,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "paper Dory me that looked at waiting",
      "offset": 326.16,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "different domains of the pile to get the",
      "offset": 329.44,
      "duration": 6.72
    },
    {
      "lang": "en",
      "text": "best um LLM pre-training performance. Um",
      "offset": 331.68,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "so there's yeah it can it can go lots of",
      "offset": 336.16,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "different ways. There's there's another",
      "offset": 338.32,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "paper I'm thinking of I can't remember",
      "offset": 340,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "the name but they looked at selecting",
      "offset": 341.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "the best data points for training a",
      "offset": 343.759,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "model a priori. So, not even active",
      "offset": 346.08,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "learning where you're starting with the",
      "offset": 348.72,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "results of the model to determine which",
      "offset": 350.88,
      "duration": 4.159
    },
    {
      "lang": "en",
      "text": "um additional data points you should",
      "offset": 353.52,
      "duration": 6.239
    },
    {
      "lang": "en",
      "text": "have labeled, but um but just with a",
      "offset": 355.039,
      "duration": 8.481
    },
    {
      "lang": "en",
      "text": "with a data set from scratch using",
      "offset": 359.759,
      "duration": 6.16
    },
    {
      "lang": "en",
      "text": "linear algebra to figure out which uh",
      "offset": 363.52,
      "duration": 7.32
    },
    {
      "lang": "en",
      "text": "which data points are worth labeling.",
      "offset": 365.919,
      "duration": 4.921
    }
  ],
  "cleanText": "[Music]\nThe impetus, uh, for having an episode when we talked about it on the train already a year ago was this idea of Data-Centric Machine Learning. And so this is now a topic that is, you know, it's not this isn't just like, oh, there's some analogies here that might be relevant to your industry. Data-Centric ML is relevant to every listener, anybody who's working with data. Um, this is relevant. And so tell us about Data-Centric Machine Learning research, DMLR, and, um, and my understanding is that you fell into DMLR as a result of how, how messy the data are in the legal space.\n\nYeah, that's right. So, um, in my first R&D role, I was really focused on on algorithms and on finding the best classification algorithms, uh, for these classification tasks that we've discussed. Um, at a certain point, I realized that the label data I was working with was so noisy, just had so many, uh, mislabeled instances and all of that, um, that it really curtailed my ability to evaluate the performance of the algorithm, um, just because I couldn't necessarily trust my data. Uh, so that led me to be very interested in what Andrew Ng coined Data-Centric AI. Um, and I ended up getting involved with a working group at ML Commons called DataPerf, uh, where we were looking to benchmark Data-Centric Machine Learning. Um, that ended up leading to a few different workshops that we've organized at ICLR and ICML.\n\nUm, DataPerf also became a NeurIPS paper. Um, and, uh, yeah, basically it turned into a whole community. So now there's a DMLR journal, there are the DMLR workshops at these conferences, and then DataPerf morphed into the Data-Centric Machine Learning Research working group with ML Commons. So we have a lot of different things going on. We're working in partnership with Common Crawl, the foundation that curates the data sets that most LLMs have been trained on. Um, we're partnering with them on a challenge that will result in a low-resource language data set that will be publicly available. So if you're interested in joining the working group, please do get involved. Uh, again, it's with ML Commons. Um, you can go to that site and sign up for the working group. We'll be sure to have a link to ML Commons in the show notes.\n\nAnd so when you say a low-resource language, this is, you know, languages for which there are not many data available online. Uh, they could be, you know, rarely spoken languages or for whatever reason, languages that even if they're spoken relatively commonly, they aren't represented on the internet.\n\nExactly. Exactly. Nice. That sounds really cool. And so those acronyms that you were that you were saying there earlier where this DMLR initiative was getting traction. So, uh, conferences like ICLR, ICML, NeurIPS, these are the biggest conferences that there are, academic conferences that there are, and so really cool that you get such an impact there. And it's also interesting to hear the connection to Andrew Ng there. Um, because he, so he, I have in my notes here somewhere. I'm kind of scrolling around in here. Yeah. So, uh, at the inaugural DMLR workshop, Andrew Ng was the keynote.\n\nYes. Yes. Exactly. And he was involved with DataPerf as well. He's on that DataPerf.\n\nOkay. So, I'm, I'm now very clear on the importance of DMLR and the traction it's getting and big wigs like Andrew Ng being involved. Probably most of our listeners know who Andrew Ng is. He's one of the biggest names in data science period. And if, uh, if you aren't already familiar with him, we, he was on our show in December. So episode 841, you can go back to. We'll have a link to that in the show notes as well. Um, so yeah, so now I have a clear understanding of, you know, Data-Centric Machine Learning being very important, gaining traction, but our listeners still might not have a great understanding of what it is.\n\nYeah. Yeah. So, um, the best way I can explain it is that in traditional machine learning paradigms, you're iterating on the model. You're iterating on the model architecture, on the, um, on the learning algorithm, all of those sorts of pieces. And that's where you're really focused on improving performance is by iterating on the model. With Data-Centric Machine Learning, you're iterating on the data. So you're holding the model fixed and you're improving the data. You're systematically engineering better data. And then there are all these different questions, right? So there's the question of whether to aggregate labels or not. Um, there's a really interesting paper, Dory, that looked at weighting different domains of the pile to get the best, um, LLM pre-training performance. Um, so there's, yeah, it can go lots of different ways. There's another paper I'm thinking of, I can't remember the name, but they looked at selecting the best data points for training a model a priori. So, not even active learning where you're starting with the results of the model to determine which, um, additional data points you should have labeled, but, um, but just with a data set from scratch using linear algebra to figure out which data points are worth labeling.",
  "dumpedAt": "2025-07-21T18:43:25.309Z"
}