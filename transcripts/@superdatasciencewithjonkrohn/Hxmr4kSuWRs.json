{
  "episodeId": "Hxmr4kSuWRs",
  "channelSlug": "@superdatasciencewithjonkrohn",
  "title": "AI Engineer in 2025: What Skills Do You Actually Need?",
  "publishedAt": "2025-06-25T11:00:26.000Z",
  "rawLines": [
    {
      "lang": "en",
      "text": "[Music]",
      "offset": 2.35,
      "duration": 5.409
    },
    {
      "lang": "en",
      "text": "in this current um like where we are in",
      "offset": 4.24,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "terms of AI, you know, like if it feels",
      "offset": 7.759,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "to me like AI engineering, LLM",
      "offset": 10.24,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "engineering, etc., all those roles,",
      "offset": 13.759,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "they're um kind of like going through",
      "offset": 15.839,
      "duration": 6.481
    },
    {
      "lang": "en",
      "text": "that same arch that data science was",
      "offset": 19.039,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "going through like 10 years ago. Uh so",
      "offset": 22.32,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "it's very blurry at the moment like what",
      "offset": 24.56,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "does AI engineer mean to you? Yeah, it's",
      "offset": 26.88,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "an interesting thing because",
      "offset": 29.84,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "theoretically up until",
      "offset": 31.76,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "two years ago, if you said you were an",
      "offset": 34.96,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "AI engineer, that probably meant that",
      "offset": 37.36,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "you were like an AI researcher. So, you",
      "offset": 40.16,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "know, you know, maybe at a frontier lab",
      "offset": 42.399,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "like Meta, Google, OpenAI, Anthropic,",
      "offset": 44.879,
      "duration": 7.36
    },
    {
      "lang": "en",
      "text": "and you're your engineering, you know,",
      "offset": 48.8,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "you're figuring out how to get",
      "offset": 52.239,
      "duration": 2.96
    },
    {
      "lang": "en",
      "text": "transformers to work together more",
      "offset": 53.68,
      "duration": 5.359
    },
    {
      "lang": "en",
      "text": "quickly across a bunch of GPUs, how to",
      "offset": 55.199,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "uh automatically clean up data in some",
      "offset": 59.039,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "way that it improves the outputs of a",
      "offset": 60.879,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "trained large language model. like",
      "offset": 63.84,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "that's what I would have up until",
      "offset": 65.84,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "recently, up until a couple years ago,",
      "offset": 67.76,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "thought an AI engineer was, but now it",
      "offset": 69.52,
      "duration": 7.12
    },
    {
      "lang": "en",
      "text": "seems to mostly mean somebody who is",
      "offset": 72.72,
      "duration": 7.92
    },
    {
      "lang": "en",
      "text": "using existing LLMs and calling APIs.",
      "offset": 76.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "And so there's still\n there's still",
      "offset": 80.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "thoughtfulness that needs to go into",
      "offset": 82.64,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "making sure the data are clean and",
      "offset": 84.64,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "consistent and you have guardrails up.",
      "offset": 86.32,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "Um, but it's you're you're working at a",
      "offset": 88.64,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "at a at a more abstract level. um at",
      "offset": 92.079,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "least in like kind of the the simplest",
      "offset": 94.72,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "way of thinking about the job.\n So do you",
      "offset": 96.96,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "reckon you could get that job done",
      "offset": 98.88,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "without any knowledge whatsoever of",
      "offset": 100.56,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "fundamentals of machine learning or even",
      "offset": 102.72,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "the underlying",
      "offset": 105.36,
      "duration": 5.84
    },
    {
      "lang": "en",
      "text": "um deep learning tensor what's it",
      "offset": 107.439,
      "duration": 7.281
    },
    {
      "lang": "en",
      "text": "called? Um\n yeah TensorFlow PyTorch\n yeah",
      "offset": 111.2,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "TensorFlow PyTorch but even even",
      "offset": 114.72,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "transformer architectures.\n Oh yeah. Oh",
      "offset": 116.72,
      "duration": 5.439
    },
    {
      "lang": "en",
      "text": "yeah. I mean, you could definitely, you",
      "offset": 119.28,
      "duration": 4.799
    },
    {
      "lang": "en",
      "text": "know, for that, you know, like I'm",
      "offset": 122.159,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "saying, you know, you could you could",
      "offset": 124.079,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "get Yeah, there's lots of LLM jobs out",
      "offset": 125.759,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "there where you basically need to, you",
      "offset": 128.479,
      "duration": 4.641
    },
    {
      "lang": "en",
      "text": "know, you need to understand how to be",
      "offset": 130.479,
      "duration": 4.241
    },
    {
      "lang": "en",
      "text": "evaluating data that are going in as",
      "offset": 133.12,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "inputs and outputs. You know, you need",
      "offset": 134.72,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "to be able to do some exploratory data",
      "offset": 136,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "analysis in Python, those kinds of data",
      "offset": 138.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "science skills. But you wouldn't",
      "offset": 140.8,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "necessarily need to understand how",
      "offset": 142.56,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "stochastic gradient descent works or",
      "offset": 144.56,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "reinforcement learning works. You know,",
      "offset": 146.239,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "just because those kinds of approaches",
      "offset": 148.56,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "were used to train the LLM that you're",
      "offset": 150,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "using,\n it's probably more exper more",
      "offset": 151.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "important to just have lots of",
      "offset": 154.64,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "experience with prompting LLMs and",
      "offset": 156.08,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "seeing what they can do, understanding,",
      "offset": 158.08,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "you know, experimenting with, okay, if I",
      "offset": 160.48,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "use a three billion parameter LLM, how",
      "offset": 162,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "does that perform relative to using, you",
      "offset": 164.879,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "know, the latest and greatest Claude 4",
      "offset": 166.72,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "from Anthropic?",
      "offset": 168.64,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "um what can I you know there might be",
      "offset": 170.56,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "fine-tuning involved so um you know",
      "offset": 172.72,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "understanding",
      "offset": 175.599,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "the the approaches that exist out there",
      "offset": 177.36,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "for uh for fine-tuning so things like",
      "offset": 180,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Laura low rank adaptation being aware of",
      "offset": 182.4,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "those kinds of things to be able to take",
      "offset": 184.72,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "a three billion parameter open-source",
      "offset": 186.48,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "llama model from meta and then be able",
      "offset": 188.8,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "to fine-tune it to some specific task",
      "offset": 191.2,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "you might actually be able to with a",
      "offset": 193.92,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "three billion parameter model running on",
      "offset": 195.28,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "your own infrastructure or running",
      "offset": 197.04,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "through some cloud provider like Hugging",
      "offset": 199.12,
      "duration": 5.199
    },
    {
      "lang": "en",
      "text": "Face or PyTorch Lightning, you can have",
      "offset": 200.8,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "this very small LLM running on some",
      "offset": 204.319,
      "duration": 6.161
    },
    {
      "lang": "en",
      "text": "small on some very specific task or some",
      "offset": 207.68,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "uh small number of very specific tasks",
      "offset": 210.48,
      "duration": 3.839
    },
    {
      "lang": "en",
      "text": "and because you fine-tuned it fine-tuned",
      "offset": 212.48,
      "duration": 4.24
    },
    {
      "lang": "en",
      "text": "it to those tasks, it can outperform the",
      "offset": 214.319,
      "duration": 4.881
    },
    {
      "lang": "en",
      "text": "latest and greatest like Claude for. And",
      "offset": 216.72,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "so th those are the kinds of things, you",
      "offset": 219.2,
      "duration": 3.119
    },
    {
      "lang": "en",
      "text": "know, that kind of like empirical",
      "offset": 220.799,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "experience with playing around with LLMs",
      "offset": 222.319,
      "duration": 6.321
    },
    {
      "lang": "en",
      "text": "um is probably more important. You know,",
      "offset": 226.159,
      "duration": 4.961
    },
    {
      "lang": "en",
      "text": "like you don't really need to understand",
      "offset": 228.64,
      "duration": 6.64
    },
    {
      "lang": "en",
      "text": "how Laura works to use Laura.\n Yeah.\n Um",
      "offset": 231.12,
      "duration": 7.119
    },
    {
      "lang": "en",
      "text": "and by the way, that Laura, it's L A if",
      "offset": 235.28,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "you're\n That's right.\n Googling that. I'll",
      "offset": 238.239,
      "duration": 3.121
    },
    {
      "lang": "en",
      "text": "try to remember to put\n low rank",
      "offset": 239.92,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "adaptation.\n Low rank adaptation. I'll",
      "offset": 241.36,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "try to remember to put a link in the",
      "offset": 243.68,
      "duration": 4.96
    },
    {
      "lang": "en",
      "text": "show notes to our episode on Laura which",
      "offset": 245.92,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "is uh back in episode 674.",
      "offset": 248.64,
      "duration": 7.76
    },
    {
      "lang": "en",
      "text": "I I kind of give an introduction to",
      "offset": 253.68,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "fine-tuning.\n Yeah, I remember that one.",
      "offset": 256.4,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "That was that was a great introduction.",
      "offset": 258,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "Yeah. Interesting. Like the the analogy",
      "offset": 260.56,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that comes to my mind is like I love",
      "offset": 262.72,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "cooking and for me it's like let's say",
      "offset": 264.88,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "for cooking you're using a blender,",
      "offset": 268.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "right? Or other tools like an oven and",
      "offset": 270.4,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "things like that. You don't need to know",
      "offset": 272.24,
      "duration": 4.08
    },
    {
      "lang": "en",
      "text": "how an oven works in in in the back end",
      "offset": 274.08,
      "duration": 5.92
    },
    {
      "lang": "en",
      "text": "or a microwave or uh that's lazy cooking",
      "offset": 276.32,
      "duration": 5.599
    },
    {
      "lang": "en",
      "text": "or a blender. You don't need to be able",
      "offset": 280,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "to pull apart a blender and put it back",
      "offset": 281.919,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "together, but you can use it, you know.",
      "offset": 283.84,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "So, same thing here with um like AI is",
      "offset": 285.759,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "is going towards that direction where uh",
      "offset": 289.84,
      "duration": 7.04
    },
    {
      "lang": "en",
      "text": "these um tools are actually just tools.",
      "offset": 293.12,
      "duration": 7.2
    },
    {
      "lang": "en",
      "text": "you can can get away and and in fact you",
      "offset": 296.88,
      "duration": 5.12
    },
    {
      "lang": "en",
      "text": "don't as you said you don't have to go",
      "offset": 300.32,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "into the depths of understanding these",
      "offset": 302,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "tools to be able to use them and I'm",
      "offset": 304.16,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just wondering like what percentage of",
      "offset": 305.68,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "jobs uh are going to be for AI engineers",
      "offset": 307.36,
      "duration": 6.32
    },
    {
      "lang": "en",
      "text": "who know how to use these tools versus",
      "offset": 311.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "uh the percentage of jobs that where you",
      "offset": 313.68,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "actually need to understand the",
      "offset": 315.68,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "underlying uh technology and be able to",
      "offset": 317.52,
      "duration": 4.239
    },
    {
      "lang": "en",
      "text": "tinker with it like what what should",
      "offset": 320.08,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "people be focused on learning? I know it",
      "offset": 321.759,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "depends on their interest, but in terms",
      "offset": 324.56,
      "duration": 4.88
    },
    {
      "lang": "en",
      "text": "of like um supply or demand for jobs, I",
      "offset": 325.84,
      "duration": 5.04
    },
    {
      "lang": "en",
      "text": "wonder how it's going to play out in the",
      "offset": 329.44,
      "duration": 3.68
    },
    {
      "lang": "en",
      "text": "coming months and years.\n Yeah, I think",
      "offset": 330.88,
      "duration": 5.039
    },
    {
      "lang": "en",
      "text": "we we move more and more towards the",
      "offset": 333.12,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "abstract. You know, you don't",
      "offset": 335.919,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "I love that analogy that you just gave",
      "offset": 338.72,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "with the blenders and microwaves and",
      "offset": 340.4,
      "duration": 5.6
    },
    {
      "lang": "en",
      "text": "stuff, you know, you you can more and",
      "offset": 342.08,
      "duration": 6.559
    },
    {
      "lang": "en",
      "text": "more rely on the abstractions.",
      "offset": 346,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "Um, but I still think",
      "offset": 348.639,
      "duration": 6.961
    },
    {
      "lang": "en",
      "text": "you can you can probably get",
      "offset": 352,
      "duration": 5.759
    },
    {
      "lang": "en",
      "text": "ultimately like as you progress further",
      "offset": 355.6,
      "duration": 6.8
    },
    {
      "lang": "en",
      "text": "in your career, I think you can get uh",
      "offset": 357.759,
      "duration": 6.401
    },
    {
      "lang": "en",
      "text": "higher paying roles. I mean, it depends",
      "offset": 362.4,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "on exactly which way you go because you",
      "offset": 364.16,
      "duration": 2.319
    },
    {
      "lang": "en",
      "text": "could kind of say, okay, you know what",
      "offset": 365.52,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "I'm going to do?\n I'm kind of riffing",
      "offset": 366.479,
      "duration": 3.521
    },
    {
      "lang": "en",
      "text": "here, by the way, Carol. This is like",
      "offset": 368.4,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "just my thoughts, but I'm kind of like",
      "offset": 370,
      "duration": 3.52
    },
    {
      "lang": "en",
      "text": "thinking",
      "offset": 371.6,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "if you",
      "offset": 373.52,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "if you",
      "offset": 375.44,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "uh you know, if you want to focus on",
      "offset": 377.28,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "commercial impact, you could actually",
      "offset": 378.8,
      "duration": 4.399
    },
    {
      "lang": "en",
      "text": "say, you know what, I actually I'm going",
      "offset": 380.479,
      "duration": 5.121
    },
    {
      "lang": "en",
      "text": "to use training like there is available",
      "offset": 383.199,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "in superdatience.com",
      "offset": 385.6,
      "duration": 4.48
    },
    {
      "lang": "en",
      "text": "and use that to become proficient at",
      "offset": 387.6,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "building LLMs and I'm going to figure",
      "offset": 390.08,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "out how to make those uh look nice in",
      "offset": 392.319,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "like a grado app or something, you know,",
      "offset": 395.6,
      "duration": 3.439
    },
    {
      "lang": "en",
      "text": "some kind of user interface. you can",
      "offset": 397.759,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "quickly put together so that you can uh",
      "offset": 399.039,
      "duration": 4.321
    },
    {
      "lang": "en",
      "text": "you know have a click and point",
      "offset": 402.24,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "interface for people to be using your AI",
      "offset": 403.36,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "models in the background or you know",
      "offset": 405.44,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "some kind of AI solution that you come",
      "offset": 407.28,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "up with through using LLMs and you might",
      "offset": 408.72,
      "duration": 4.4
    },
    {
      "lang": "en",
      "text": "not you know yeah understand how",
      "offset": 411.6,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "gradient descent works or uh you know",
      "offset": 413.12,
      "duration": 6.24
    },
    {
      "lang": "en",
      "text": "fundamentals of machine learning but",
      "offset": 415.6,
      "duration": 5.52
    },
    {
      "lang": "en",
      "text": "you're able to put together powerful",
      "offset": 419.36,
      "duration": 3.679
    },
    {
      "lang": "en",
      "text": "commercial applications you could do",
      "offset": 421.12,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "that on a small team or on a big team or",
      "offset": 423.039,
      "duration": 4.401
    },
    {
      "lang": "en",
      "text": "just on your own and you could",
      "offset": 425.68,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "potentially be enormously successful",
      "offset": 427.44,
      "duration": 6.08
    },
    {
      "lang": "en",
      "text": "like in in today as well as more and",
      "offset": 429.759,
      "duration": 5.761
    },
    {
      "lang": "en",
      "text": "more the further we go into the future,",
      "offset": 433.52,
      "duration": 4.64
    },
    {
      "lang": "en",
      "text": "the more you will be able to have",
      "offset": 435.52,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "agents, teams of agents working on",
      "offset": 438.16,
      "duration": 3.759
    },
    {
      "lang": "en",
      "text": "different tasks for you. And you know,",
      "offset": 440.08,
      "duration": 3.36
    },
    {
      "lang": "en",
      "text": "you could build a big business. You",
      "offset": 441.919,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "could be a solo entrepreneur and have",
      "offset": 443.44,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "like hundreds of agents working on",
      "offset": 445.68,
      "duration": 2.959
    },
    {
      "lang": "en",
      "text": "different tasks for different clients",
      "offset": 447.28,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "and that'll get better and better. And",
      "offset": 448.639,
      "duration": 4.801
    },
    {
      "lang": "en",
      "text": "you know, so you could potentially have",
      "offset": 451.44,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "a lot of success um that way using just",
      "offset": 453.44,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "the abstractions. But where I was",
      "offset": 456.72,
      "duration": 3.199
    },
    {
      "lang": "en",
      "text": "originally going before I thought of",
      "offset": 458.8,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "that second idea is that you similarly,",
      "offset": 459.919,
      "duration": 5.201
    },
    {
      "lang": "en",
      "text": "you know, you could as you advance in",
      "offset": 463.12,
      "duration": 3.359
    },
    {
      "lang": "en",
      "text": "your career, you could say, okay, I'm",
      "offset": 465.12,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "going to I'm going to peel back layers",
      "offset": 466.479,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "of the onion more and more. I'm going to",
      "offset": 468.16,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "understand what's going on under these",
      "offset": 469.759,
      "duration": 3.921
    },
    {
      "lang": "en",
      "text": "abstractions more and more and kind of",
      "offset": 472.08,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "just chip away over years, over decades,",
      "offset": 473.68,
      "duration": 5.68
    },
    {
      "lang": "en",
      "text": "you become more and more expert at",
      "offset": 477.36,
      "duration": 3.76
    },
    {
      "lang": "en",
      "text": "understanding machine learning",
      "offset": 479.36,
      "duration": 5.679
    },
    {
      "lang": "en",
      "text": "fundamentals and mathematics and physics",
      "offset": 481.12,
      "duration": 7.519
    },
    {
      "lang": "en",
      "text": "and uh engineering, uh maybe",
      "offset": 485.039,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "electronics. Like there's all kinds of",
      "offset": 488.639,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "related fields that you could dig more",
      "offset": 491.039,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "and more into. And as you dig more and",
      "offset": 492.639,
      "duration": 7.361
    },
    {
      "lang": "en",
      "text": "more um you know your value",
      "offset": 495.759,
      "duration": 7.84
    },
    {
      "lang": "en",
      "text": "to your clients, your users, your",
      "offset": 500,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "employer",
      "offset": 503.599,
      "duration": 4.081
    },
    {
      "lang": "en",
      "text": "um I think I think does increase and I",
      "offset": 505.28,
      "duration": 3.599
    },
    {
      "lang": "en",
      "text": "think that that will continue to be the",
      "offset": 507.68,
      "duration": 3.28
    },
    {
      "lang": "en",
      "text": "case in the future. Even though those",
      "offset": 508.879,
      "duration": 3.361
    },
    {
      "lang": "en",
      "text": "kinds of things like you know doing",
      "offset": 510.96,
      "duration": 3.519
    },
    {
      "lang": "en",
      "text": "mathematics, being able to solve data",
      "offset": 512.24,
      "duration": 3.84
    },
    {
      "lang": "en",
      "text": "science problems, computer science",
      "offset": 514.479,
      "duration": 3.761
    },
    {
      "lang": "en",
      "text": "problems, even though that's something",
      "offset": 516.08,
      "duration": 4.639
    },
    {
      "lang": "en",
      "text": "that LLMs will be able to do more and",
      "offset": 518.24,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "more and more, I think there will still",
      "offset": 520.719,
      "duration": 6.001
    },
    {
      "lang": "en",
      "text": "be and maybe I'm just a dinosaur with",
      "offset": 523.68,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "like outdated ideas, but I think if you",
      "offset": 526.72,
      "duration": 5.2
    },
    {
      "lang": "en",
      "text": "understand that stuff, one, it's",
      "offset": 529.68,
      "duration": 3.44
    },
    {
      "lang": "en",
      "text": "interesting. And so it's kind of",
      "offset": 531.92,
      "duration": 4.8
    },
    {
      "lang": "en",
      "text": "interesting like you know if you're",
      "offset": 533.12,
      "duration": 6.88
    },
    {
      "lang": "en",
      "text": "you know if if you are",
      "offset": 536.72,
      "duration": 4.32
    },
    {
      "lang": "en",
      "text": "Yeah. because it's kind of an",
      "offset": 540,
      "duration": 2.399
    },
    {
      "lang": "en",
      "text": "interesting thing here because it's like",
      "offset": 541.04,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "if you're a chef who's able to",
      "offset": 542.399,
      "duration": 5.521
    },
    {
      "lang": "en",
      "text": "understand how a microwave works and",
      "offset": 545.2,
      "duration": 5.36
    },
    {
      "lang": "en",
      "text": "make a better microwave that somehow you",
      "offset": 547.92,
      "duration": 5.28
    },
    {
      "lang": "en",
      "text": "know you can put like a raw raw pizza",
      "offset": 550.56,
      "duration": 4.56
    },
    {
      "lang": "en",
      "text": "ingredients in there and it turns into",
      "offset": 553.2,
      "duration": 4.079
    },
    {
      "lang": "en",
      "text": "like this. It seems like it's like this",
      "offset": 555.12,
      "duration": 4.719
    },
    {
      "lang": "en",
      "text": "great pizza oven cooked pizza in just",
      "offset": 557.279,
      "duration": 4.481
    },
    {
      "lang": "en",
      "text": "like 10 seconds. like it's, you know,",
      "offset": 559.839,
      "duration": 4
    },
    {
      "lang": "en",
      "text": "that kind of magic would only be",
      "offset": 561.76,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "possible if you learned like the nuclear",
      "offset": 563.839,
      "duration": 4.721
    },
    {
      "lang": "en",
      "text": "physics of how the microwave works, you",
      "offset": 565.92,
      "duration": 4.72
    },
    {
      "lang": "en",
      "text": "know, it's like there's there's there's",
      "offset": 568.56,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "magic and possibility if you if you do",
      "offset": 570.64,
      "duration": 6
    },
    {
      "lang": "en",
      "text": "dig deep. But um yeah, so I think either",
      "offset": 572.72,
      "duration": 5.44
    },
    {
      "lang": "en",
      "text": "way, you know, you can follow your",
      "offset": 576.64,
      "duration": 3.92
    },
    {
      "lang": "en",
      "text": "passions like if your passions are deep",
      "offset": 578.16,
      "duration": 4.16
    },
    {
      "lang": "en",
      "text": "in the nitty-gritty of what's underlying",
      "offset": 580.56,
      "duration": 2.88
    },
    {
      "lang": "en",
      "text": "machine learning models, I think you can",
      "offset": 582.32,
      "duration": 2.48
    },
    {
      "lang": "en",
      "text": "have a huge amount of success there the",
      "offset": 583.44,
      "duration": 3.04
    },
    {
      "lang": "en",
      "text": "more and more you learn. But equally, if",
      "offset": 584.8,
      "duration": 3.2
    },
    {
      "lang": "en",
      "text": "you're more interested in applications",
      "offset": 586.48,
      "duration": 3.12
    },
    {
      "lang": "en",
      "text": "and just making a big impact and you",
      "offset": 588,
      "duration": 3.6
    },
    {
      "lang": "en",
      "text": "want to stay with abstractions, you can",
      "offset": 589.6,
      "duration": 7.239
    },
    {
      "lang": "en",
      "text": "also have a lot of success that way too.",
      "offset": 591.6,
      "duration": 5.239
    }
  ],
  "cleanText": "[Music]\nIn this current, um, like where we are in terms of AI, you know, like if it feels to me like AI engineering, LLM engineering, etc., all those roles, they're, um, kind of like going through that same arch that data science was going through like 10 years ago.\nUh, so it's very blurry at the moment, like what does AI engineer mean to you?\nYeah, it's an interesting thing because theoretically up until two years ago, if you said you were an AI engineer, that probably meant that you were like an AI researcher.\nSo, you know, maybe at a frontier lab like Meta, Google, OpenAI, Anthropic, and you're your engineering, you know, you're figuring out how to get transformers to work together more quickly across a bunch of GPUs, how to, uh, automatically clean up data in some way that it improves the outputs of a trained large language model.\nLike that's what I would have up until recently, up until a couple years ago, thought an AI engineer was, but now it seems to mostly mean somebody who is using existing LLMs and calling APIs.\nAnd so there's still, there's still thoughtfulness that needs to go into making sure the data are clean and consistent and you have guardrails up.\nUm, but it's you're, you're working at a, at a, at a more abstract level, um, at least in like kind of the the simplest way of thinking about the job.\nSo do you reckon you could get that job done without any knowledge whatsoever of fundamentals of machine learning or even the underlying, um, deep learning tensor, what's it called?\nUm, yeah, TensorFlow, PyTorch, yeah, TensorFlow, PyTorch, but even, even transformer architectures.\nOh yeah.\nOh yeah.\nI mean, you could definitely, you know, for that, you know, like I'm saying, you know, you could, you could get.\nYeah, there's lots of LLM jobs out there where you basically need to, you know, you need to understand how to be evaluating data that are going in as inputs and outputs.\nYou know, you need to be able to do some exploratory data analysis in Python, those kinds of data science skills.\nBut you wouldn't necessarily need to understand how stochastic gradient descent works or reinforcement learning works.\nYou know, just because those kinds of approaches were used to train the LLM that you're using, it's probably more exper, more important to just have lots of experience with prompting LLMs and seeing what they can do, understanding, you know, experimenting with, okay, if I use a three billion parameter LLM, how does that perform relative to using, you know, the latest and greatest Claude 4 from Anthropic?\nUm, what can I, you know, there might be fine-tuning involved, so, um, you know, understanding the the approaches that exist out there for, uh, for fine-tuning, so things like Laura, low rank adaptation, being aware of those kinds of things to be able to take a three billion parameter open-source llama model from meta and then be able to fine-tune it to some specific task, you might actually be able to with a three billion parameter model running on your own infrastructure or running through some cloud provider like Hugging Face or PyTorch Lightning, you can have this very small LLM running on some small, on some very specific task or some, uh, small number of very specific tasks, and because you fine-tuned it, fine-tuned it to those tasks, it can outperform the latest and greatest like Claude for.\nAnd so those are the kinds of things, you know, that kind of like empirical experience with playing around with LLMs, um, is probably more important.\nYou know, like you don't really need to understand how Laura works to use Laura.\nYeah.\nUm, and by the way, that Laura, it's L A if you're.\nThat's right.\nGoogling that.\nI'll try to remember to put low rank adaptation.\nLow rank adaptation.\nI'll try to remember to put a link in the show notes to our episode on Laura, which is, uh, back in episode 674.\nI, I kind of give an introduction to fine-tuning.\nYeah, I remember that one.\nThat was, that was a great introduction.\nYeah.\nInteresting.\nLike the the analogy that comes to my mind is like I love cooking, and for me it's like, let's say for cooking you're using a blender, right?\nOr other tools like an oven and things like that.\nYou don't need to know how an oven works in in in the back end or a microwave or, uh, that's lazy cooking or a blender.\nYou don't need to be able to pull apart a blender and put it back together, but you can use it, you know.\nSo, same thing here with, um, like AI is is going towards that direction where, uh, these, um, tools are actually just tools.\nYou can can get away, and, and in fact, you don't, as you said, you don't have to go into the depths of understanding these tools to be able to use them, and I'm just wondering like what percentage of jobs, uh, are going to be for AI engineers who know how to use these tools versus, uh, the percentage of jobs that where you actually need to understand the underlying, uh, technology and be able to tinker with it, like what, what should people be focused on learning?\nI know it depends on their interest, but in terms of like, um, supply or demand for jobs, I wonder how it's going to play out in the coming months and years.\nYeah, I think we, we move more and more towards the abstract.\nYou know, you don't.\nI love that analogy that you just gave with the blenders and microwaves and stuff, you know, you, you can more and more rely on the abstractions.\nUm, but I still think you can, you can probably get, ultimately, like as you progress further in your career, I think you can get, uh, higher paying roles.\nI mean, it depends on exactly which way you go because you could kind of say, okay, you know what I'm going to do?\nI'm kind of riffing here, by the way, Carol.\nThis is like just my thoughts, but I'm kind of like thinking, if you, if you, uh, you know, if you want to focus on commercial impact, you could actually say, you know what, I actually, I'm going to use training like there is available in superdatascience.com and use that to become proficient at building LLMs, and I'm going to figure out how to make those, uh, look nice in like a Grado app or something, you know, some kind of user interface.\nYou can quickly put together so that you can, uh, you know, have a click and point interface for people to be using your AI models in the background or, you know, some kind of AI solution that you come up with through using LLMs, and you might not, you know, yeah, understand how gradient descent works or, uh, you know, fundamentals of machine learning, but you're able to put together powerful commercial applications.\nYou could do that on a small team or on a big team or just on your own, and you could potentially be enormously successful, like in in today as well as more and more the further we go into the future, the more you will be able to have agents, teams of agents working on different tasks for you.\nAnd you know, you could build a big business.\nYou could be a solo entrepreneur and have like hundreds of agents working on different tasks for different clients, and that'll get better and better.\nAnd you know, so you could potentially have a lot of success, um, that way using just the abstractions.\nBut where I was originally going before I thought of that second idea is that you similarly, you know, you could as you advance in your career, you could say, okay, I'm going to, I'm going to peel back layers of the onion more and more.\nI'm going to understand what's going on under these abstractions more and more and kind of just chip away over years, over decades, you become more and more expert at understanding machine learning fundamentals and mathematics and physics and, uh, engineering, uh, maybe electronics.\nLike there's all kinds of related fields that you could dig more and more into.\nAnd as you dig more and more, um, you know, your value to your clients, your users, your employer, um, I think, I think does increase, and I think that that will continue to be the case in the future.\nEven though those kinds of things, like, you know, doing mathematics, being able to solve data science problems, computer science problems, even though that's something that LLMs will be able to do more and more and more, I think there will still be, and maybe I'm just a dinosaur with like outdated ideas, but I think if you understand that stuff, one, it's interesting.\nAnd so it's kind of interesting, like, you know, if you're, you know, if, if you are.\nYeah, because it's kind of an interesting thing here because it's like if you're a chef who's able to understand how a microwave works and make a better microwave that somehow you know, you can put like a raw, raw pizza ingredients in there and it turns into like this.\nIt seems like it's like this great pizza oven cooked pizza in just like 10 seconds.\nLike it's, you know, that kind of magic would only be possible if you learned like the nuclear physics of how the microwave works, you know, it's like there's, there's, there's magic and possibility if you, if you do dig deep.\nBut, um, yeah, so I think either way, you know, you can follow your passions, like if your passions are deep in the nitty-gritty of what's underlying machine learning models, I think you can have a huge amount of success there the more and more you learn.\nBut equally, if you're more interested in applications and just making a big impact and you want to stay with abstractions, you can also have a lot of success that way too.\n",
  "dumpedAt": "2025-07-21T18:43:26.230Z"
}